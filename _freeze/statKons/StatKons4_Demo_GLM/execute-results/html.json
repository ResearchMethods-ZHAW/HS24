{
  "hash": "ff6e4af745e6442817ab9f10be877b9c",
  "result": {
    "markdown": "---\ndate: 2023-11-21\nlesson: StatKons4\nthema: GLM\nindex: 1\nformat:\n  html:\n    code-tools:\n      source: true\n---\n\n\n# StatKons4: Demo\n\n- Download dieses Demoscript via \"\\</\\>Code\" (oben rechts)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\n\nmytheme <-\n  theme_classic() +\n  theme(\n    axis.line = element_line(color = \"black\"),\n    axis.text = element_text(size = 12, color = \"black\"),\n    axis.title = element_text(size = 12, color = \"black\"),\n    axis.ticks = element_line(size = .75, color = \"black\"),\n    axis.ticks.length = unit(.5, \"cm\")\n  )\n```\n:::\n\n\n## Poisson Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n############\n# quasipoisson regression\n############\n\ncars <- mtcars |>\n  mutate(kml = (235.214583 / mpg))\n\nglm.poisson <- glm(hp ~ kml, data = cars, family = \"poisson\")\n\nsummary(glm.poisson) # klare overdisperion\n## \n## Call:\n## glm(formula = hp ~ kml, family = \"poisson\", data = cars)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -6.438  -2.238  -1.159   2.457  10.576  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) 3.894293   0.050262   77.48   <2e-16 ***\n## kml         0.081666   0.003414   23.92   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 958.27  on 31  degrees of freedom\n## Residual deviance: 426.59  on 30  degrees of freedom\n## AIC: 645.67\n## \n## Number of Fisher Scoring iterations: 4\n\n# deshalb quasipoisson\nglm.quasipoisson <- glm(hp ~ kml, data = cars, family = quasipoisson(link = log))\n\nsummary(glm.quasipoisson)\n## \n## Call:\n## glm(formula = hp ~ kml, family = quasipoisson(link = log), data = cars)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -6.438  -2.238  -1.159   2.457  10.576  \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.89429    0.19508  19.963  < 2e-16 ***\n## kml          0.08167    0.01325   6.164 8.82e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 15.06438)\n## \n##     Null deviance: 958.27  on 31  degrees of freedom\n## Residual deviance: 426.59  on 30  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4\n\n# visualisiere\nggplot2::ggplot(cars, aes(x = kml, y = hp)) +\n  geom_point(size = 8) +\n  geom_smooth(\n    method = \"glm\", method.args = list(family = \"poisson\"), se = F,\n    color = \"green\", size = 2\n  ) +\n  scale_x_continuous(limits = c(0, 35)) +\n  scale_y_continuous(limits = c(0, 400)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](StatKons4_Demo_GLM_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Rücktransformation meines Outputs für ein besseres Verständnis\nglm.quasi.back <- exp(coef(glm.quasipoisson))\n\n# für ein schönes ergebnis\nglm.quasi.back |>\n  broom::tidy() |>\n  knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n|names       |      x|\n|:-----------|------:|\n|(Intercept) | 49.121|\n|kml         |  1.085|\n:::\n\n```{.r .cell-code}\n\n# for more infos, also for posthoc tests\n# here: https://rcompanion.org/handbook/J_01.html\n```\n:::\n\n\n## logistische Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n############\n# logistische regression\n############\n\ncars <- mtcars\n\n# erstelle das modell\nglm.binar <- glm(vs ~ hp, data = cars, family = binomial(link = logit)) \n\n#achtung Model gibt Koeffizienten als logit() zurück\nsummary(glm.binar)\n## \n## Call:\n## glm(formula = vs ~ hp, family = binomial(link = logit), data = cars)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.12148  -0.20302  -0.01598   0.51173   1.20083  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  8.37802    3.21593   2.605  0.00918 **\n## hp          -0.06856    0.02740  -2.502  0.01234 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 43.860  on 31  degrees of freedom\n## Residual deviance: 16.838  on 30  degrees of freedom\n## AIC: 20.838\n## \n## Number of Fisher Scoring iterations: 7\n\n# überprüfe das modell\ncars$predicted <- predict(glm.binar, type = \"response\")\n\n# visualisiere\nggplot(cars, aes(x = hp, y = vs)) +    \n  geom_point(size = 8) +\n  geom_point(aes(y = predicted), shape  = 1, size = 6) +\n  guides(color = \"none\") +\n  geom_smooth(method = \"glm\", method.args = list(family = 'binomial'), \n              se = FALSE,\n              size = 2) +\n  # geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  mytheme\n```\n\n::: {.cell-output-display}\n![](StatKons4_Demo_GLM_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n#Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.binar$deviance,glm.binar$df.resid)  \n## [1] 0.9744718\n\n#Modellgüte (pseudo-R²)\n1 - (glm.binar$dev / glm.binar$null)  \n## [1] 0.6161072\n\n#Steilheit der Beziehung (relative Änderung der odds von x + 1 vs. x)\nexp(glm.binar$coefficients[2])\n##        hp \n## 0.9337368\n\n#LD50 (wieso negativ: weil zweiter koeffizient negative steigung hat)\nabs(glm.binar$coefficients[1]/glm.binar$coefficients[2])\n## (Intercept) \n##    122.1986\n\n# kreuztabelle (confusion matrix): fasse die ergebnisse aus predict und \n# \"gegebenheiten, realität\" zusammen\ntab1 <- table(cars$predicted>.5, cars$vs)\ndimnames(tab1) <- list(c(\"M:S-type\",\"M:V-type\"), c(\"T:S-type\", \"T:V-type\"))\ntab1 \n##          T:S-type T:V-type\n## M:S-type       15        2\n## M:V-type        3       12\n\nprop.table(tab1, 2) \n##           T:S-type  T:V-type\n## M:S-type 0.8333333 0.1428571\n## M:V-type 0.1666667 0.8571429\n#was könnt ihr daraus ablesen? Ist unser Modell genau?\n\n# Funktion die die logits in Wahrscheinlichkeiten transformiert\n# mehr infos hier: https://sebastiansauer.github.io/convert_logit2prob/\n# dies ist interessant, falls ihr mal ein kategorialer Prädiktor habt\nlogit2prob <- function(logit){\n  odds <- exp(logit)\n  prob <- odds / (1 + odds)\n  return(prob)\n}\n```\n:::\n\n\n## GAM's\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###########\n# LOESS & GAM\n###########\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point(size = 8) +\n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) +\n  geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) +\n  geom_smooth(method = \"glm\", size = 2, color = \"blue\", se = F) +\n  scale_x_continuous(limits = c(0, 35)) +\n  scale_y_continuous(limits = c(0, 400)) +\n  mytheme\n```\n\n::: {.cell-output-display}\n![](StatKons4_Demo_GLM_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point(size = 8) +\n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) +\n  # geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) +\n  geom_smooth(method = \"glm\", size = 2, color = \"grey\", se = F) +\n  scale_x_continuous(limits = c(0, 35)) +\n  scale_y_continuous(limits = c(0, 400)) +\n  mytheme\n```\n\n::: {.cell-output-display}\n![](StatKons4_Demo_GLM_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "StatKons4_Demo_GLM_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}