{
  "hash": "8c13be6a9605aa2d444a0021e64177c6",
  "result": {
    "markdown": "---\ndate: 2023-11-06\nlesson: Stat3\nthema: Lineare Modelle II\nindex: 1\nformat:\n  html:\n    code-tools:\n      source: true\nknitr:\n    opts_chunk:\n        collapse: false\n---\n\n\n# Stat3: Demo\n\n-   Download dieses Demoscript via \"\\</\\>Code\" (oben rechts)\n-   Datensatz *ipomopsis.csv*\n-   Datensatz *loyn.csv*\n\n## ANCOVA\n\nExperiment zur Fruchtproduktion (\"Fruit\") von Ipomopsis sp. in Abhängigkeit von der Beweidung (\"Grazing\" mit 2 Levels: \"Grazed\", \"Ungrazed\") und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: \"Root\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daten einlesen und anschauen\nlibrary(\"readr\")\n\ncompensation <- read_delim(\"datasets/stat1-4/ipomopsis.csv\", \",\",  col_types = cols(\"Grazing\" = \"f\"))\nhead(compensation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n   ...1  Root Fruit Grazing \n  <dbl> <dbl> <dbl> <fct>   \n1     1  6.22  59.8 Ungrazed\n2     2  6.49  61.0 Ungrazed\n3     3  4.92  14.7 Ungrazed\n4     4  5.13  19.3 Ungrazed\n5     5  5.42  34.2 Ungrazed\n6     6  5.36  35.5 Ungrazed\n```\n:::\n\n```{.r .cell-code}\nsummary(compensation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      ...1            Root            Fruit            Grazing  \n Min.   : 1.00   Min.   : 4.426   Min.   : 14.73   Ungrazed:20  \n 1st Qu.:10.75   1st Qu.: 6.083   1st Qu.: 41.15   Grazed  :20  \n Median :20.50   Median : 7.123   Median : 60.88                \n Mean   :20.50   Mean   : 7.181   Mean   : 59.41                \n 3rd Qu.:30.25   3rd Qu.: 8.510   3rd Qu.: 76.19                \n Max.   :40.00   Max.   :10.253   Max.   :116.05                \n```\n:::\n\n```{.r .cell-code}\n# Pflanzengrösse (\"Root\") vs. Fruchtproduktion (\"Fruit\")\nplot(Fruit ~ Root, data = compensation)\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n-> Je grösser die Pflanze, desto grösser ihre Fruchtproduktion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beweidung (\"Grazing\") vs. Fruchtroduktion (\"Fruit)\nboxplot(Fruit ~ Grazing, data = compensation)\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n-> In der beweideten Gruppe scheint die Fruchtproduktion grösser. Liegt dies an der Beweidung oder an unterschiedlichen Pflanzengrössen zwischen den Gruppen?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plotten der vollständigen Daten/Information\nlibrary(\"ggplot2\")\nggplot(compensation, aes(Root, Fruit, color = Grazing)) +\n  geom_point() +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n-> Die grössere Fruchtproduktion innerhalb der beweideten Gruppe scheint also ein Resultat von unterschiedlichen Pflanzengrössen zwischen den Gruppen zu sein und nicht an der Beweidung zu liegen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lineare Modelle definieren und anschauen\n\naoc.1 <- lm(Fruit ~ Root * Grazing, data = compensation) # Volles Modell mit Interaktion\nsummary.aov(aoc.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nRoot          1  16795   16795 359.968  < 2e-16 ***\nGrazing       1   5264    5264 112.832 1.21e-12 ***\nRoot:Grazing  1      5       5   0.103     0.75    \nResiduals    36   1680      47                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\naoc.2 <- lm(Fruit ~ Grazing + Root, data = compensation) # Finales Modell ohne die (nicht signifikante) Interaktion\nsummary.aov(aoc.2) # ANOVA-Tabelle\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value  Pr(>F)    \nGrazing      1   2910    2910   63.93 1.4e-09 ***\nRoot         1  19149   19149  420.62 < 2e-16 ***\nResiduals   37   1684      46                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nsummary(aoc.2) # Parameter-Tabelle\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Fruit ~ Grazing + Root, data = compensation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1920  -2.8224   0.3223   3.9144  17.3290 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -91.726      7.115  -12.89 2.96e-15 ***\nGrazingGrazed  -36.103      3.357  -10.75 6.11e-13 ***\nRoot            23.560      1.149   20.51  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.747 on 37 degrees of freedom\nMultiple R-squared:  0.9291,\tAdjusted R-squared:  0.9252 \nF-statistic: 242.3 on 2 and 37 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# Residualplots anschauen\npar(mfrow = c(2, 2))\nplot(aoc.2)\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n-> Das ANCOVA-Modell widerspiegelt die Zusammenhänge wie sie aufgrund der grafisch dargestellten Daten zu vermuten sind gut. Die Residual-Plots zeigen 3 Ausreisser (Beobachtungen 27, 34 und 37), welche \"aus der Reihe tanzen\".\n\n## Polynomische Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daten generieren und Modelle rechnen\npred <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24) # \"pred\" sei unsere unabhängige Variable\nresp <- c(12, 15, 10, 7, 2, 10, 12, 11, 13, 10, 9, 2, 4, 7, 13) # \"resp\" sei unsere abhängige Variable\n\nplot(pred, resp) # So sehen die Daten aus\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Modelle definieren\nlm.1 <- lm(resp ~ pred) # Einfaches lineares Modell\nlm.quad <- lm(resp ~ pred + I(pred^2)) # lineares Modell mit quadratischem Term\n\nsummary(lm.1) # Modell anschauen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = resp ~ pred)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0549 -1.7015  0.5654  2.0617  5.6406 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  12.2879     2.4472   5.021 0.000234 ***\npred         -0.1541     0.1092  -1.412 0.181538    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 13 degrees of freedom\nMultiple R-squared:  0.1329,\tAdjusted R-squared:  0.06622 \nF-statistic: 1.993 on 1 and 13 DF,  p-value: 0.1815\n```\n:::\n:::\n\n\n-> kein signifikanter Zusammenhang im einfachen linearen Modell und entsprechend kleines Bestimmtheitsmass (adj. R^2^ = 0.07)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm.quad) # lineares Modell mit quadratischem Term anschauen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = resp ~ pred + I(pred^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3866 -1.1018 -0.2027  1.3831  4.4211 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.239308   3.811746  -0.587  0.56777   \npred         1.330933   0.360105   3.696  0.00306 **\nI(pred^2)   -0.031587   0.007504  -4.209  0.00121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.555 on 12 degrees of freedom\nMultiple R-squared:  0.6499,\tAdjusted R-squared:  0.5915 \nF-statistic: 11.14 on 2 and 12 DF,  p-value: 0.001842\n```\n:::\n:::\n\n\n-> signifikanter Zusammenhang und viel besseres Bestimmtheitsmass (adj. R^2^ = 0.60)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modelle plotten\n\npar(mfrow = c(1, 2))\n\n# 1. lineares Modell\nplot(resp ~ pred, main = \"Lineares Modell\")\nabline(lm.1, col = \"blue\")\n\n# 2. quadratisches Modell\nplot(resp ~ pred, main = \"Quadratisches  Modell\")\nxv <- seq(0, 40, 0.1) # Input für Modellvoraussage via predict ()\nyv2 <- predict(lm.quad, list(pred = xv))\nlines(xv, yv2, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residualplots\npar(mfrow = c(2, 2))\nplot(lm.1, main = \"Lineares Modell\")\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(lm.quad, main = \"Quadratisches  Modell\")\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\n-> Die Plots sehen beim Modell mit quadratischem Term besser aus\n\n### Simulation Overfitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beispieldaten mit 6 Datenpunkten\ntest <- data.frame(\"x\" = c(1, 2, 3, 4, 5, 6), \"y\" = c(34, 21, 70, 47, 23, 45))\n\npar(mfrow = c(1, 1))\nplot(y ~ x, data = test)\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Zunehmend komplizierte Modelle (je komplizierter desto overfitteter) definieren\nlm.0 <- lm(y ~ 1, data = test) # Modell \nlm.1 <- lm(y ~ x, data = test)\nlm.2 <- lm(y ~ x + I(x^2), data = test)\nlm.3 <- lm(y ~ x + I(x^2) + I(x^3), data = test)\nlm.4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = test)\nlm.5 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5), data = test)\n\n# Summaries rechnen\nsmy.0 <- summary(lm.0)\nsmy.1 <- summary(lm.1)\nsmy.2 <- summary(lm.2)\nsmy.3 <- summary(lm.3)\nsmy.4 <- summary(lm.4)\nsmy.5 <- summary(lm.5)\n\n# R2 vergleichen\n\nsmy.0$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nsmy.1$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01242685\n```\n:::\n\n```{.r .cell-code}\nsmy.2$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1105981\n```\n:::\n\n```{.r .cell-code}\nsmy.3$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1697982\n```\n:::\n\n```{.r .cell-code}\nsmy.4$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.874639\n```\n:::\n\n```{.r .cell-code}\nsmy.5$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n-> R2 wird immer grösser, d.h. die Modelle werden immer besser. ;-)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modelle plotten\nxv <- seq(from = 0, to = 10, by = 0.1)\nplot(y ~ x, cex = 2, col = \"black\", lwd = 3, data = test)\n\nabline(h = lm.0$coefficients, col = \"darkgray\", lwd = 3 )\ntext(x = 1.5, y = 70, \"lm.0\", col = \"darkgray\")\nyv <- predict(lm.1, list(x = xv))\nlines(xv, yv, col = \"red\", lwd = 3)\ntext(x = 1.5, y = 67, \"lm.1\", col = \"red\")\nyv <- predict(lm.2, list(x = xv))\nlines(xv, yv, col = \"blue\", lwd = 3)\ntext(x = 1.5, y = 64, \"lm.2\", col = \"blue\")\nyv <- predict(lm.3, list(x = xv))\nlines(xv, yv, col = \"green\", lwd = 3)\ntext(x = 1.5, y = 61, \"lm.3\", col = \"green\")\nyv <- predict(lm.4, list(x = xv))\nlines(xv, yv, col = \"orange\", lwd = 3)\ntext(x = 1.5, y = 58, \"lm.4\", col = \"orange\")\nyv <- predict(lm.5, list(x = xv))\nlines(xv, yv, col = \"violet\", lwd = 3)\ntext(x = 1.5, y = 55, \"lm.5\", col = \"violet\")\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n-> Auch der optische Fit wird immer besser. Wir bestreiben jedoch Overfitting und Overfittig ist nicht gut: Denn, macht es Sinn, 6 Datenpunkte mit einem Modell mit 6 geschätzen Parametern zu fitten?? Die zunehmend komplizierten Modelle beschreiben lediglich die vorhandenen Datenpunkte, während sich die Voraussagekraft für weitere aus der Realität stammenden Daten zunehmend verschlechtert.\n\n## Multiple lineare Regression (basierend auf @logan2010, Beispiel 9A)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daten laden und anschauen\nlibrary(\"readr\")\n\nloyn <- read_delim(\"datasets/stat1-4/loyn.csv\", \",\")\nsummary(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      ...1           ABUND            AREA            YR.ISOL    \n Min.   : 1.00   Min.   : 1.50   Min.   :   0.10   Min.   :1890  \n 1st Qu.:14.75   1st Qu.:12.40   1st Qu.:   2.00   1st Qu.:1928  \n Median :28.50   Median :21.05   Median :   7.50   Median :1962  \n Mean   :28.50   Mean   :19.51   Mean   :  69.27   Mean   :1950  \n 3rd Qu.:42.25   3rd Qu.:28.30   3rd Qu.:  29.75   3rd Qu.:1966  \n Max.   :56.00   Max.   :39.60   Max.   :1771.00   Max.   :1976  \n      DIST            LDIST            GRAZE            ALT       \n Min.   :  26.0   Min.   :  26.0   Min.   :1.000   Min.   : 60.0  \n 1st Qu.:  93.0   1st Qu.: 158.2   1st Qu.:2.000   1st Qu.:120.0  \n Median : 234.0   Median : 338.5   Median :3.000   Median :140.0  \n Mean   : 240.4   Mean   : 733.3   Mean   :2.982   Mean   :146.2  \n 3rd Qu.: 333.2   3rd Qu.: 913.8   3rd Qu.:4.000   3rd Qu.:182.5  \n Max.   :1427.0   Max.   :4426.0   Max.   :5.000   Max.   :260.0  \n```\n:::\n:::\n\n\n### Korrelation zwischen den Prädiktoren\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wir setzen die Schwelle bei |0.7|\n\ncor <- cor(loyn[, 3:8]) # Korrelationen rechnen details siehe: \"?cor\"\n\n# Korrelationen Visualisieren (google: \"correlation plot r\"...)\nlibrary(\"corrplot\")\n\ncorrplot.mixed(cor, lower = \"ellipse\", upper = \"number\", order = \"AOE\")\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n-> Keine Korrelation ist \\>\\|0.7\\|, so können wir alle Prädiktoren \"behalten\". Aber es gilt zu beachten , dass GRAZE ziemlich stark \\|\\>0.6\\| mit YR.ISOL korreliert ist\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Volles Modell definieren\n\nnames(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"...1\"    \"ABUND\"   \"AREA\"    \"YR.ISOL\" \"DIST\"    \"LDIST\"   \"GRAZE\"  \n[8] \"ALT\"    \n```\n:::\n\n```{.r .cell-code}\nlm.1 <- lm(ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\n\nlibrary(\"car\")\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n-> Plot sieht zwar OK aus, aber mit 6 Prädiktoren für \\|\\<60\\| Beobachtungen ist das Modell wohl \"overfitted\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Andere Vatiante, um korrelierte Prädiktoren zu finden\nvif(lm.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n YR.ISOL     AREA     DIST    LDIST    GRAZE      ALT \n1.841657 1.337627 1.227387 1.255028 2.307661 1.574537 \n```\n:::\n:::\n\n\n### Modellvereinfachung\n\nSchrittweise die am wenigsten signifkanten Terme entfernen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Volles Modell anschauen\nsummary(lm.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + \n    ALT, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.6638  -4.6409  -0.0883   4.2858  20.1042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.097e+02  1.133e+02  -0.968  0.33791   \nYR.ISOL      6.693e-02  5.684e-02   1.177  0.24472   \nAREA         8.866e-04  4.657e-03   0.190  0.84980   \nDIST         3.811e-03  5.418e-03   0.703  0.48514   \nLDIST        1.418e-03  1.310e-03   1.082  0.28451   \nGRAZE       -3.447e+00  1.107e+00  -3.114  0.00308 **\nALT          4.772e-02  3.089e-02   1.545  0.12878   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.947 on 49 degrees of freedom\nMultiple R-squared:  0.5118,\tAdjusted R-squared:  0.452 \nF-statistic: 8.561 on 6 and 49 DF,  p-value: 2.24e-06\n```\n:::\n\n```{.r .cell-code}\nlm.2 <- update(lm.1, ~ . - AREA) # Prädiktor mit grösstem p-Wert entfernen\nanova(lm.1, lm.2) # Modelle vergleichen (falls signifikant, so müssten man den Prädiktor wieder ins Modell nehmen)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + ALT\nModel 2: ABUND ~ YR.ISOL + DIST + LDIST + GRAZE + ALT\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     49 3094.2                           \n2     50 3096.5 -1   -2.2886 0.0362 0.8498\n```\n:::\n\n```{.r .cell-code}\nsummary(lm.2) # Neues einfacheres Modell anschauen und Prädiktor mit grösstem p-Wert ausfindig machen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + DIST + LDIST + GRAZE + ALT, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.7240  -4.7245   0.0206   4.2698  20.0630 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.044e+02  1.089e+02  -0.959  0.34202   \nYR.ISOL      6.418e-02  5.445e-02   1.179  0.24409   \nDIST         3.884e-03  5.352e-03   0.726  0.47145   \nLDIST        1.440e-03  1.292e-03   1.115  0.27036   \nGRAZE       -3.500e+00  1.060e+00  -3.303  0.00177 **\nALT          4.964e-02  2.891e-02   1.717  0.09212 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.87 on 50 degrees of freedom\nMultiple R-squared:  0.5114,\tAdjusted R-squared:  0.4626 \nF-statistic: 10.47 on 5 and 50 DF,  p-value: 6.532e-07\n```\n:::\n\n```{.r .cell-code}\n# Oben beschriebene Schritte wiederholen bis nur noch signifikante Prädiktoren im Modell\nlm.3 <- update(lm.2, ~ . - DIST)\nanova(lm.2, lm.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: ABUND ~ YR.ISOL + DIST + LDIST + GRAZE + ALT\nModel 2: ABUND ~ YR.ISOL + LDIST + GRAZE + ALT\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     50 3096.5                           \n2     51 3129.1 -1   -32.609 0.5265 0.4714\n```\n:::\n\n```{.r .cell-code}\nsummary(lm.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + LDIST + GRAZE + ALT, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.4659  -4.8236   0.1506   4.9245  19.8891 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -99.587487 108.158382  -0.921 0.361513    \nYR.ISOL       0.062627   0.054157   1.156 0.252910    \nLDIST         0.001677   0.001245   1.347 0.184026    \nGRAZE        -3.699613   1.018706  -3.632 0.000653 ***\nALT           0.046485   0.028446   1.634 0.108386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.833 on 51 degrees of freedom\nMultiple R-squared:  0.5063,\tAdjusted R-squared:  0.4676 \nF-statistic: 13.07 on 4 and 51 DF,  p-value: 2.123e-07\n```\n:::\n\n```{.r .cell-code}\nlm.4 <- update(lm.3, ~ . - YR.ISOL)\nanova(lm.3, lm.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: ABUND ~ YR.ISOL + LDIST + GRAZE + ALT\nModel 2: ABUND ~ LDIST + GRAZE + ALT\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     51 3129.1                           \n2     52 3211.2 -1   -82.047 1.3372 0.2529\n```\n:::\n\n```{.r .cell-code}\nsummary(lm.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ LDIST + GRAZE + ALT, data = loyn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.155  -4.148  -0.503   4.649  18.588 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 25.289313   6.080034   4.159  0.00012 ***\nLDIST        0.001455   0.001234   1.179  0.24362    \nGRAZE       -4.430947   0.801206  -5.530 1.05e-06 ***\nALT          0.043565   0.028425   1.533  0.13144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.858 on 52 degrees of freedom\nMultiple R-squared:  0.4933,\tAdjusted R-squared:  0.4641 \nF-statistic: 16.88 on 3 and 52 DF,  p-value: 8.777e-08\n```\n:::\n\n```{.r .cell-code}\nlm.5 <- update(lm.4, ~ . - LDIST)\nanova(lm.4, lm.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: ABUND ~ LDIST + GRAZE + ALT\nModel 2: ABUND ~ GRAZE + ALT\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     52 3211.2                           \n2     53 3297.1 -1   -85.892 1.3909 0.2436\n```\n:::\n\n```{.r .cell-code}\nsummary(lm.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ GRAZE + ALT, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1677  -4.8261   0.0266   4.6944  19.1054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 28.55582    5.43245   5.257 2.67e-06 ***\nGRAZE       -4.59679    0.79167  -5.806 3.67e-07 ***\nALT          0.03191    0.02675   1.193    0.238    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.887 on 53 degrees of freedom\nMultiple R-squared:  0.4798,\tAdjusted R-squared:  0.4602 \nF-statistic: 24.44 on 2 and 53 DF,  p-value: 3.011e-08\n```\n:::\n\n```{.r .cell-code}\nlm.6 <- update(lm.5, ~ . - ALT)\nanova(lm.5, lm.6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: ABUND ~ GRAZE + ALT\nModel 2: ABUND ~ GRAZE\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     53 3297.1                           \n2     54 3385.6 -1   -88.519 1.4229 0.2382\n```\n:::\n\n```{.r .cell-code}\nsummary(lm.6) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1066  -5.4097   0.0934   4.4856  18.2747 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  34.3692     2.4095  14.264  < 2e-16 ***\nGRAZE        -4.9813     0.7259  -6.862  6.9e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.918 on 54 degrees of freedom\nMultiple R-squared:  0.4658,\tAdjusted R-squared:  0.4559 \nF-statistic: 47.09 on 1 and 54 DF,  p-value: 6.897e-09\n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(lm.6)\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n-> das minimal adäquate Modell enthält nur noch einen Prädiktor (GRAZE) und dessen Residualplots sehen ok aus.\n\n### Hierarchical partitioning\n\nWir können auch schauen wie bedeutsam die einzelnen Variablen sind:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"relaimpo\")\n\n# Lineares Modell definieren\nlm_1 <- lm(ABUND~YR.ISOL+ AREA +  DIST + LDIST + GRAZE + ALT, data = loyn)\n\n# Berechnen...\nmetrics <- calc.relimp(lm_1, type = c(\"lmg\", \"first\", \"last\",\"betasq\", \"pratt\"))\nIJ <- cbind(I = metrics$lmg, J = metrics$first - metrics$lmg, Total = metrics$first)\n\nIJ\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 I            J       Total\nYR.ISOL 0.11827368  0.135095338 0.253369016\nAREA    0.02359769  0.041923060 0.065520747\nDIST    0.02566349  0.030085610 0.055749104\nLDIST   0.01270789 -0.005112317 0.007595573\nGRAZE   0.25879164  0.207030145 0.465821782\nALT     0.07275743  0.076112117 0.148869552\n```\n:::\n:::\n\n\n-> auch hier sticht GRAZE heraus. (und an zweiter Stelle YR.ISOL, der mit GRAZE am stärksten korreliert ist)\n\n### Partial regressions\n\n\n::: {.cell}\n\n```{.r .cell-code}\navPlots(lm.1, ask = F)\n```\n\n::: {.cell-output-display}\n![](Statistik3_Demo_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Multimodel inference\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"MuMIn\")\n\nglobal.model <- lm(ABUND ~ YR.ISOL + AREA +  DIST + LDIST + GRAZE + ALT, data = loyn)\n\noptions(na.action = \"na.fail\")\n\nallmodels <- dredge(global.model)\nallmodels\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGlobal model call: lm(formula = ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + \n    ALT, data = loyn)\n---\nModel selection table \n       (Int)     ALT        ARE      DIS    GRA       LDI  YR.ISO df   logLik\n9    34.3700                             -4.981                    3 -194.315\n10   28.5600 0.03191                     -4.597                    4 -193.573\n41  -62.7500                             -4.440           0.04898  4 -193.886\n26   25.2900 0.04356                     -4.431 0.0014550          5 -192.834\n25   33.7400                             -4.967 0.0007979          4 -194.071\n13   33.2300                    0.003224 -4.858                    4 -194.102\n11   33.9000          1.969e-03          -4.871                    4 -194.201\n14   25.6300 0.03834            0.004970 -4.330                    5 -193.081\n42  -73.5800 0.03285                     -4.017           0.05143  5 -193.087\n58  -99.5900 0.04649                     -3.700 0.0016770 0.06263  6 -192.109\n57  -74.9700                             -4.359 0.0009527 0.05477  5 -193.538\n12   28.6100 0.03094  5.055e-04          -4.580                    5 -193.566\n43  -85.8200          3.243e-03          -4.133           0.06023  5 -193.596\n45  -68.9900                    0.003542 -4.277           0.05150  5 -193.626\n46  -85.3900 0.03990            0.005386 -3.679           0.05577  6 -192.503\n30   23.6500 0.04645            0.003635 -4.262 0.0012290          6 -192.583\n27   33.3100          1.892e-03          -4.861 0.0007836          5 -193.965\n29   33.0400                    0.002360 -4.880 0.0006227          5 -193.968\n15   32.8100          1.886e-03 0.003153 -4.755                    5 -193.997\n28   25.2400 0.04416 -2.645e-04          -4.438 0.0014660          6 -192.832\n44  -85.1900 0.02954  1.785e-03          -3.891           0.05737  6 -193.006\n16   25.6300 0.03828  2.748e-05 0.004967 -4.329                    6 -193.081\n62 -104.4000 0.04964            0.003884 -3.500 0.0014400 0.06418  7 -191.816\n59  -98.3600          3.274e-03          -4.049 0.0009602 0.06617  6 -193.239\n47  -91.6700          3.200e-03 0.003490 -3.977           0.06256  6 -193.341\n60 -106.3000 0.04414  1.117e-03          -3.627 0.0016430 0.06612  7 -192.076\n61  -77.0200                    0.002499 -4.260 0.0007691 0.05543  6 -193.420\n48  -93.8000 0.03722  1.342e-03 0.005240 -3.593           0.06013  7 -192.456\n31   32.6300          1.848e-03 0.002304 -4.779 0.0006130          6 -193.867\n32   23.5300 0.04762 -5.038e-04 0.003682 -4.274 0.0012470          7 -192.576\n63 -100.1000          3.239e-03 0.002430 -3.956 0.0007817 0.06669  7 -193.127\n64 -109.7000 0.04772  8.866e-04 0.003811 -3.447 0.0014180 0.06693  8 -191.795\n38 -325.3000 0.07807            0.011030                  0.16960  5 -198.542\n50 -355.4000 0.08742                            0.0027220 0.18470  5 -198.549\n54 -336.0000 0.08950            0.008464        0.0020850 0.17380  6 -197.343\n52 -363.9000 0.07191  5.500e-03                 0.0024540 0.19020  6 -197.858\n40 -336.7000 0.06363  5.431e-03 0.009912                  0.17650  6 -197.870\n56 -344.8000 0.07638  4.590e-03 0.007723        0.0019170 0.17930  7 -196.852\n34 -348.5000 0.07006                                      0.18350  4 -200.670\n36 -360.2000 0.05243  7.028e-03                           0.19060  5 -199.584\n35 -393.4000          1.036e-02                           0.21140  4 -201.103\n39 -380.7000          9.678e-03 0.007598                  0.20400  5 -200.121\n51 -402.6000          1.019e-02                 0.0014210 0.21560  5 -200.496\n55 -389.0000          9.682e-03 0.006312        0.0009292 0.20800  6 -199.883\n37 -377.6000                    0.008890                  0.20260  4 -202.444\n33 -392.3000                                              0.21120  3 -203.690\n49 -402.3000                                    0.0015230 0.21580  4 -203.054\n53 -385.8000                    0.007611        0.0009246 0.20660  5 -202.227\n6     1.1570 0.10280            0.013820                           4 -204.646\n22   -1.1380 0.11310            0.011680        0.0017790          5 -203.948\n8     2.2060 0.09511  3.112e-03 0.013240                           5 -204.467\n18    1.1530 0.11220                            0.0026530          4 -205.786\n2     5.5980 0.09515                                               3 -207.358\n24   -0.2323 0.10680  2.339e-03 0.011360        0.0016890          6 -203.846\n20    2.4160 0.10280  3.509e-03                 0.0024810          5 -205.568\n4     6.9990 0.08318  5.050e-03                                    4 -206.917\n7    16.3800          9.404e-03 0.010330                           4 -208.625\n3    18.8000          1.033e-02                                    3 -209.974\n5    16.7300                    0.011570                           3 -210.265\n1    19.5100                                                       2 -211.871\n19   18.1300          1.022e-02                 0.0009186          4 -209.789\n23   16.3100          9.404e-03 0.010120        0.0001591          5 -208.620\n21   16.6700                    0.011360        0.0001598          4 -210.260\n17   18.7700                                    0.0010210          3 -211.658\n    AICc delta weight\n9  395.1  0.00  0.145\n10 395.9  0.84  0.095\n41 396.6  1.46  0.070\n26 396.9  1.78  0.059\n25 396.9  1.84  0.058\n13 397.0  1.90  0.056\n11 397.2  2.10  0.051\n14 397.4  2.27  0.046\n42 397.4  2.28  0.046\n58 397.9  2.84  0.035\n57 398.3  3.19  0.029\n12 398.3  3.24  0.029\n43 398.4  3.30  0.028\n45 398.5  3.36  0.027\n46 398.7  3.63  0.024\n30 398.9  3.79  0.022\n27 399.1  4.04  0.019\n29 399.1  4.04  0.019\n15 399.2  4.10  0.019\n28 399.4  4.29  0.017\n44 399.7  4.63  0.014\n16 399.9  4.79  0.013\n62 400.0  4.87  0.013\n59 400.2  5.10  0.011\n47 400.4  5.31  0.010\n60 400.5  5.40  0.010\n61 400.6  5.46  0.009\n48 401.2  6.15  0.007\n31 401.4  6.36  0.006\n32 401.5  6.39  0.006\n63 402.6  7.50  0.003\n64 402.7  7.56  0.003\n38 408.3 13.19  0.000\n50 408.3 13.21  0.000\n54 408.4 13.31  0.000\n52 409.4 14.34  0.000\n40 409.5 14.36  0.000\n56 410.0 14.95  0.000\n34 410.1 15.03  0.000\n36 410.4 15.28  0.000\n35 411.0 15.90  0.000\n39 411.4 16.35  0.000\n51 412.2 17.10  0.000\n55 413.5 18.39  0.000\n37 413.7 18.58  0.000\n33 413.8 18.75  0.000\n49 414.9 19.80  0.000\n53 415.7 20.56  0.000\n6  418.1 22.99  0.000\n22 419.1 24.01  0.000\n8  420.1 25.04  0.000\n18 420.4 25.27  0.000\n2  421.2 26.09  0.000\n24 421.4 26.32  0.000\n20 422.3 27.25  0.000\n4  422.6 27.53  0.000\n7  426.0 30.94  0.000\n3  426.4 31.32  0.000\n5  427.0 31.90  0.000\n1  428.0 32.88  0.000\n19 428.4 33.27  0.000\n23 428.4 33.35  0.000\n21 429.3 34.21  0.000\n17 429.8 34.69  0.000\nModels ranked by AICc(x) \n```\n:::\n\n```{.r .cell-code}\n# Variable importance\nsw(allmodels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     GRAZE ALT  YR.ISOL LDIST DIST AREA\nSum of weights:      1.00  0.44 0.34    0.32  0.28 0.25\nN containing models:   32    32   32      32    32   32\n```\n:::\n:::\n\n\n-> Auch mit dieser Sichtweise ist GRAZE der wichtigste Prädiktor\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model averaging\navgmodel <- model.avg(allmodels, subset = TRUE)\nsummary(avgmodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nmodel.avg(object = allmodels, subset = TRUE)\n\nComponent model call: \nlm(formula = ABUND ~ <64 unique rhs>, data = loyn)\n\nComponent models: \n       df  logLik   AICc delta weight\n4       3 -194.31 395.09  0.00   0.14\n14      4 -193.57 395.93  0.84   0.10\n46      4 -193.89 396.56  1.46   0.07\n145     5 -192.83 396.87  1.78   0.06\n45      4 -194.07 396.93  1.84   0.06\n34      4 -194.10 396.99  1.90   0.06\n24      4 -194.20 397.19  2.10   0.05\n134     5 -193.08 397.36  2.27   0.05\n146     5 -193.09 397.37  2.28   0.05\n1456    6 -192.11 397.93  2.84   0.03\n456     5 -193.54 398.28  3.19   0.03\n124     5 -193.57 398.33  3.24   0.03\n246     5 -193.60 398.39  3.30   0.03\n346     5 -193.63 398.45  3.36   0.03\n1346    6 -192.50 398.72  3.63   0.02\n1345    6 -192.58 398.88  3.79   0.02\n245     5 -193.97 399.13  4.04   0.02\n345     5 -193.97 399.14  4.04   0.02\n234     5 -194.00 399.19  4.10   0.02\n1245    6 -192.83 399.38  4.29   0.02\n1246    6 -193.01 399.73  4.63   0.01\n1234    6 -193.08 399.88  4.79   0.01\n13456   7 -191.82 399.96  4.87   0.01\n2456    6 -193.24 400.19  5.10   0.01\n2346    6 -193.34 400.40  5.31   0.01\n12456   7 -192.08 400.49  5.40   0.01\n3456    6 -193.42 400.55  5.46   0.01\n12346   7 -192.46 401.25  6.15   0.01\n2345    6 -193.87 401.45  6.36   0.01\n12345   7 -192.58 401.49  6.39   0.01\n23456   7 -193.13 402.59  7.50   0.00\n123456  8 -191.79 402.65  7.56   0.00\n136     5 -198.54 408.28 13.19   0.00\n156     5 -198.55 408.30 13.21   0.00\n1356    6 -197.34 408.40 13.31   0.00\n1256    6 -197.86 409.43 14.34   0.00\n1236    6 -197.87 409.45 14.36   0.00\n12356   7 -196.85 410.04 14.95   0.00\n16      4 -200.67 410.13 15.03   0.00\n126     5 -199.58 410.37 15.28   0.00\n26      4 -201.10 410.99 15.90   0.00\n236     5 -200.12 411.44 16.35   0.00\n256     5 -200.50 412.19 17.10   0.00\n2356    6 -199.88 413.48 18.39   0.00\n36      4 -202.44 413.67 18.58   0.00\n6       3 -203.69 413.84 18.75   0.00\n56      4 -203.05 414.89 19.80   0.00\n356     5 -202.23 415.65 20.56   0.00\n13      4 -204.65 418.08 22.99   0.00\n135     5 -203.95 419.10 24.01   0.00\n123     5 -204.47 420.13 25.04   0.00\n15      4 -205.79 420.36 25.27   0.00\n1       3 -207.36 421.18 26.09   0.00\n1235    6 -203.85 421.41 26.32   0.00\n125     5 -205.57 422.34 27.25   0.00\n12      4 -206.92 422.62 27.53   0.00\n23      4 -208.63 426.04 30.94   0.00\n2       3 -209.97 426.41 31.32   0.00\n3       3 -210.27 426.99 31.90   0.00\n(Null)  2 -211.87 427.97 32.88   0.00\n25      4 -209.79 428.36 33.27   0.00\n235     5 -208.62 428.44 33.35   0.00\n35      4 -210.26 429.30 34.21   0.00\n5       3 -211.66 429.78 34.69   0.00\n\nTerm codes: \n    ALT    AREA    DIST   GRAZE   LDIST YR.ISOL \n      1       2       3       4       5       6 \n\nModel-averaged coefficients:  \n(full average) \n              Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept) -7.5601112 84.0460963  85.2129010   0.089    0.929    \nGRAZE       -4.4923671  0.9653718   0.9836120   4.567  4.9e-06 ***\nALT          0.0168833  0.0269646   0.0272737   0.619    0.536    \nYR.ISOL      0.0192028  0.0420833   0.0426707   0.450    0.653    \nLDIST        0.0003700  0.0009029   0.0009158   0.404    0.686    \nDIST         0.0010880  0.0033129   0.0033689   0.323    0.747    \nAREA         0.0004120  0.0023720   0.0024205   0.170    0.865    \n \n(conditional average) \n             Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept) -7.560111  84.046096   85.212901   0.089    0.929    \nGRAZE       -4.497801   0.953221    0.971711   4.629  3.7e-06 ***\nALT          0.038392   0.028768    0.029424   1.305    0.192    \nYR.ISOL      0.056407   0.055710    0.057008   0.989    0.322    \nLDIST        0.001152   0.001280    0.001308   0.881    0.378    \nDIST         0.003833   0.005305    0.005428   0.706    0.480    \nAREA         0.001673   0.004554    0.004656   0.359    0.719    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n",
    "supporting": [
      "Statistik3_Demo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}