{
  "hash": "8477483b0755b55dc0d12ad9e6adeeba",
  "result": {
    "markdown": "---\ndate: 2023-11-07\nlesson: Stat4\nthema: Komplexere Regressionsmethoden\nindex: 1\nformat:\n  html:\n    code-tools:\n      source: true\nknitr:\n    opts_chunk:\n        collapse: false\n---\n\n\n# Stat4: Demo\n\n-   Download dieses Demoscript via \"\\</\\>Code\" (oben rechts)\n-   Datensatz *loyn.csv*\n\n## von LMs zu GLMs\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daten erstellen und anschauen\ntemp <- c(10, 12, 16, 20, 24, 25, 30, 33, 37)\nbesucher <- c(40, 12, 50, 500, 400, 900, 1500, 900, 2000)\nstrand <- data.frame(\"Temperatur\" = temp, \"Besucher\" = besucher)\n\nplot(besucher ~ temp, data = strand)\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modell definieren und anschauen\nlm.strand <- lm(Besucher ~ Temperatur, data = strand)\nsummary(lm.strand)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Besucher ~ Temperatur, data = strand)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-476.41 -176.89   55.59  218.82  353.11 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.7 on 7 degrees of freedom\nMultiple R-squared:  0.8244,\tAdjusted R-squared:  0.7993 \nF-statistic: 32.86 on 1 and 7 DF,  p-value: 0.0007115\n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(lm.strand)\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\nxv <- seq(0, 40, by = .1)\nyv <- predict(lm.strand, list(Temperatur = xv))\nplot(strand$Temperatur, strand$Besucher, xlim = c(0, 40))\nlines(xv, yv, lwd = 3, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# GLMs definieren und anschauen\nglm.gaussian <- glm(Besucher ~ Temperatur, family = \"gaussian\", data = strand) # ist dasselbe wie ein LM\nglm.poisson <- glm(Besucher ~ Temperatur, family = \"poisson\", data = strand) # Poisson passt besser zu den Daten \n\nsummary(glm.gaussian)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = \"gaussian\", data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-476.41  -176.89    55.59   218.82   353.11  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 97138.03)\n\n    Null deviance: 3871444  on 8  degrees of freedom\nResidual deviance:  679966  on 7  degrees of freedom\nAIC: 132.63\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n\n```{.r .cell-code}\nsummary(glm.poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = \"poisson\", data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-13.577  -12.787   -4.491    9.515   15.488  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 3.500301   0.056920   61.49   <2e-16 ***\nTemperatur  0.112817   0.001821   61.97   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: 1185.1\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\nRücktranformation der Werte auf die orginale Skale (Hier Exponentialfunktion da family=possion als Link-Funktion den natürlichen Logarithmus (log) verwendet) Besucher = exp(3.50 + 0.11 Temperatur/°C)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.poisson$coefficients # So kann man auf die Coefficients des Modells \"extrahieren\" und dann mit[] auswählen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)  Temperatur \n  3.5003009   0.1128168 \n```\n:::\n\n```{.r .cell-code}\nexp(glm.poisson$coefficients[1]) # Anzahl besucher bei 0°C\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   33.12542 \n```\n:::\n\n```{.r .cell-code}\nexp(glm.poisson$coefficients[1] + 30 * glm.poisson$coefficients[2]) # Anzahl besucher bei 30°C\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   977.3102 \n```\n:::\n\n```{.r .cell-code}\n# Test Overdispersion\nlibrary(\"AER\")\ndispersiontest(glm.poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOverdispersion test\n\ndata:  glm.poisson\nz = 3.8576, p-value = 5.726e-05\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  116.5467 \n```\n:::\n:::\n\n-> Es liegt Overdispersion vor. Darum quasipoisson wählen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.quasipoisson <- glm(Besucher ~ Temperatur, family = \"quasipoisson\", data = strand)\nsummary(glm.quasipoisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = \"quasipoisson\", \n    data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-13.577  -12.787   -4.491    9.515   15.488  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  3.50030    0.69639   5.026  0.00152 **\nTemperatur   0.11282    0.02227   5.065  0.00146 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 149.6826)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(glm.gaussian, main = \"glm.gaussian\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(glm.poisson, main = \"glm.poisson\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(glm.quasipoisson, main = \"glm.quasipoisson\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n-> Die Outputs von glm.poisson und glm.quasipoisson sind bis auf die p-Werte identisch.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\nplot(strand$Temperatur, strand$Besucher, xlim = c(0, 40))\nxv <- seq(0, 40, by = .1)\n\nyv <- predict(lm.strand, list(Temperatur = xv))\nlines(xv, yv, lwd = 3, col = \"blue\")\ntext(x = 5, y = 1500, \"lm = gaussian\", col = \"blue\")\n\nyv2 <- predict(glm.poisson, list(Temperatur = xv))\nlines(xv, exp(yv2), lwd = 3, col = \"red\")\ntext(x = 5, y = 1300, \"poisson\", col = \"red\")\n\nyv3 <- predict(glm.quasipoisson, list(Temperatur = xv))\nlines(xv, exp(yv3), lwd = 3, col = \"green\", lty=2)\ntext(x = 5, y = 1100, \"quasipoisson\", col = \"green\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Logistische Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbathing <- data.frame(\n  \"temperature\" = c(1, 2, 5, 9, 14, 14, 15, 19, 22, 24, 25, 26, 27, 28, 29),\n  \"bathing\" = c(0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1)\n)\nplot(bathing ~ temperature, data = bathing)\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Logistisches Modell definieren\nglm.1 <- glm(bathing ~ temperature, family = \"binomial\", data = bathing)\nsummary(glm.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = bathing ~ temperature, family = \"binomial\", data = bathing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7408  -0.4723  -0.1057   0.5123   1.8615  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -5.4652     2.8501  -1.918   0.0552 .\ntemperature   0.2805     0.1350   2.077   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20.728  on 14  degrees of freedom\nResidual deviance: 10.829  on 13  degrees of freedom\nAIC: 14.829\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n\n```{.r .cell-code}\n# Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.1$deviance, glm.1$df.resid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6251679\n```\n:::\n\n```{.r .cell-code}\n# Modellgüte (pseudo-R²)\n1 - (glm.1$dev / glm.1$null)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4775749\n```\n:::\n\n```{.r .cell-code}\n# Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x)\nexp(glm.1$coefficients[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntemperature \n   1.323807 \n```\n:::\n\n```{.r .cell-code}\n# LD50 (also hier: Temperatur, bei der 50% der Touristen baden)\n-glm.1$coefficients[1] / glm.1$coefficients[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   19.48311 \n```\n:::\n\n```{.r .cell-code}\n# Vorhersagen\npredicted <- predict(glm.1, type = \"response\")\n\n# Konfusionsmatrix\nkm <- table(bathing$bathing, predicted > 0.5)\nkm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n    FALSE TRUE\n  0     7    1\n  1     1    6\n```\n:::\n\n```{.r .cell-code}\n# Missklassifizierungsrate\n1 - sum(diag(km) / sum(km))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1333333\n```\n:::\n\n```{.r .cell-code}\n# Plotting\nxs <- seq(0, 30, l = 1000)\nmodel.predict <- predict(glm.1,\n  type = \"response\", se = TRUE,\n  newdata = data.frame(temperature = xs)\n)\n\nplot(bathing ~ temperature,\n  xlab = \"Temperature (°C)\",\n  ylab = \"% Bathing\", pch = 16, col = \"red\", data = bathing\n)\nlines(model.predict$fit ~ xs, type = \"l\")\nlines(model.predict$fit + model.predict$se.fit ~ xs, type = \"l\", lty = 2) # Standardfehler hinzufügen\nlines(model.predict$fit - model.predict$se.fit ~ xs, type = \"l\", lty = 2) # Standardfehler hinzufügen\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n## Nicht-lineare Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"AICcmodavg\")\nlibrary(\"nlstools\")\nlibrary(\"readr\")\n\nloyn <- read_delim(\"datasets/stat1-4/loyn.csv\", \",\")\n\n# Selbstdefinierte Funktion, hier Potenzfunktion\npower.model <- nls(ABUND ~ c * AREA^z, start = (list(c = 1, z = 0)), data = loyn)\nsummary(power.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFormula: ABUND ~ c * AREA^z\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \nc 13.39418    1.30721  10.246 2.87e-14 ***\nz  0.16010    0.02438   6.566 2.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.995 on 54 degrees of freedom\n\nNumber of iterations to convergence: 12 \nAchieved convergence tolerance: 7.124e-06\n```\n:::\n\n```{.r .cell-code}\nAICc(power.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 396.1723\n```\n:::\n\n```{.r .cell-code}\n# Modeldiagnostik (in nlstools)\nplot(nlsResiduals(power.model))\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Vordefinierte \"Selbststartfunktionen\"#\n?selfStart\nlogistic.model <- nls(ABUND ~ SSlogis(AREA, Asym, xmid, scal), data = loyn)\nsummary(logistic.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFormula: ABUND ~ SSlogis(AREA, Asym, xmid, scal)\n\nParameters:\n     Estimate Std. Error t value Pr(>|t|)    \nAsym   31.306      2.207  14.182  < 2e-16 ***\nxmid    6.501      2.278   2.854  0.00614 ** \nscal    9.880      3.152   3.135  0.00280 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.274 on 53 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 4.371e-06\n```\n:::\n\n```{.r .cell-code}\nAICc(logistic.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 386.8643\n```\n:::\n\n```{.r .cell-code}\n# Modeldiagnostik (in nlstools)\nplot(nlsResiduals(logistic.model))\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Visualisierung\nplot(ABUND ~ AREA, data = loyn)\npar(mfrow = c(1, 1))\nxv <- seq(0, 2000, 0.01)\n\n# 1. Potenzfunktion\nyv1 <- predict(power.model, list(AREA = xv))\nlines(xv, yv1, col = \"green\")\ntext(x = 1000, y = 20, \"power.model\", col = \"green\")\n\n# 2. Logistische Funktion\nyv2 <- predict(logistic.model, list(AREA = xv))\nlines(xv, yv2, col = \"blue\")\ntext(x = 1000, y = 15, \"logistic.model\", col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Visualisierung mit logarithmiert y-Achse\nplot(ABUND ~ log10(AREA), data = loyn)\n\n# 1. Potenzfunktion\nyv1 <- predict(power.model, list(AREA = xv))\nlines(log10(xv), yv1, col = \"green\")\ntext(x = 2, y = 20, \"power.model\", col = \"green\")\n\n# 2. Logistische Funktion\nyv2 <- predict(logistic.model, list(AREA = xv))\nlines(log10(xv), yv2, col = \"blue\")\ntext(x = 2, y = 15, \"logistic.model\", col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-12-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# Model seletcion zwischen den nicht-lineraen Modelen\ncand.models <- list()\ncand.models[[1]] <- power.model\ncand.models[[2]] <- logistic.model\n\nModnames <- c(\"Power\", \"Logistic\")\n\naictab(cand.set = cand.models, modnames = Modnames)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel selection based on AICc:\n\n         K   AICc Delta_AICc AICcWt Cum.Wt      LL\nLogistic 4 386.86       0.00   0.99   0.99 -189.04\nPower    3 396.17       9.31   0.01   1.00 -194.86\n```\n:::\n:::\n\n-> Gemäss AIC passt also das \"logistic model\" besser als das \"power model\"\n\n## Smoother\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn$log_AREA <- log10(loyn$AREA)\nplot(ABUND ~ log_AREA, data = loyn)\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.25), lwd = 2, col = \"red\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.5), lwd = 2, col = \"blue\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 1), lwd = 2, col = \"green\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## GAMs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"mgcv\")\n\ngam.1 <- gam(ABUND ~ s(log_AREA), data = loyn)\ngam.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nEstimated degrees of freedom:\n2.88  total = 3.88 \n\nGCV score: 52.145     \n```\n:::\n\n```{.r .cell-code}\nsummary(gam.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  19.5143     0.9309   20.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df     F p-value    \ns(log_AREA) 2.884  3.628 21.14  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.579   Deviance explained = 60.1%\nGCV = 52.145  Scale est. = 48.529    n = 56\n```\n:::\n\n```{.r .cell-code}\n# Gam mit den anderen Modellen per AIC vergleichen\nAICc(gam.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 383.2109\n```\n:::\n\n```{.r .cell-code}\nAICc(logistic.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 386.8643\n```\n:::\n\n```{.r .cell-code}\nAICc(power.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 396.1723\n```\n:::\n\n```{.r .cell-code}\n# Alle Modelle anschauen\n\n# Visualisierung mit logarithmiert y-Achse (gleicher code wie weiter oben)\nplot(ABUND ~ log10(AREA), data = loyn)\n# 1. Potenzfunktion\nyv1 <- predict(power.model, list(AREA = xv))\nlines(log10(xv), yv1, col = \"green\")\ntext(x = 2, y = 20, \"power.model\", col = \"green\")\n# 2. Logistische Funktion\nyv2 <- predict(logistic.model, list(AREA = xv))\nlines(log10(xv), yv2, col = \"blue\")\ntext(x = 2, y = 15, \"logistic.model\", col = \"blue\")\n# 3. Gam model\nxv2 <- seq(-2, 4, by = 0.1)\nyv <- predict(gam.1, list(log_AREA = xv2))\nlines(xv2, yv, lwd = 2, col = \"red\")\ntext(x = 0, y = 20, \"GAM\", col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Statistik4_Demo_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n-> Sowohl gemäss AIC als auch optisch auf dem Plot scheint das GAM Model am besten zu den Daten zu passen  ",
    "supporting": [
      "Statistik4_Demo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}