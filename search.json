[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods HS24",
    "section": "",
    "text": "Willkommen\nDas Modul „Research Methods” vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen” auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen”. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverarbeitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "PrePro.html",
    "href": "PrePro.html",
    "title": "Pre-Processing",
    "section": "",
    "text": "Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on” Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der lesson sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.\nDie Lesson vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape”, „split-apply-combine”). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser lesson lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.",
    "crumbs": [
      "Pre-Processing"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Vorbereitung.html",
    "href": "prepro/Prepro1_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "R ist ohne Zusatzpackete, sogenannte “Packages” nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Allerdings prüft R dabei nicht, ob das Package bereits vorhanden ist oder nicht: Auch bereits installierte Packages werden nochmal installiert, was unter Umständen ziemlich unpraktisch sein kann.\nAlternativ zu install.packages können Packages auch mittels der Funktion p_install installiert werden. In der Funktion p_install wird zuerst geprüft, ob das Package vorhanden ist. Ist das jeweilige Package vorhanden, wird auf eine Installation verzichtet (bei force = FALSE).\nDie Funktion p_install wird von dem Package pacman zur Verfügung gestellt. Dieses Package muss initial ganz klassisch mit install.packages installiert werden. Um die Funktion p_install aus pacman zu verwenden, muss das Package nach der installation mittels library(\"pacman\") geladen werden.\n\n\n# so werden packages klassischerweise installiert:\ninstall.packages(\"lubridate\")\n\n# so werden sie in die aktuelle Session geladen:\nlibrary(lubridate)\n\n# nun kann eine Funktion aus dem geladenen Package verwendet werden\n# (die Funktion \"now()\" war vorher nicht verfübar)\nnow()\n\n# so werden packages mit \"pacman installiert:\nlibrary(pacman)\np_install(\"dplyr\", character.only = TRUE, force = FALSE)\n\n\n\n\n\n\n\nWichtig\n\n\n\nDie häufigste Verwirrung von Einsteigern liegt in der Verwendung von Packages. Dieses Kapitel unbedingt vormerken und bei Bedarf nochmal lesen.\n\n\nIm Rahmen von Prepro 1 - 3 werden wir folgende Packages brauchen: dplyr, ggplot2, lubridate, readr und tidyr. Wir empfehlen, diese bereits vor der ersten Lektion mit pacman zu installieren (s.u.).\n\nlibrary(pacman)\np_install(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"tidyr\", \n  character.only = TRUE,  force = FALSE)\n  \n# character.only = TRUE: die Packages werden in Quotes angegeben\n# force = FALSE:         die Packages werden nur installiert, \n#                        wenn noch nicht vorhanden",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Demo.html",
    "href": "prepro/Prepro1_Demo.html",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "Datentypen",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Prepro 1: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#footnotes",
    "href": "prepro/Prepro1_Demo.html#footnotes",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "ordered = T kann nur bei der Funktion factor() spezifiziert werden, nicht bei as.factor(). Ansonsten sind factor() und as.factor() sehr ähnlich.↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Prepro 1: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html",
    "href": "prepro/Prepro1_Uebung.html",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Arbeiten mit RStudio “Project”\nWir empfehlen die Verwendung von “Projects” innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr hier.\nDas Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "href": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (setwd()). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt)\nAutomatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session\nFestlegen verschiedener projektspezifischer Optionen\nVerwendung von Versionsverwaltungssystemen (z.B. git)\n\n\n\n\n\n\n\nImportant 3.1: Prüfungsrelevant\n\n\n\nDie korrekte Verwendung von RStudio Projects und relativen Pfaden wird an der praktischen Prüfung vorausgesetzt!",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nErstelle eine data.frame mit nachstehenden Daten.\n\n\nMusterlösung\ndf &lt;- data.frame(\n  Tierart = c(\"Fuchs\", \"Bär\", \"Hase\", \"Elch\"),\n  Anzahl = c(2, 5, 1, 3),\n  Gewicht = c(4.4, 40.3, 1.1, 120),\n  Geschlecht = c(\"m\", \"f\", \"m\", \"m\"),\n  Beschreibung = c(\"Rötlich\", \"Braun, gross\", \"klein, mit langen Ohren\", \"Lange Beine, Schaufelgeweih\")\n)\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\n\n\nBär\n5\n40.3\nf\nBraun, gross\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nWas für Datentypen wurden in der letzten Aufgabe automatisch angenommen? Ermittle diese mit str() und prüfe, ob diese sinnvoll sind und wandle um wo nötig.\n\n\nMusterlösung\nstr(df)\n## 'data.frame':    4 obs. of  5 variables:\n##  $ Tierart     : chr  \"Fuchs\" \"Bär\" \"Hase\" \"Elch\"\n##  $ Anzahl      : num  2 5 1 3\n##  $ Gewicht     : num  4.4 40.3 1.1 120\n##  $ Geschlecht  : chr  \"m\" \"f\" \"m\" \"m\"\n##  $ Beschreibung: chr  \"Rötlich\" \"Braun, gross\" \"klein, mit langen Ohren\" \"Lange Beine, Schaufelgeweih\"\ntypeof(df$Anzahl)\n## [1] \"double\"\n# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`.\n\ndf$Anzahl &lt;- as.integer(df$Anzahl)",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNutze die Spalte Gewicht, um die Tiere in 3 Gewichtskategorien einzuteilen:\n\nleicht: &lt; 5kg\nmittel: 5 - 100 kg\nschwer: &gt; 100kg\n\n\n\nMusterlösung\ndf$Gewichtsklasse[df$Gewicht &gt; 100] &lt;- \"schwer\"\ndf$Gewichtsklasse[df$Gewicht &lt;= 100 & df$Gewicht &gt; 5] &lt;- \"mittel\"\ndf$Gewichtsklasse[df$Gewicht &lt;= 5] &lt;- \"leicht\"\n\n\nDas Resultat:\n\n\n\n\n\n\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\nGewichtsklasse\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\nleicht\n\n\nBär\n5\n40.3\nf\nBraun, gross\nmittel\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\nleicht\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih\nschwer",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nAuf Moodle findest du ein Zip-File mit dem Namen prepro.zip. Lade das File herunter und entpacke es in deinem Projektordner. Importiere die Datei weather.csv. Falls du dafür das RStudio GUI verwendest, speichere den Import-Befehl in deinem R-Script ab. Bitte verwende einen relativen Pfad (also kein Pfad, der mit C:/, ~/ o.ä. beginnt).)\n\n\n\n\n\n\nNote 3.1\n\n\n\nWir nutzen readr, um csvs zu importieren, und verwenden die Funktion read_delim (mit underscore) als alternative zu read.csv oder read.delim (mit Punkt). Das ist eine persönliche Präferenz1, es ist euch überlassen, welche Funktion ihr verwendet. Beachtet, dass die beiden Funktionen leicht andere Parameter erwarten.\n\n\n\n\nMusterlösung\nlibrary(\"readr\")\n\n\nwetter &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nSchau dir die Rückmeldung von read_delim() an. Sind die Daten korrekt interpretiert worden?\n\n\nMusterlösung\n# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es\n# sich offensichtlich um Zeitangaben.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nDie Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() ein und spezifiziere sowohl format wie auch tz.\n\n\n\n\n\n\nTipp\n\n\n\n\nWenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe metadata.csv)\nas.POSIXct erwartet character: Wenn du eine Fehlermeldung hast die 'origin' must be supplied (o.ä) heisst, hast du der Funktion vermutlich einen Numeric übergeben.\n\n\n\n\n\nMusterlösung\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\", tz = \"UTC\")\n\n\n\n\n\nDie neue Tabelle sollte so aussehen\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nErstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte sowie eine geeignete Funktion aus lubridate.\n\n\nMusterlösung\nlibrary(\"lubridate\")\n\nwetter$wochentag &lt;- wday(wetter$time, label = T)\nwetter$kw &lt;- week(wetter$time)\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa\n1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nErstelle eine neue Spalte basierend auf den Temperaturwerten mit der Einteilung “kalt” (unter Null Grad) und “warm” (über Null Grad)\n\n\nMusterlösung\nwetter$temp_kat[wetter$tre200h0 &gt; 0] &lt;- \"warm\"\nwetter$temp_kat[wetter$tre200h0 &lt;= 0] &lt;- \"kalt\"\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\ntemp_kat\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa\n1\nkalt\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa\n1\nkalt\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa\n1\nkalt\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa\n1\nkalt\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa\n1\nkalt\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa\n1\nkalt\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa\n1\nkalt\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa\n1\nkalt\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa\n1\nkalt\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa\n1\nkalt",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#footnotes",
    "href": "prepro/Prepro1_Uebung.html#footnotes",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Vorteile von read_delim gegenüber read.csv: https://stackoverflow.com/a/60374974/4139249↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html",
    "href": "prepro/Prepro2_Demo.html",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "Piping\nGegeben ist ein character string (diary). Wir wollen aus diesem Text die Temperaturangabe aus dem String extrahieren, danach den Wert von Kelvin in Celsius nach der folgenden Formel umwandeln und zum Schluss den Mittelwert über all diese Werte berechnen.\n\\[°C = K - 273.15\\]\ndiary &lt;- c(\n  \"The temperature is 310° Kelvin\",\n  \"The temperature is 322° Kelvin\",\n  \"The temperature is 410° Kelvin\"\n)\n\ndiary\n## [1] \"The temperature is 310° Kelvin\" \"The temperature is 322° Kelvin\"\n## [3] \"The temperature is 410° Kelvin\"\nDazu brauchen wir die Funktion substr(), welche aus einem character einen Teil “raus schnipseln” kann.\n# Wenn die Buchstaben einzelne _Elemente_ eines Vektors wären, würden wir diese\n# folgendermassen subsetten:\n\ncharvec1 &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\")\ncharvec1[4:6]\n## [1] \"d\" \"e\" \"f\"\n\n# Aber wenn diese in einem einzigen character gespeichert sind, brauchen wir substr:\ncharvec2 &lt;- \"abcdefgh\"\nsubstr(charvec2, 4, 6)\n## [1] \"def\"\nZudem nutzen wir eine Hilfsfunktion subtrahieren, welche zwei Werte annimmt, den minuend und den subtrahend:\nsubtrahieren &lt;- function(minuend, subtrahend) {\n  minuend - subtrahend\n}\n\nsubtrahieren(10, 4)\n## [1] 6\nÜbersetzt in R-Code entsteht folgende Operation:\noutput &lt;- mean(subtrahieren(as.numeric(substr(diary, 20, 22)), 273.15))\n#                                             \\_1_/\n#                                      \\________2__________/\n#                           \\___________________3___________/\n#              \\________________________________4__________________/\n#         \\_____________________________________5____________________/\n\n# 1. Nimm diary\n# 2. Extrahiere auf jeder Zeile die Werte 20 bis 22\n# 3. Konvertiere \"character\" zu \"numeric\"\n# 4. Subtrahiere 273.15\n# 5. Berechne den Mittlwert\nDie ganze Operation liest sich etwas leichter, wenn diese sequentiell notiert wird:\ntemp &lt;- substr(diary, 20, 22)      # 2\ntemp &lt;- as.numeric(temp)           # 3\ntemp &lt;- subtrahieren(temp, 273.15) # 4\noutput &lt;- mean(temp)               # 5\nUmständlich ist dabei einfach, dass die Zwischenresultate immer abgespeichert und in der darauf folgenden Operation wieder abgerufen werden müssen. Hier kommt “piping” ins Spiel: Mit “piping” wird der Output der einen Funktion der erste Parameter der darauf folgenden Funktion.\ndiary |&gt;                  # 1\n  substr(20, 22) |&gt;       # 2\n  as.numeric() |&gt;         # 3\n  subtrahieren(273.15) |&gt; # 4\n  mean()                  # 5\n## [1] 74.18333",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "Wichtig\n\n\n\n\nder |&gt; Pipe Operator wurde erst in R 4.1 eingeführt\nNeben dem base R Pipe Operator existiert im Package magrittr ein sehr ähnlicher1 Pipe Operator: %&gt;%\nDie Tastenkombination Ctrl+Shift+M in RStudio fügt einen Pipe Operator ein.\nWelcher Pipe Operator |&gt; oder %&gt;% mit der obigen Tastenkombination eingeführt wird, kann über die RStudio Settings Tools → Global Options → Code → Häckchen setzen bei Use nativ pipe operator\nWir empfehlen die base-R Pipe |&gt; zu verwenden",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudierende &lt;- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  PLZ = c(8006, 8001, 8820)\n)\n\nstudierende\n##   Matrikel_Nr   Studi  PLZ\n## 1      100002 Patrick 8006\n## 2      100003 Manuela 8001\n## 3      200003     Eva 8820\n\nortschaften &lt;- data.frame(\n  PLZ = c(8003, 8006, 8810, 8820),\n  Ortsname = c(\"Zürich\", \"Zürich\", \"Horgen\", \"Wädenswil\")\n)\n\nortschaften\n##    PLZ  Ortsname\n## 1 8003    Zürich\n## 2 8006    Zürich\n## 3 8810    Horgen\n## 4 8820 Wädenswil\n\n\n# Load library\nlibrary(\"dplyr\")\n\ninner_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n\nleft_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      &lt;NA&gt;\n## 3      200003     Eva 8820 Wädenswil\n\nright_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n## 3          NA    &lt;NA&gt; 8003    Zürich\n## 4          NA    &lt;NA&gt; 8810    Horgen\n\nfull_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      &lt;NA&gt;\n## 3      200003     Eva 8820 Wädenswil\n## 4          NA    &lt;NA&gt; 8003    Zürich\n## 5          NA    &lt;NA&gt; 8810    Horgen\n\n\nstudierende &lt;- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Wohnort = c(8006, 8001, 8006)\n)\n\nleft_join(studierende, ortschaften, by = c(\"Wohnort\" = \"PLZ\"))\n##   Matrikel_Nr   Studi Wohnort Ortsname\n## 1      100002 Patrick    8006   Zürich\n## 2      100003 Manuela    8001     &lt;NA&gt;\n## 3      200003  Pascal    8006   Zürich",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#footnotes",
    "href": "prepro/Prepro2_Demo.html#footnotes",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "siehe https://stackoverflow.com/q/67633022/4139249↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html",
    "href": "prepro/Prepro2_Uebung_A.html",
    "title": "Prepro 2: Übung A",
    "section": "",
    "text": "Aufgabe 1\nLese die Wetterdaten von letzer Woche weather.csv (Quelle MeteoSchweiz) in R ein. Sorge dafür, dass die Spalten korrekt formatiert sind (stn als factor, time als POSIXct, tre200h0 als numeric.)\nMusterlösung\nlibrary(\"readr\")\n\nwetter &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\nwetter$stn &lt;- as.factor(wetter$stn)\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\", tz = \"UTC\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nLese den Datensatz metadata.csv ebenfalls als csv ein.\n\n\n\n\n\n\nTipp\n\n\n\nWenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. das è in Genève), hat das vermutlich mit der Zeichencodierung zu tun. Das File ist aktuell in UTF-8 codiert. Wenn Umlaute nicht korrekt dargestellt werden, hat R diese Codierung nicht erkannt und sie muss in der Import-Funktion spezifitiert werden. Dies wird je nach verwendete import Funktion unterschiedlich gemacht:\n\nFunktionen aus dem Package readr: locale = locale(encoding = \"UTF-8\")\nBase-R Funktionen: fileEncoding = \"UTF-8\"\n\nWenn ihr die Codierung eines Files nicht kennt, könnt ihr wie folgt vorgehen: Anleitung für Windows, für Mac und für Linux.\n\n\n\n\nMusterlösung\nmetadata &lt;- read_delim(\"datasets/prepro/metadata.csv\", locale = locale(encoding = \"UTF-8\"))",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNun wollen wir den Datensatz wetter mit den Informationen aus metadata anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Selektiere diese Spalten.\n\n\nMusterlösung\nmetadata &lt;- metadata[, c(\"stn\", \"Name\", \"x\", \"y\", \"Meereshoehe\")]",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nJetzt kann metadata mit dem Datensatz wetter verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir “joinen” können.\nNutze die Join-Möglichkeiten von dplyr (Hilfe via ?dplyr::join), um die Datensätze wetter und metadata zu verbinden.\n\n\nMusterlösung\nlibrary(\"dplyr\")\nwetter &lt;- left_join(wetter, metadata, by = \"stn\")\n\n# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.\n# Attribut: \"stn\"",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nErstelle eine neue Spalte month, welche den jeweiligen Monat (aus time) beinhaltet. Nutze dafür die Funktion lubridate::month().\n\n\nMusterlösung\nlibrary(\"lubridate\")\n\nwetter$month &lt;- month(wetter$time)",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nBerechne mit der Spalte month die Durchschnittstemperatur pro Monat.\n\n\nMusterlösung\nmean(wetter$tre200h0[wetter$month == 1])\n## [1] -1.963239\nmean(wetter$tre200h0[wetter$month == 2])\n## [1] 0.3552632\nmean(wetter$tre200h0[wetter$month == 3])\n## [1] 2.965054\n\n# usw. für alle 12 Monate",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html",
    "href": "prepro/Prepro2_Uebung_B.html",
    "title": "Prepro 2: Übung B",
    "section": "",
    "text": "Aufgabe 1\nGegeben sind die Daten von drei Sensoren (sensor1.csv, sensor2.csv, sensor3.csv). Lese die Datensätze ein.\nMusterlösung\nlibrary(\"readr\")\n\nsensor1 &lt;- read_delim(\"datasets/prepro/sensor1.csv\")\nsensor2 &lt;- read_delim(\"datasets/prepro/sensor2.csv\")\nsensor3 &lt;- read_delim(\"datasets/prepro/sensor3.csv\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle aus den 3 Dataframes einen einzigen Dataframe, welcher aussieht wie unten dargestellt. Nutze dafür zwei joins aus dplyr, um 3 data.frames miteinander zu verbinden. Bereinige im Anschluss die Spaltennamen (wie geht das?).\n\n\nMusterlösung\nlibrary(\"dplyr\")\n\nsensor1_2 &lt;- full_join(sensor1, sensor2, \"Datetime\")\n\nsensor1_2 &lt;- rename(sensor1_2, sensor1 = Temp.x, sensor2 = Temp.y)\n\nsensor_all &lt;- full_join(sensor1_2, sensor3, by = \"Datetime\")\n\nsensor_all &lt;- rename(sensor_all, sensor3 = Temp)\n\n\n\n\n\n\n\nDatetime\nsensor1\nsensor2\nsensor3\n\n\n\n\n16102017_1800\n23.5\n13.5\n26.5\n\n\n17102017_1800\n25.4\n24.4\n24.4\n\n\n18102017_1800\n12.4\n22.4\n13.4\n\n\n19102017_1800\n5.4\n12.4\n7.4\n\n\n23102017_1800\n23.5\n13.5\nNA\n\n\n24102017_1800\n21.3\n11.3\nNA",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Datei sensor_fail.csv in R.\nsensor_fail.csv hat eine Variabel SensorStatus: 1 bedeutet der Sensor misst, 0 bedeutet der Sensor misst nicht. Fälschlicherweise wurde auch dann der Messwert Temp = 0 erfasst, wenn Sensorstatus = 0. Richtig wäre hier NA (not available). Korrigiere den Datensatz entsprechend.\n\n\nMusterlösung\nsensor_fail &lt;- read_delim(\"datasets/prepro/sensor_fail.csv\")\n\n# mit base-R:\nsensor_fail$Temp_correct[sensor_fail$SensorStatus == 0] &lt;- NA\nsensor_fail$Temp_correct[sensor_fail$SensorStatus != 0] &lt;- sensor_fail$Temp #Warnmeldung kann ignoriert werden.\n\n# das gleiche mit dplyr:\nsensor_fail &lt;- sensor_fail |&gt;\n  mutate(Temp_correct = ifelse(SensorStatus == 0, NA, Temp))\n\n\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\nTemp_correct\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n0.6\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n0.3\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n0.0\n\n\nSen102\n0.0\n86\n19102017_1800\n0\nNA\n\n\nSen102\n0.0\n98\n23102017_1800\n0\nNA\n\n\nSen102\n0.0\n98\n24102017_1800\n0\nNA\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n0.0\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n-0.3\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n-0.7\n\n\nSen103\n-1.2\n98\n28102017_1800\n1\n-1.2",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWarum spielt es eine Rolle, ob 0 oder NA erfasst wird? Berechne die Mittlere der Temperatur / Feuchtigkeit nach der Korrektur.\n\n\nMusterlösung\n# Mittelwerte der falschen Sensordaten: 0 fliesst in die Berechnung\n# ein und verfälscht den Mittelwert\nmean(sensor_fail$Temp)\n## [1] -0.13\n\n# Mittelwerte der korrigierten Sensordaten: mit na.rm = TRUE werden\n# NA-Werte aus der Berechnung entfernt.\nmean(sensor_fail$Temp_correct, na.rm = TRUE)\n## [1] -0.1857143",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html",
    "href": "prepro/Prepro3_Demo.html",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "Split-Apply-Combine",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "Daten laden\nWir laden die Wetterdaten (Quelle MeteoSchweiz) der letzten Übung.\n\nlibrary(\"readr\")\n\nwetter &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\n\nwetter$stn &lt;- as.factor(wetter$stn)\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\")\n\n\n\nKennwerte berechnen\nWir möchten den Mittelwert aller gemessenen Temperaturwerten berechnen. Dazu könnten wir folgenden Befehl verwenden:\n\nmean(wetter$tre200h0, na.rm = TRUE)\n## [1] 6.324744\n\nDie Option na.rm = T bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen.\nMit derselben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (max()), Minimum (min()), Median (median()) u.v.m.).\nDiese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über alle Beobachtungen für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur pro Monat berechnen wollen.\n\n\nConvenience Variablen\nUm diese Aufgabe zu lösen, muss zuerst der Monat extrahiert werden (der Monat ist die convenience variable). Hierfür brauchen wir die Funktion lubridate::month().\nNun kann kann die convenience Variable “Month” erstellt werden. Ohne dpylr wird eine neue Spalte folgendermassen hinzugefügt.\n\nlibrary(\"lubridate\")\n\nwetter$month &lt;- month(wetter$time)\n\nMit dplyr (siehe 3) sieht der gleiche Befehl folgendermassen aus:\n\nlibrary(\"dplyr\")\n\nwetter &lt;- mutate(wetter, month = month(time))\n\nDer grosse Vorteil von dplyr ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.\n\n\nKennwerte nach Gruppen berechnen\nUm mit base R den Mittelwert pro Monat zu berechnen, kann man zuerst ein Subset mit [] erstellen und davon den Mittelwert berechnen, z.B. folgendermassen:\n\nmean(wetter$tre200h0[wetter$month == 1], na.rm = TRUE)\n## [1] -1.963239\n\nDies müssen wir pro Monat wiederholen, was natürlich sehr umständlich ist. Deshalb nutzen wir das package dplyr. Damit geht die Aufgabe (Temperaturmittel pro Monat berechnen) folgendermassen:\n\nsummarise(group_by(wetter, month), temp_mittel = mean(tre200h0, na.rm = TRUE))\n## # A tibble: 13 × 2\n##    month temp_mittel\n##    &lt;dbl&gt;       &lt;dbl&gt;\n##  1     1      -1.96 \n##  2     2       0.355\n##  3     3       2.97 \n##  4     4       4.20 \n##  5     5      11.0  \n##  6     6      12.4  \n##  7     7      13.0  \n##  8     8      15.0  \n##  9     9       9.49 \n## 10    10       8.79 \n## 11    11       1.21 \n## 12    12      -0.898\n## 13    NA       2.95\n\n\n\nVerketten vs. verschachteln\nAuf Deutsch übersetzt heisst die obige Operation folgendermassen:\n\nnimm den Datensatz wetter\nBilde Gruppen pro Jahr (group_by(wetter,year))\nBerechne das Temperaturmittel (mean(tre200h0))\n\nDiese Übersetzung R -&gt; Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch verkettet ausgesprochen wird (Operation 1-&gt;2-&gt;3) während der Computer verschachtelt liest 3(2(1)). Um R näher an die gesprochene Sprache zu bringen, kann man den |&gt;-Operator verwenden (siehe 4).\n\n# 1 nimm den Datensatz \"wetter\"\n# 2 Bilde Gruppen pro Monat\n# 3 berechne das Temperaturmittel\n\nsummarise(group_by(wetter, month), temp_mittel = mean(tre200h0))\n#                  \\_1_/\n#         \\__________2_________/\n# \\__________________3_______________________________________/\n\n# wird zu:\n\nwetter |&gt;                                 # 1\n  group_by(month) |&gt;                      # 2\n  summarise(temp_mittel = mean(tre200h0)) # 3\n\nDieses Verketten mittels |&gt; (genannt “pipe”) macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Die “pipe” wird mit dem package magrittr bereitgestellt und mit dplyr mitinstalliert.\nZu dplyr gibt es etliche Tutorials online (siehe5), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in dpylr: mutate() und summarise().\n\nsummarise() fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der “summarise” Funktion spezifiziert wurde (z.B. temp_mittel).\nmit mutate wird ein data.frame vom Umfang her belassen, es werden lediglich zusätzliche Variablen (Spalten) hinzugefügt (siehe Beispiel unten).\n\n\n# Maximal und minimal Temperatur pro Kalenderwoche\nweather_summary &lt;- wetter |&gt;                # 1) nimm den Datensatz \"wetter\"\n  filter(month == 1) |&gt;                     # 2) filter auf den Monat Januar\n  mutate(day = day(time)) |&gt;                # 3) erstelle eine neue Spalte \"day\"\n  group_by(day) |&gt;                          # 4) Nutze die neue Spalte um Gruppen zu bilden\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), # 5) Berechne das Maximum\n    temp_min = min(tre200h0, na.rm = TRUE)  # 6) Berechne das Minimum\n  )\n\nweather_summary\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#reshaping-data",
    "href": "prepro/Prepro3_Demo.html#reshaping-data",
    "title": "Prepro 3: Demo",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nBreit → lang\nDie Umformung von Tabellen breit→lang erfolgt mittels tidyr(siehe 6). Auch dieses Package funktioniert wunderbar mit piping (|&gt;).\n\nlibrary(\"tidyr\")\nweather_summary |&gt;\n  pivot_longer(c(temp_max, temp_min))\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nIm Befehl pivot_longer() müssen wir festlegen, welche Spalten zusammengefasst werden sollen (hier: temp_max,temp_min,temp_mean). Alternativ können wir angeben, welche Spalten wir nicht zusammenfassen wollen:\n\nweather_summary |&gt;\n  pivot_longer(-day)\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nWenn wir die Namen neuen Spalten festlegen wollen (anstelle von name und value) erreichen wir dies mit names_to bzw. values_to:\n\nweather_summary_long &lt;- weather_summary |&gt;\n  pivot_longer(-day, names_to = \"Messtyp\", values_to = \"Messwert\")\n\nDie ersten 6 Zeilen von weather_summary_long:\n\n\n\n\n\nday\nMesstyp\nMesswert\n\n\n\n\n1\ntemp_max\n5.8\n\n\n1\ntemp_min\n-4.4\n\n\n2\ntemp_max\n2.8\n\n\n2\ntemp_min\n-4.3\n\n\n3\ntemp_max\n4.2\n\n\n3\ntemp_min\n-3.1\n\n\n\n\n\nDie ersten 6 Zeilen von wetter_sry:\n\n\n\n\n\nday\ntemp_max\ntemp_min\n\n\n\n\n1\n5.8\n-4.4\n\n\n2\n2.8\n-4.3\n\n\n3\n4.2\n-3.1\n\n\n4\n4.7\n-2.8\n\n\n5\n11.4\n-0.6\n\n\n6\n6.7\n-1.6\n\n\n\n\n\nBeachte: weather_summary_long umfasst 62 Beobachtungen (Zeilen), das sind doppelt soviel wie weather_summary, da wir ja zwei Spalten zusammengefasst haben.\n\nnrow(weather_summary)\n## [1] 31\nnrow(weather_summary_long)\n## [1] 62\n\nLange Tabellen sind in verschiedenen Situationen praktischer. Beispielsweise ist das Visualisieren mittels ggplot2 (dieses Package werdet ihr im Block “InfoVis” kennenlernen) mit long tables wesentlich einfacher.\n\n\nlibrary(\"ggplot2\")\nggplot(weather_summary_long, aes(day, Messwert, colour = Messtyp)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\nLang → breit\nDas Gegenstück zu pivot_longer ist pivot_wider. Mit dieser Funktion können wir eine lange Tabelle in eine breite überführen. Dazu müssen wir in names_from angeben, aus welcher Spalte die neuen Spaltennamen erstellt werden sollen (names_from) und aus welcher Spalte die Werte entstammen sollen (values_from):\n\nweather_summary_long |&gt;\n  pivot_wider(names_from = Messtyp, values_from = Messwert)\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows\n\nZum Vergleich: mit einer wide table müssen wir in ggplot2 jede Spalte einzeln plotten. Dies ist bei wenigen Variabeln wie hier noch nicht problematisch, aber bei einer hohen Anzahl wird dies schnell mühsam.\n\nggplot(weather_summary) +\n  geom_line(aes(day, temp_max)) +\n  geom_line(aes(day, temp_min))\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, und Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#footnotes",
    "href": "prepro/Prepro3_Demo.html#footnotes",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "http://r4ds.had.co.nz/↩︎\nhttps://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093↩︎\nWickham und Grolemund (2017), Kapitel 10 / http://r4ds.had.co.nz/transform.html↩︎\nWickham und Grolemund (2017), Kapitel 14 / http://r4ds.had.co.nz/pipes.html↩︎\nWickham und Grolemund (2017), Kapitel 10 / http://r4ds.had.co.nz/transform.html, oder Hands-on dplyr tutorial..↩︎\nhttps://r4ds.had.co.nz/tidy-data.html#pivoting↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html",
    "href": "prepro/Prepro3_Uebung.html",
    "title": "Prepro 3: Übung",
    "section": "",
    "text": "Aufgabe 1\nGegeben sei ein Datensatz sensors_combined.csv, mit den Temperaturwerten von drei verschiedenen Sensoren. Importiere ihn als csv in R (als sensors_combined).\nFormatiere die Datetime Spalte in POSIXct um. Verwende dazu die Funktion as.POSIXct (lies mit ?strftime() nochmal nach, wie du das spezifische Format (die “Schablone”) festlegen kannst.\nMusterlösung\nlibrary(\"readr\")\n\nsensors_combined &lt;- read_delim(\"datasets/prepro/sensors_combined.csv\", \",\")\n\nsensors_combined$Datetime &lt;- as.POSIXct(sensors_combined$Datetime, format = \"%d%m%Y_%H%M\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nÜberführe die Tabelle in ein langes Format (verwende dazu die Funktion pivot_longer aus tidyr) und speichere den output als sensors_long.\nTipp:\n\nim Argument cols kannst du entweder die Spalten auflisten, die “pivotiert” werden sollen.\nAlternativ kannst du (mit vorangestelltem Minuszeichen, -) die Spalte bezeichnen, die nicht pivotiert werden soll.\nIn beiden Fällen musst du die Spalten weder mit Anführungs- und Schlusszeichen noch mit dem $-Zeichen versehen.\n\n\n\nMusterlösung\nlibrary(\"tidyr\")\n\n# Variante 1 (Spalten abwählen)\nsensors_long &lt;- pivot_longer(sensors_combined, -Datetime) \n\n# Variante 2 (Spalten anwählen)\nsensors_long &lt;- pivot_longer(sensors_combined, c(sensor1:sensor3))",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nGruppiere sensors_long nach der neuen Spalte, wo die Sensor-Information enthalten ist (default: name) mit group_by und berechne den Mittelwert der Temperatur pro Sensor (summarise). Hinweis: Beide Funktionen sind Teil des Packages dplyr.\nDer Output sieht folgendermassen aus:\n\n\nMusterlösung\nlibrary(\"dplyr\")\n\nsensors_long |&gt;\n  group_by(name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 3 × 2\n##   name    temp_mean\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 sensor1      14.7\n## 2 sensor2      12.0\n## 3 sensor3      14.4",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nErstelle für sensors_long eine neue convenience Variabel month, welche den Monat beinhaltet (Tipp: verwende dazu die Funktion month aus lubridate). Gruppiere nun nach month und Sensor und berechne den Mittelwert der Temperatur.\n\n\nMusterlösung\nlibrary(\"lubridate\")\n\nsensors_long |&gt;\n  mutate(month = month(Datetime)) |&gt;\n  group_by(month, name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 6 × 3\n## # Groups:   month [2]\n##   month name    temp_mean\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n## 1    10 sensor1     14.7 \n## 2    10 sensor2     12.7 \n## 3    10 sensor3     14.4 \n## 4    11 sensor1    NaN   \n## 5    11 sensor2      8.87\n## 6    11 sensor3    NaN",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nLade jetzt nochmal den Datensatz weather.csv (Quelle MeteoSchweiz) herunter und importiere ihn als CSV mit den korrekten Spaltentypen (stn als factor, time als POSIXct, tre200h0 als double).\n\n\nMusterlösung\nweather &lt;- read_delim(\"datasets/prepro/weather.csv\", col_types = cols(col_factor(), col_datetime(\"%Y%m%d%H\"), col_double()), \",\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nErstelle nun eine convenience Variable für die Kalenderwoche pro Messung (lubridate::week). Berechne im Anschluss den Mittelwert der Temperatur pro Kalenderwoche.\n\n\nMusterlösung\nweather_summary &lt;- weather |&gt;\n  mutate(week = week(time)) |&gt;\n  group_by(week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\nVisualisiere im Anschluss das Resultat:\n\nMusterlösung\nplot(weather_summary$week, weather_summary$temp_mean, type = \"l\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nIn der vorherigen Aufgabe haben wir den Mittelwert der Temperatur pro Kalenderwoche über alle Jahre (2000 und 2001) berechnet. Wenn wir die Jahre aber miteinander vergleichen wollen, müssen wir das Jahr als zusätzliche convenience Variable erstellen und danach gruppieren. Versuche dies mit den Wetterdaten und visualisiere den Output anschliessend.\n\n\nMusterlösung\nweather_summary2 &lt;- weather |&gt;\n  mutate(\n    week = week(time),\n    year = year(time)\n    ) |&gt;\n  group_by(year, week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\n\n\nMusterlösung\nplot(weather_summary2$week, weather_summary2$temp_mean, type = \"l\")\n\n\n\n\n\n\n\n\nAbbildung 8.1: baseplot mag keine long tables und macht aus den beiden Jahren eine kontinuierliche Linie",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nÜberführe den Output aus der letzten Übung in eine wide table. Nun lassen sich die beiden Jahre viel besser miteinander vergleichen.\n\n\nMusterlösung\nweather_summary2 &lt;- weather_summary2 |&gt;\n  pivot_wider(names_from = year, values_from = temp_mean,names_prefix = \"year\")\n\n\n\n\nMusterlösung\nplot(weather_summary2$week, weather_summary2$year2000, type = \"l\",col = \"blue\")\nlines(weather_summary2$week, weather_summary2$year2001, type = \"l\",col = \"red\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "InfoVis.html",
    "href": "InfoVis.html",
    "title": "InfoVis",
    "section": "",
    "text": "Infovis 1\nDie konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.",
    "crumbs": [
      "InfoVis"
    ]
  },
  {
    "objectID": "InfoVis.html#infovis-2",
    "href": "InfoVis.html#infovis-2",
    "title": "InfoVis",
    "section": "Infovis 2",
    "text": "Infovis 2\nDie Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser lesson eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.",
    "crumbs": [
      "InfoVis"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Vorbereitung.html",
    "href": "infovis/Infovis1_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von InfoVis 1 - 2 werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog Vorbereitung könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\npacman::p_install(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \n  \"scales\", \"tidyr\", character.only = TRUE,  force = FALSE)\n\nZudem könnt ihr die Daten für die Übungen auf Moodle herunterladen.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html",
    "href": "infovis/Infovis1_Demo.html",
    "title": "Infovis 1: Demo A",
    "section": "",
    "text": "Ggplot2\nIn ggplot wird durch den Befehl ggplot() initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (data =), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (mapping = aes()).\n# Datensatz: \"temperature\" | Beeinflussende Variabeln: \"time\" und \"temp\"\nggplot(data = temperature, mapping = aes(time, SHA))\nWeiter braucht es mindestens ein “Layer”, der beschreibt, wie die Daten dargestellt werden sollen (z.B. geom_point()). Anders als bei “Piping” (|&gt;) wird ein Layer mit + hinzugefügt.\nggplot(data = temperature, mapping = aes(time, SHA)) +\n  # Layer: \"geom_point\" entspricht Punkten in einem Scatterplot\n  geom_point()\nDa ggplot die Eingaben in der Reihenfolge data = und dann mapping = erwartet, können wir diese Spezifizierungen auch weglassen.\nggplot(temperature, aes(time, SHA)) +\n  geom_point()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nWie wir in PrePro 2 bereits erwähnt haben, ist ggplot2 auf long tables ausgelegt. Wir überführen deshalb an dieser Stelle die breite in eine lange Tabelle:\n\ntemperature_long &lt;- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nNun wollen wir die Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in aes().\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWir können noch einen Layer mit Linien hinzufügen:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point() +\n  geom_line()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "href": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "title": "Infovis 1: Demo A",
    "section": "Beschriftungen (labels)",
    "text": "Beschriftungen (labels)\nWeiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (geom_point()) nun weg, da mir diese nicht gefallen.\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  )",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIm obigen Plot fällt auf, dass stündliche Werte eine zu hohe Auflösung haben, wenn wir die Daten über 2 Jahre visualisieren. Mit Split Apply Combine (PrePro 3) können wir die Auflösung unserer Daten verändern:\n\ntemperature_day &lt;- temperature_long |&gt;\n  mutate(time = as.Date(time))\n\ntemperature_day\n## # A tibble: 35,088 × 3\n##    time       station  temp\n##    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n##  1 2000-01-01 SHA       0.2\n##  2 2000-01-01 ZER      -8.8\n##  3 2000-01-01 SHA       0.3\n##  4 2000-01-01 ZER      -8.7\n##  5 2000-01-01 SHA       0.3\n##  6 2000-01-01 ZER      -9  \n##  7 2000-01-01 SHA       0.3\n##  8 2000-01-01 ZER      -8.7\n##  9 2000-01-01 SHA       0.4\n## 10 2000-01-01 ZER      -8.5\n## # ℹ 35,078 more rows\n\ntemperature_day &lt;- temperature_day |&gt;\n  group_by(station, time) |&gt;\n  summarise(temp = mean(temp))\n\ntemperature_day\n## # A tibble: 1,462 × 3\n## # Groups:   station [2]\n##    station time        temp\n##    &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;\n##  1 SHA     2000-01-01  1.25\n##  2 SHA     2000-01-02  1.73\n##  3 SHA     2000-01-03  1.59\n##  4 SHA     2000-01-04  1.78\n##  5 SHA     2000-01-05  4.66\n##  6 SHA     2000-01-06  3.49\n##  7 SHA     2000-01-07  3.87\n##  8 SHA     2000-01-08  3.28\n##  9 SHA     2000-01-09  3.24\n## 10 SHA     2000-01-10  3.24\n## # ℹ 1,452 more rows",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "href": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "title": "Infovis 1: Demo A",
    "section": "X/Y-Achse anpassen",
    "text": "X/Y-Achse anpassen\nMan kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat ggplot eine Annahme auf der Basis der Daten getroffen).\nBei unserer y-Achse handelt es sich um numerische Daten, ggplot nennt diese: scale_y_continuous(). Unter ggplot2.tidyverse.org findet man noch andere x/y-Achsentypen (scale_x_irgenwas bzw. scale_y_irgendwas).\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) # y-Achsenabschnitt bestimmen\n\n\n\n\n\n\n\n\nDas gleiche Spiel kann man für die x-Achse betreiben. Bei unserer x-Achse handelt es sich ja um Datumsangaben. ggplot nennt diese: scale_x_date().\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  )",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nSehr praktisch sind auch die Funktionen für “Small multiples”. Dies erreicht man mit facet_wrap() (oder facet_grid(), mehr dazu später). Man muss mit einem Tilde-Symbol “~” nur festlegen, welche Variable für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll.\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station)\n\n\n\n\n\n\n\n\nAuch facet_wrap kann man auf seine Bedürfnisse anpassen: Beispielweise kann man mit ncol = die Anzahl facets pro Zeile bestimmen.\nZudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb theme(legend.position=\"none\")\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#plot-exportieren",
    "href": "infovis/Infovis1_Demo.html#plot-exportieren",
    "title": "Infovis 1: Demo A",
    "section": "Plot exportieren",
    "text": "Plot exportieren\nFolgendermassen kann ich den letzten Plot als png-File abspeichern:\n\nggsave(filename = \"plot.png\")",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html",
    "href": "infovis/Infovis1_Uebung.html",
    "title": "Infovis 1: Übung",
    "section": "",
    "text": "Aufgabe 1\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_kanton.csv Datensatz:\nTipp:",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "title": "Infovis 1: Übung",
    "section": "",
    "text": "Nutze ggplot(kanton, aes(auslanderanteil, ja_anteil)), um den ggplot zu initiieren. Füge danach einen Punkte-Layer hinzu (geom_point())\nNutze coord_fixed(), um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).\nOptional:\n\nSetze die Achsen Start- und Endwerte mittels scale_y_continuous bzw. scale_x_continuous.\nSetze analog Kovic (2014) die breaks (0.0, 0.1…0.7) manuell (innerhalb scale_*_continuous)\nNutze labs() für die Beschriftung der Achsen",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot:\nTipp:\n\nNutze geom_smooth",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Gemeindedaten tagi_data_gemeinden.csv.\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_point()\nNutze labs()\nNutze coord_fixed()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap um einen Plot pro Kanton darzustellen.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nRekonstrukturieren folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. „Je weniger Ausländer, desto mehr Ja-Stimmen? Wirklich?“ Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: Script EDA",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"scales\")\n\n# create some data about age and height of people\npeople &lt;- data.frame(\n  ID = c(1:30),\n  age = c(\n    5.0, 7.0, 6.5, 9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,\n    63.5, 65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,\n    15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3\n  ),\n  height = c(\n    0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,\n    1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,\n    1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80\n  ),\n  weight = c(\n    45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,\n    84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,\n    48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3\n  )\n)\n\n# build a scatterplot for a first inspection\nggplot(people, aes(x = age, y = height)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -&gt; Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 25593\n##   10 | 037\n##   12 | 523\n##   14 | 19556\n##   16 | 255789916\n##   18 | 04774\nstem(people$height, scale = 2)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 2559\n##    9 | 3\n##   10 | \n##   11 | 037\n##   12 | 5\n##   13 | 23\n##   14 | 19\n##   15 | 556\n##   16 | 2557899\n##   17 | 16\n##   18 | 0477\n##   19 | 4\n\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    5.00    8.70   30.20   59.14   65.15  512.30\nboxplot(people$age)\n\n\n\n\n\n\n\n\n\nsummary(people$height)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.820   1.190   1.555   1.455   1.690   1.940\nboxplot(people$height)\n\n\n\n\n\n\n\n\n\n# explore data with a histgram\nggplot(people, aes(x = age)) +\n  geom_histogram(binwidth = 20)\n\n\n\n\n\n\n\n\n\ndensity(x = people$height)\n## \n## Call:\n##  density.default(x = people$height)\n## \n## Data: people$height (30 obs.);   Bandwidth 'bw' = 0.1576\n## \n##        x                y           \n##  Min.   :0.3472   Min.   :0.001593  \n##  1st Qu.:0.8636   1st Qu.:0.102953  \n##  Median :1.3800   Median :0.510601  \n##  Mean   :1.3800   Mean   :0.483553  \n##  3rd Qu.:1.8964   3rd Qu.:0.722660  \n##  Max.   :2.4128   Max.   :1.216350\n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes\n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n\n# logarithmic axis: respond to skewness in the data, e.g. log10\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean &lt;- people |&gt;\n  filter(ID != 27) |&gt; # Diese Person war zu klein.\n  filter(age &lt; 100) # Fehler in der Erhebung des Alters\n\n\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10)\n\n\n\n\n\n\n\n\n\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10) +\n  theme_bw() # specifying the theme\n\n\n\n\n\n\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5) +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age &lt; 15) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# filter \"teenies\": No trend\npeopleClean |&gt;\n  filter(age &gt; 55) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[, 2:4], panel = panel.smooth)\n\n\n\n\n\n\n\n\n\npairs(peopleClean[, 2:4], panel = panel.smooth)",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Script EDA</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html",
    "href": "infovis/Infovis2_Uebung_A.html",
    "title": "Infovis 2: Übung A",
    "section": "",
    "text": "Aufgabe 1\nMache aus der wide table eine long table, die wie folgt aussieht.\ntime\nstation\ntemperature\n\n\n\n\n2005-01-01\nALT\n1.3\n\n\n2005-01-01\nBUS\n1.5\n\n\n2005-01-01\nGVE\n1.1\n\n\n2005-01-01\nINT\n0.2\n\n\n2005-01-01\nOTL\n2.2\n\n\n2005-01-01\nLUG\n1.7\nImportiere anschliessend den Datensatz temperature_2005_metadata.csv und verbinde die beiden Datensätze mit einem left_join via station (bzw. stn).",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle ein Scatterplot (time vs. temperature), wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot (scale_color_gradient). Verkleinere die Punkte, um übermässiges Überplotten der Punkten zu vermeiden (size =). Weiter sollen auf der x-Achse im Abstand von 3 Monaten der jeweilige Monat vermerkt sein (date_breaks bzw. date_labels von scale_x_datetime()).",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nErstelle eine Zusatzvariabel Date mit dem Datum der jeweiligen Messung ( mit as.Date). Nutze diese Spalte, um die Tagesmitteltemperatur pro Station zu berechnen (mit summarise()).\nUm die Metadaten (Name, Meereshoehe, x, y) nicht zu verlieren, kannst du den Join aus der ersten Übung wieder ausführen. Alternativ (schneller aber auch schwerer zu verstehen) kannst du diese Variabeln innerhalb deines group_by verwenden.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWiederhole nun den Plot aus der ersten Aufgabe mit den aggregierten Daten aus der vorherigen Aufgabe. Um die labels korrekt zu setzen, musst du scale_x_datetime mit scale_x_date ersetzen.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nFüge am obigen Plot eine schwarze, gestrichelte Trendlinie hinzu.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nPositioniere die Legende oberhalb des Plots (nutze dazu theme() mit legend.position).",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 7 (optional, fortgeschritten)",
    "text": "Aufgabe 7 (optional, fortgeschritten)\nFüge den Temperaturwerten auf der y-Ache ein °C hinzu (siehe unten und studiere diesen Tipp zur Hilfe).",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nJetzt verlassen wir den Scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein.\n\nBeachte den Unterschied zwischen colour = und fill =\nBeachte den Unterschied zwischen facet_wrap() und facet_grid()\nfacet_grid() braucht übrigens noch einen Punkt (.) zur Tilde (~).\nBeachte den Unterschied zwischen “.~” und “~.” bei facet_grid()\nverschiebe nach Bedarf die Legende",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 9",
    "text": "Aufgabe 9\nAls letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm geom_histogram() mit den Temperaturwerten. Teile dazu die Stationen in verschiedene Höhenlagen ein (Tieflage [&lt; 400 m], Mittellage [400 - 600 m] und Hochlage [&gt; 600 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen mit einem Histogramm.\nTip: Nutze cut um die Stationen in die drei Gruppen aufzuteilen\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, und Garrett Grolemund. 2023. R for data science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html",
    "href": "infovis/Infovis2_Uebung_B.html",
    "title": "Infovis 2: Übung optional",
    "section": "",
    "text": "Aufgabe 1: Parallel coordinate plots\nErstelle einen parallel coordinate plot. Dafür eignet sich der integrierte Datensatz mtcars. Extrahiere die Fahrzeugnamen mit rownames_to_column.\nZudem müssen die Werte jeweiles auf eine gemeinsame Skala normalisiert werden. Hierfür kannst du die Funktion scales::rescale verwenden.\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\nSo sieht der fertige Plot aus:",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Infovis 2: Übung optional</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "title": "Infovis 2: Übung optional",
    "section": "Aufgabe 2: Polar Plot mit Biber Daten",
    "text": "Aufgabe 2: Polar Plot mit Biber Daten\nPolar Plots eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:\n\nbeaver1 und beaver2\nAirPassenger\n\nBeide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.\nWenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen.\nZudem müssen wir die Zeitangabe noch anpassen: Gemäss der Datenbeschreibung handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als “0330” notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln.\nSo sieht der fertige Plot aus:",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Infovis 2: Übung optional</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "title": "Infovis 2: Übung optional",
    "section": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren",
    "text": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren\nAnalog Aufgabe 2, dieses Mal mit dem Datensatz AirPassengers\nAirPassengers kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein data.frame oder eine matrix, ist aber von der Klasse ts.\n\n##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 1949 112 118 132 129 121 135 148 148 136 119 104 118\n## 1950 115 126 141 135 125 149 170 170 158 133 114 140\n## 1951 145 150 178 163 172 178 199 199 184 162 146 166\n## 1952 171 180 193 181 183 218 230 242 209 191 172 194\n## 1953 196 196 236 235 229 243 264 272 237 211 180 201\n## 1954 204 188 235 227 234 264 302 293 259 229 203 229\n## 1955 242 233 267 269 270 315 364 347 312 274 237 278\n## 1956 284 277 317 313 318 374 413 405 355 306 271 306\n## 1957 315 301 356 348 355 422 465 467 404 347 305 336\n## 1958 340 318 362 348 363 435 491 505 404 359 310 337\n## 1959 360 342 406 396 420 472 548 559 463 407 362 405\n## 1960 417 391 419 461 472 535 622 606 508 461 390 432\n## [1] \"ts\"\n\nDamit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine matrix umwandeln. Wie das geht habe ich hier erfahren.\n\n##       month\n## year   Apr Aug Dec Feb Jan Jul Jun Mar May Nov Oct Sep\n##   1949 129 148 118 118 112 148 135 132 121 104 119 136\n##   1950 135 170 140 126 115 170 149 141 125 114 133 158\n##   1951 163 199 166 150 145 199 178 178 172 146 162 184\n##   1952 181 242 194 180 171 230 218 193 183 172 191 209\n##   1953 235 272 201 196 196 264 243 236 229 180 211 237\n##   1954 227 293 229 188 204 302 264 235 234 203 229 259\n##   1955 269 347 278 233 242 364 315 267 270 237 274 312\n##   1956 313 405 306 277 284 413 374 317 318 271 306 355\n##   1957 348 467 336 301 315 465 422 356 355 305 347 404\n##   1958 348 505 337 318 340 491 435 362 363 310 359 404\n##   1959 396 559 405 342 360 548 472 406 420 362 407 463\n##   1960 461 606 432 391 417 622 535 419 472 390 461 508\n\nAus der matrix muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen.\nSo sieht der fertige Plot aus:",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Infovis 2: Übung optional</span>"
    ]
  },
  {
    "objectID": "Statistik.html",
    "href": "Statistik.html",
    "title": "Statistik",
    "section": "",
    "text": "Statistik 1\nIn Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-2",
    "href": "Statistik.html#statistik-2",
    "title": "Statistik",
    "section": "Statistik 2",
    "text": "Statistik 2\nIn Statistik 2 lernen die Studierenden die Voraussetzungen und die praktische Anwendung “einfacher” linearer Modelle in R (sowie teilweise ihrer “nicht-parametrischen” bzw. “robusten” Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-3",
    "href": "Statistik.html#statistik-3",
    "title": "Statistik",
    "section": "Statistik 3",
    "text": "Statistik 3\nIn Statistik 3 beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlauben, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Dann besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet. Abschliessend fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-4",
    "href": "Statistik.html#statistik-4",
    "title": "Statistik",
    "section": "Statistik 4",
    "text": "Statistik 4\nIn Statistik 4 geht es um multiple Regressionen, die versuchen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-5",
    "href": "Statistik.html#statistik-5",
    "title": "Statistik",
    "section": "Statistik 5",
    "text": "Statistik 5\nIn Statistik 5 geht es um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-6",
    "href": "Statistik.html#statistik-6",
    "title": "Statistik",
    "section": "Statistik 6",
    "text": "Statistik 6\nIn Statistik 6 lernen die Studierenden Lösungen kennen, welche die diversen Limitierungen von linearen Modellen überwinden. Während generalized linear models (GLMs) aus Statistik 4 bekannt sind, geht es jetzt um linear mixed effect models (LMMs) und generalized linear mixed effect models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den Beobachtungen. Einfachere Fälle von LMMs, wie split-plot und repeated-measures ANOVAs, lassen sich noch mit dem aov-Befehl in Base R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen Abhängigkeiten, erlauben.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-7",
    "href": "Statistik.html#statistik-7",
    "title": "Statistik",
    "section": "Statistik 7",
    "text": "Statistik 7\nStatistik 7 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit mehreren abhängigen und mehrenen unabhängigen Variablen zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-8",
    "href": "Statistik.html#statistik-8",
    "title": "Statistik",
    "section": "Statistik 8",
    "text": "Statistik 8\nStatistik 8 beinhaltet eine konzeptionelle Einführung in die Idee von Ordinationen als einer Technik der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion visualisiert. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Neben der Beschreibung der Datenstruktur in komplexen Datensätzen kann eine PCA auch dazu dienen, aus diesen unabhängie Variablen zu generieren, die anschliessend in einer multiplen Regression als Prädiktoren genutzt werden können Im Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "statistik/Statistik0_Vorbereitung.html",
    "href": "statistik/Statistik0_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von Statistik 1 - 8 werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog Vorbereitung könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\npacman::p_load(\"agricolae\", \"car\", \"corrplot\", \"DHARMa\", \"doBy\", \"factoextra\",\n\"FSA\", \"glmmTMB\", \"labdsv\", \"lme4\", \"MASS\", \"MuMIn\", \"pacman\", \"performance\",\n\"performance\", \"relaimpo\", \"sjPlot\", \"tidyverse\", \"vegan\",\"FSA\",\ncharacter.only = TRUE)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html",
    "href": "statistik/Statistik1_Demo.html",
    "title": "Statistik 1: Demo",
    "section": "",
    "text": "t-Test",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html#t-test",
    "href": "statistik/Statistik1_Demo.html#t-test",
    "title": "Statistik 1: Demo",
    "section": "",
    "text": "Daten generieren und anschauen\n\n# Je 10 Messwerte für Sorte a und b zu einem Data Frame im long-Format verbinden \nMesswerte_a &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14) # Messwerte von Cultivar a\nMesswerte_b &lt;- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10) # Messwerte von Cultivar b\ncultivar &lt;- as.factor( c(rep(\"a\", 10), rep(\"b\", 10))) # Bezeichnug der Cultivare in der Tabelle\nblume &lt;- data.frame(\"cultivar\" = cultivar, \"size\" = c(Messwerte_a, Messwerte_b)) # Data frame erstellen \n\n\n# Boxplots\n\nlibrary(ggplot2)\n\nggplot(blume, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot() + # Boxplots\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) # Datenpunkte darstellen\n\n\n\n\n\n\n\n\n\n# Histogramme\n\nggplot(blume, aes(x = size, fill = cultivar)) +\n  geom_histogram(binwidth = 2) +\n  facet_wrap(~ cultivar)\n\n\n\n\n\n\n\n\n\n\nZweiseitiger t-Test\n\n# Links der Tilde (\"~\") steht immer die abhängige Variable, rechsts die erklärende(n) Variable(n)\n# Alternativ kann man die Werte auch direkt in die t.test()-Funktion eigeben:\n# t.test(Messwerte_a, Messwerte_b)\n\nt.test(size ~ cultivar, data = blume) # Zweiseitig \"Test auf a ≠ b\" (default)\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n\n\n\nEinseitiger t-Test\n\nt.test(size ~ cultivar, alternative = \"greater\",  data = blume) # Einseitig \"Test auf a &gt; b\"\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.02827\nalternative hypothesis: true difference in means between group a and group b is greater than 0\n95 percent confidence interval:\n 0.5954947       Inf\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\nt.test(size ~ cultivar, alternative = \"less\",  data = blume) # Einseitig \"Test auf a &lt; b\"\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.9717\nalternative hypothesis: true difference in means between group a and group b is less than 0\n95 percent confidence interval:\n     -Inf 7.204505\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n\n\n\nKlassischer t-Test vs. Welch Test\n\n# Varianzen gleich: klassischer t-Test\nt.test(size ~ cultivar, var.equal = TRUE, data = blume)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n# Varianzen ungleich: Welch's t-Test (siehe Titelzeile des R-Outputs!)\nt.test(size ~ cultivar, data = blume) # dasselbe wie var.equal = FALSE\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n\n\n\nGepaarter t-Test\n\n# Gepaarter t-Test: erster Wert von a wird mit erstem Wert von\n# b gepaart, zweiter Wert von a mit zweitem von b ect.\n\nt.test(Messwerte_a, Messwerte_b, paired = TRUE) # für gepaarten t-Test funktioniert Notation \"size ~ cultivar\" nicht\n\n\n    Paired t-test\n\ndata:  Messwerte_a and Messwerte_b\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.366339 6.433661\nsample estimates:\nmean difference \n            3.9",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html#binomialtest",
    "href": "statistik/Statistik1_Demo.html#binomialtest",
    "title": "Statistik 1: Demo",
    "section": "Binomialtest",
    "text": "Binomialtest\nIn Klammern übergibt man die Anzahl der Erfolge und die Stichprobengrösse\n\nbinom.test(84, 200) # Anzahl Frauen im Nationalrat (≙ 42.0 %; Stand 2019)\n\n\n    Exact binomial test\n\ndata:  84 and 200\nnumber of successes = 84, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.3507439 0.4916638\nsample estimates:\nprobability of success \n                  0.42 \n\nbinom.test(116, 200) # Anzahl Männer im Nationalrat (≙ 58.0 %; Stand 2019)\n\n\n    Exact binomial test\n\ndata:  116 and 200\nnumber of successes = 116, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5083362 0.6492561\nsample estimates:\nprobability of success \n                  0.58 \n\nbinom.test(3, 7) # Anzahl Frauen im Bundesrat (≙ 42.9 %; Stand 2019)\n\n\n    Exact binomial test\n\ndata:  3 and 7\nnumber of successes = 3, number of trials = 7, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.09898828 0.81594843\nsample estimates:\nprobability of success \n             0.4285714",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "href": "statistik/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "title": "Statistik 1: Demo",
    "section": "Chi-Quadrat-Test & Fishers Test",
    "text": "Chi-Quadrat-Test & Fishers Test\nErmitteln des kritischen Wertes\n\nqchisq(0.95, 1)\n\n[1] 3.841459\n\n\n\nDirekter Test in R (dazu Werte als Matrix nötig)\n\n# Matrix mit Haarfarbe&Augenfarbe-Kombinationen erstellen\n# 38 blond&blau, 14 dunkel&blau, 11 blond&braun, 51 dunkel&braun\ncount &lt;- matrix(c(38, 14, 11, 51), nrow = 2)\ncount # Check\n\n     [,1] [,2]\n[1,]   38   11\n[2,]   14   51\n\nrownames(count) &lt;- c(\"blond\", \"dunkel\") # Benennen für Übersicht\ncolnames(count) &lt;- c(\"blau\", \"braun\") #  Benennen für Übersicht\ncount # Check\n\n       blau braun\nblond    38    11\ndunkel   14    51\n\n# Tests durchführen\nchisq.test(count)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  count\nX-squared = 33.112, df = 1, p-value = 8.7e-09\n\nfisher.test(count)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  count\np-value = 2.099e-09\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  4.746351 34.118920\nsample estimates:\nodds ratio \n  12.22697",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html",
    "href": "statistik/Statistik1_Uebung.html",
    "title": "Statistik 1: Übung",
    "section": "",
    "text": "Anleitung zu den Übungen\nZu erstellen sind:\nWichtige Hinweise zur Ausgestaltung:",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html#aufgabe-1.1-assoziationstest",
    "href": "statistik/Statistik1_Uebung.html#aufgabe-1.1-assoziationstest",
    "title": "Statistik 1: Übung",
    "section": "Aufgabe 1.1: Assoziationstest",
    "text": "Aufgabe 1.1: Assoziationstest\nFührt einen Assoziationstest zweier kategorialer Variablen (mit je zwei Ausprägungen) mit Chi-Quadrat und Fishers exaktem Test durch. Dazu erhebt ihr selbst die Daten (wozu ihr euch auch in Teams zusammenschliessen könnt). Ihr könnt z.B. eine Datenerhebung unter Mitstudierenden durchführen (etwa Nutzung Mac/Windows vs. männlich/weiblich). Bitte formuliert vor der Datenerhebung eine Hypothese, d.h. eine Erwartungshaltung, ob und welche Assoziation vorliegt und wenn ja warum. Beachtet, dass ihr für diese Form des Assoziationstests genau zwei binäre Variablen benötigt. Wenn ihr also kategoriale Variablen mit mehr als zwei Ausprägungen habt, so könnt ihr entweder Ausprägungen sinnvoll zusammenfassen oder seltene Ausprägungen im Test unberücksichtigt lassen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "href": "statistik/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "title": "Statistik 1: Übung",
    "section": "Aufgabe 1.2: t-Test",
    "text": "Aufgabe 1.2: t-Test\n**T-Test mit Datensatz *Novanimal_Weeks.csv**\nIm Forschungsprojekt NOVANIMAL wurde u.a. in der ZHAW-Mensa untersucht, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren. Dazu wurden in sogenannten Interventionswochen mehr vegane und vegetarische Gerichte (zu Lasten kleinerer Fleischgerichtsauswahl) angeboten. Eine erste grundsätzliche Frage war, ob die unterschiedliche Gerichtsauswahl zu unterschiedlichen Verkaufszahlen führten. Beantwortet dazu mit dem Datensatz und geeigneten Tests folgende Frage:\nWurden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?\n\nSchau die Daten an: Verstehen und ggf. plotten.\nDefiniere die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)).\nWelche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart?\nFühre einen t-Test durch.\nWie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)?\nStelle deine Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html",
    "href": "statistik/Statistik2_Demo.html",
    "title": "Statistik 2: Demo",
    "section": "",
    "text": "ANOVA",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#anova",
    "href": "statistik/Statistik2_Demo.html#anova",
    "title": "Statistik 2: Demo",
    "section": "",
    "text": "t-test als ANOVA\n\n# Dieselben Daten wie für die t-Tests in Statistik 1\nMesswerte_a &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14) # Messwerte von Cultivar a\nMesswerte_b &lt;- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10) # Messwerte von Cultivar b\ncultivar &lt;- as.factor( c(rep(\"a\", 10), rep(\"b\", 10))) # Bezeichnug der Cultivare in der Tabelle\nblume &lt;- data.frame(\"cultivar\" = cultivar, \"size\" = c(Messwerte_a, Messwerte_b)) # Data frame erstellen\n\n# Daten anschauen\nlibrary(ggplot2)\n\nggplot(blume, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot() + # Boxplots\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) # Datenpunkte darstellen\n\n\n\n\n\n\n\n# Klassischer t-Test ausführen\nt.test(size ~ cultivar, var.equal = TRUE, data = blume)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n# ANOVA ausführen\naov(size ~ cultivar, data = blume)\n\nCall:\n   aov(formula = size ~ cultivar, data = blume)\n\nTerms:\n                cultivar Residuals\nSum of Squares     76.05    316.50\nDeg. of Freedom        1        18\n\nResidual standard error: 4.193249\nEstimated effects may be unbalanced\n\nsummary( aov(size ~ cultivar, data = blume))\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ncultivar     1   76.0   76.05   4.325 0.0521 .\nResiduals   18  316.5   17.58                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm( aov(size ~ cultivar, data = blume))\n\n\nCall:\naov(formula = size ~ cultivar, data = blume)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -2.575 -0.350  2.925  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.300      1.326   11.54 9.47e-10 ***\ncultivarb     -3.900      1.875   -2.08   0.0521 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.193 on 18 degrees of freedom\nMultiple R-squared:  0.1937,    Adjusted R-squared:  0.1489 \nF-statistic: 4.325 on 1 and 18 DF,  p-value: 0.05212\n\n\n\n\nEchte ANOVA\n\n# Ein weiterer Cultivar hinzufügen\nMesswerte_c &lt;-c(30, 19, 31, 23, 18, 25, 26, 24, 17, 20)\ncultivar &lt;- as.factor( c(rep(\"a\", 10), rep(\"b\", 10), rep(\"c\", 10))) # Bezeichnug der Cultivare in der Tabelle\nblume2 &lt;- data.frame(\"cultivar\" = cultivar, \"size\" = c(Messwerte_a, Messwerte_b, Messwerte_c)) # Data frame erstellen\n\n\n# Daten als Boxplots anschauen\nggplot(blume2, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot()  + # Boxplots\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) # Datenpunkte darstellen\n\n\n\n\n\n\n\n# Kennzahlen der Daten anschauen\n\nlibrary(\"dplyr\")\n\nblume2 |&gt; \n  group_by(cultivar) |&gt;\n  summarise(\n    Mean = mean(size), \n    SD = sd(size),\n    Min = min(size),\n    Max = max(size)\n  )\n\n# A tibble: 3 × 5\n  cultivar  Mean    SD   Min   Max\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a         15.3  5.21     8    25\n2 b         11.4  2.84     7    16\n3 c         23.3  4.85    17    31\n\n# ANOVA durchführen\naov1 &lt;- aov(size ~ cultivar, data = blume2)\naov1\n\nCall:\n   aov(formula = size ~ cultivar, data = blume2)\n\nTerms:\n                cultivar Residuals\nSum of Squares  736.0667  528.6000\nDeg. of Freedom        2        27\n\nResidual standard error: 4.424678\nEstimated effects may be unbalanced\n\nsummary(aov1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov1)\n\n\nCall:\naov(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n# Direkt als lineares Modell\nlm1 &lt;- lm(size ~ cultivar, data = blume2)\nsummary(lm1)\n\n\nCall:\nlm(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n\n\n\nTukeys Posthoc-Test\n\n# Load library\nlibrary(\"agricolae\")\n\n# Sorten mit ihren Namen bezeichnen, damit keine Verwechslung mit den Post-Hoc-Labels entsteht\nblume2n &lt;- blume2 \nblume2n$cultivar &lt;- recode(blume2n$cultivar, \"a\" = \"Andro\", \"b\" = \"Bulli\", \"c\" = \"Chroma\")\n\n# ANOVA und Posthoc-Test durchführen\naov1 &lt;- aov(size ~ cultivar, data = blume2n)\nposthoc &lt;- HSD.test(aov1, \"cultivar\", console = TRUE)\n\n\nStudy: aov1 ~ \"cultivar\"\n\nHSD Test for size \n\nMean Square Error:  19.57778 \n\ncultivar,  means\n\n       size      std  r       se Min Max   Q25  Q50   Q75\nAndro  15.3 5.207900 10 1.399206   8  25 11.50 14.5 18.75\nBulli  11.4 2.836273 10 1.399206   7  16 10.00 11.5 12.75\nChroma 23.3 4.854551 10 1.399206  17  31 19.25 23.5 25.75\n\nAlpha: 0.05 ; DF Error: 27 \nCritical Value of Studentized Range: 3.506426 \n\nMinimun Significant Difference: 4.906213 \n\nTreatments with the same letter are not significantly different.\n\n       size groups\nChroma 23.3      a\nAndro  15.3      b\nBulli  11.4      b\n\nposthoc\n\n$statistics\n   MSerror Df     Mean       CV      MSD\n  19.57778 27 16.66667 26.54807 4.906213\n\n$parameters\n   test   name.t ntr StudentizedRange alpha\n  Tukey cultivar   3         3.506426  0.05\n\n$means\n       size      std  r       se Min Max   Q25  Q50   Q75\nAndro  15.3 5.207900 10 1.399206   8  25 11.50 14.5 18.75\nBulli  11.4 2.836273 10 1.399206   7  16 10.00 11.5 12.75\nChroma 23.3 4.854551 10 1.399206  17  31 19.25 23.5 25.75\n\n$comparison\nNULL\n\n$groups\n       size groups\nChroma 23.3      a\nAndro  15.3      b\nBulli  11.4      b\n\nattr(,\"class\")\n[1] \"group\"\n\n# Darstellung der Ergebnisse mit Post-Hoc-Labels über Boxplots\n\n# Labels des Posthoc-Tests extrahieren\nlabels &lt;- posthoc$groups\nlabels$cultivar &lt;- rownames(labels)\n\n#In Plot darstellen\nggplot(blume2n, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) +\n  geom_text(data = labels, aes(x = cultivar, y = 33, label = groups)) \n\n\n\n\n\n\n\n\n\n\n2-faktorielle ANOVA\n\n# Daten generieren\nMesswerte_d &lt;- c(10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\nMesswerte_e &lt;- c(15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nMesswerte_f &lt;- c(10, 12, 11, 13, 10, 9, 2, 4, 7, 13)\n\n\nblume3 &lt;- data.frame(\n    cultivar = c(rep(\"a\", 20), rep(\"b\", 20), rep(\"c\", 20)),\n    house = as.factor( c(rep(c(rep(\"yes\", 10), rep(\"no\", 10)), 3))),\n    size = c(Messwerte_a, Messwerte_b, Messwerte_c, Messwerte_d, Messwerte_e, Messwerte_f)\n)\n\nblume3\n\n   cultivar house size\n1         a   yes   20\n2         a   yes   19\n3         a   yes   25\n4         a   yes   10\n5         a   yes    8\n6         a   yes   15\n7         a   yes   13\n8         a   yes   18\n9         a   yes   11\n10        a   yes   14\n11        a    no   12\n12        a    no   15\n13        a    no   16\n14        a    no    7\n15        a    no    8\n16        a    no   10\n17        a    no   12\n18        a    no   11\n19        a    no   13\n20        a    no   10\n21        b   yes   30\n22        b   yes   19\n23        b   yes   31\n24        b   yes   23\n25        b   yes   18\n26        b   yes   25\n27        b   yes   26\n28        b   yes   24\n29        b   yes   17\n30        b   yes   20\n31        b    no   10\n32        b    no   12\n33        b    no   11\n34        b    no   13\n35        b    no   10\n36        b    no   25\n37        b    no   12\n38        b    no   30\n39        b    no   26\n40        b    no   13\n41        c   yes   15\n42        c   yes   13\n43        c   yes   18\n44        c   yes   11\n45        c   yes   14\n46        c   yes   25\n47        c   yes   39\n48        c   yes   38\n49        c   yes   28\n50        c   yes   24\n51        c    no   10\n52        c    no   12\n53        c    no   11\n54        c    no   13\n55        c    no   10\n56        c    no    9\n57        c    no    2\n58        c    no    4\n59        c    no    7\n60        c    no   13\n\n\n\n# Daten mit Boxplots anschauen\n# Base-R-Variante wäre: boxplot(size ~ cultivar + house, data = blume3)\nggplot(blume3, aes(x = interaction(cultivar, house), y = size, fill = cultivar)) +\n  geom_boxplot() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5)\n\n\n\n\n\n\n\nsummary( aov(size ~ cultivar + house, data = blume3))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  417.1   208.5   5.005     0.01 *  \nhouse        1  992.3   992.3  23.815 9.19e-06 ***\nResiduals   56 2333.2    41.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary( aov(size ~ cultivar + house + cultivar:house, data = blume3))\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar        2  417.1   208.5   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Kurzschreibweise: \"*\" bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird\nsummary( aov(size ~ cultivar * house, data = blume3))\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar        2  417.1   208.5   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm( aov(size ~ cultivar + house, data = blume3))\n\n\nCall:\naov(formula = size ~ cultivar + house, data = blume3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.733 -4.696 -1.050  2.717 19.133 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    9.283      1.667   5.570 7.52e-07 ***\ncultivarb      6.400      2.041   3.135  0.00273 ** \ncultivarc      2.450      2.041   1.200  0.23509    \nhouseyes       8.133      1.667   4.880 9.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.455 on 56 degrees of freedom\nMultiple R-squared:  0.3766,    Adjusted R-squared:  0.3432 \nF-statistic: 11.28 on 3 and 56 DF,  p-value: 6.848e-06\n\n# Interaktionsplots\n\ninteraction.plot(blume3$cultivar, blume3$house, blume3$size)\n\n\n\n\n\n\n\ninteraction.plot(blume3$house, blume3$cultivar, blume3$size)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#modelldiagnostik",
    "href": "statistik/Statistik2_Demo.html#modelldiagnostik",
    "title": "Statistik 2: Demo",
    "section": "Modelldiagnostik",
    "text": "Modelldiagnostik\n\nBeispiel Modelldiagnostik\n\n# Beispiel mit den blume Daten\npar(mfrow = c(2, 2)) # 4 Plots in einem Fenster\nplot( lm(Messwerte_b ~ Messwerte_a))\n\n\n\n\n\n\n\n\n\n\nBeispiel Modelldiagnostik nicht ok\n\n# Daten erstellen\ng &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nh &lt;- c(12, 15, 10, 7, 8, 10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\nBsp &lt;- data.frame(g, h)\n\n# Daten betrachten\nggplot(Bsp, aes(x = g, y = h)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(lm(h ~ g))",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "href": "statistik/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "title": "Statistik 2: Demo",
    "section": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind",
    "text": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind\n\n# Nicht-parametrische Alternative zu t-Test\nwilcox.test(Messwerte_a, Messwerte_b)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Messwerte_a and Messwerte_b\nW = 73, p-value = 0.08789\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#kruskal-wallis-test-bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "href": "statistik/Statistik2_Demo.html#kruskal-wallis-test-bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "title": "Statistik 2: Demo",
    "section": "Kruskal-Wallis-Test bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen",
    "text": "Kruskal-Wallis-Test bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen\n\n# Zum Vergleich normale ANOVA noch mal\nsummary(aov(size ~ cultivar, data = blume2))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Kruskal-Wallis-Test\nkruskal.test(size ~ cultivar, data = blume2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  size by cultivar\nKruskal-Wallis chi-squared = 16.686, df = 2, p-value = 0.0002381\n\n\n\n# Load library\nlibrary(\"FSA\")\n\n# Post-Hoc mit korrigierten p-Werte nach Bejamini-Hochberg\ndunnTest(size ~ cultivar, method = \"bh\", data = blume2)\n\n  Comparison         Z      P.unadj        P.adj\n1      a - b  1.526210 1.269575e-01 0.1269575490\n2      a - c -2.518247 1.179407e-02 0.0176911039\n3      b - c -4.044457 5.244459e-05 0.0001573338",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#welch-test-bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "href": "statistik/Statistik2_Demo.html#welch-test-bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "title": "Statistik 2: Demo",
    "section": "Welch-Test bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen",
    "text": "Welch-Test bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen\n\n# Welch-Test\noneway.test(size ~ cultivar, var.equal = F, data = blume2)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  size and cultivar\nF = 21.642, num df = 2.000, denom df = 16.564, p-value = 2.397e-05",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Uebung.html",
    "href": "statistik/Statistik2_Uebung.html",
    "title": "Statistik 2: Übung",
    "section": "",
    "text": "Aufgabe 2.1 ANOVA mit Kormoran-Daten\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLesen Sie den Datensatz kormoran.csv ein, Dieser enthält Tauchzeiten (hier ohne Einheit) von Kormoranen in Abhängigkeit von Jahreszeit und Unterart. Unterarten: Phalacrocorax carbo carbo (C) und Phalacrocorax carbo sinensis (S); Jahreszeiten: F = Frühling, S = Sommer, H = Herbst, W = Winter.\nIhre Gesamtaufgabe ist es, aus diesen Daten ein minimal adäquates Modell zu ermitteln, das diese Abhängigkeit beschreibt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen, welches statistische Verfahren wenden Sie an?\nExplorative Datenanalyse, um zu sehen, ob schon vor dem Start der Analysen Transformationen o.ä. vorgenommen werden sollten\nDefinition eines vollen Modelles, das nach statistischen Kritierien zum minimal adäquaten Modell reduziert wird\nDurchführen der Modelldiagnostik, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\n\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nZu erstellen sind (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistik 2: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Demo.html",
    "href": "statistik/Statistik3_Demo.html",
    "title": "Statistik 3: Demo",
    "section": "",
    "text": "Korrelation vs. Regression\n## Korrelationen und Regressionen\n\n# Datensatz zum Einfluss von Stickstoffdepositionen auf den Pflanzenartenreichtum\nlibrary(readr)\nlibrary(ggplot2)\n\ndf &lt;- read_delim(\"datasets/stat/Nitrogen.csv\", \";\")\n\nsummary(df)\n\n  N_deposition   Species_richness\n Min.   : 2.00   Min.   :12.0    \n 1st Qu.: 9.00   1st Qu.:17.5    \n Median :20.00   Median :21.0    \n Mean   :20.53   Mean   :20.2    \n 3rd Qu.:30.50   3rd Qu.:23.0    \n Max.   :55.00   Max.   :28.0    \n\n# Plotten der Beziehung\nggplot(df, aes(x = N_deposition, y = Species_richness)) +\n  geom_point()\n# Pearson Korrelation\n# zuerst Species_richness dann N_deposition\ncor.test(df$Species_richness, df$N_deposition, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  df$Species_richness and df$N_deposition\nt = -5.2941, df = 13, p-value = 0.0001453\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9405572 -0.5450218\nsample estimates:\n       cor \n-0.8265238 \n\n# Pearson Korrelation\n# zuerst Species_richness dann N_deposition\ncor.test(df$N_deposition, df$Species_richness, method = \"pearson\") # zuerst N_deposition dann Species_richness  \n\n\n    Pearson's product-moment correlation\n\ndata:  df$N_deposition and df$Species_richness\nt = -5.2941, df = 13, p-value = 0.0001453\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9405572 -0.5450218\nsample estimates:\n       cor \n-0.8265238 \n\n# Rang-Korrelation Spearman\ncor.test(df$Species_richness, df$N_deposition, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  df$Species_richness and df$N_deposition\nS = 1015.5, p-value = 0.0002259\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.8133721 \n\n# Rang-Korrelation Kendall\ncor.test(df$Species_richness, df$N_deposition, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  df$Species_richness and df$N_deposition\nz = -3.308, p-value = 0.0009398\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n-0.657115 \n\n# Jetzt als Regression\nlm1 &lt;- lm(Species_richness ~ N_deposition, data = df) # zuerst Species_richness dann N_deposition\nlm1\n\n\nCall:\nlm(formula = Species_richness ~ N_deposition, data = df)\n\nCoefficients:\n (Intercept)  N_deposition  \n     25.6050       -0.2632  \n\nlm2 &lt;- lm(N_deposition ~ Species_richness, data = df) # zuerst N_deposition dann Species_richness  \nlm2\n\n\nCall:\nlm(formula = N_deposition ~ Species_richness, data = df)\n\nCoefficients:\n     (Intercept)  Species_richness  \n          72.957            -2.595  \n\nanova(lm1) # ANOVA-Tabelle, 1. Möglichkeit\n\nAnalysis of Variance Table\n\nResponse: Species_richness\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nN_deposition  1 233.91 233.908  28.028 0.0001453 ***\nResiduals    13 108.49   8.346                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.aov(lm1) # ANOVA-Tabelle, 2. Möglichkeit\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nN_deposition  1  233.9  233.91   28.03 0.000145 ***\nResiduals    13  108.5    8.35                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm1) # Regressionskoeffizienten\n\n\nCall:\nlm(formula = Species_richness ~ N_deposition, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9184 -1.9992  0.4493  2.0015  4.6081 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.60502    1.26440  20.251 3.25e-11 ***\nN_deposition -0.26323    0.04972  -5.294 0.000145 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.889 on 13 degrees of freedom\nMultiple R-squared:  0.6831,    Adjusted R-squared:  0.6588 \nF-statistic: 28.03 on 1 and 13 DF,  p-value: 0.0001453\n\n# Signifikantes Ergebnis visualisieren\n\nggplot(df, aes(x = N_deposition, y = Species_richness)) +\n  geom_point() +\n  geom_abline(intercept = lm1$coefficients[1], slope = lm1$coefficients[2], color = \"blue\")\n\n\n\n\n\n\n\n#BaseR Variante plot(Species_richness ~ N_deposition, data = df) + abline(lm1)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Statistik 3: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Demo.html#einfache-und-polynomische-regression",
    "href": "statistik/Statistik3_Demo.html#einfache-und-polynomische-regression",
    "title": "Statistik 3: Demo",
    "section": "Einfache und Polynomische Regression",
    "text": "Einfache und Polynomische Regression\n\n# Daten generieren \npred &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24) # \"pred\" sei unsere unabhängige Variable\nresp &lt;- c(12, 15, 10, 7, 2, 10, 12, 11, 13, 10, 9, 2, 4, 7, 13) # \"resp\" sei unsere abhängige Variable\ndata &lt;- data.frame(pred, resp) # Dataframe erstellen\n\n# Daten anschauen\nggplot(data, aes(x = pred, y = resp)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# Modell definieren ud anschauen\nlm_1 &lt;- lm(resp ~ pred) # Einfaches lineares Modell\nsummary(lm_1) # Modell anschauen\n\n\nCall:\nlm(formula = resp ~ pred)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0549 -1.7015  0.5654  2.0617  5.6406 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  12.2879     2.4472   5.021 0.000234 ***\npred         -0.1541     0.1092  -1.412 0.181538    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 13 degrees of freedom\nMultiple R-squared:  0.1329,    Adjusted R-squared:  0.06622 \nF-statistic: 1.993 on 1 and 13 DF,  p-value: 0.1815\n\n\n-&gt; kein signifikanter Zusammenhang im einfachen linearen Modell und entsprechend kleines Bestimmtheitsmass (adj. R2 = 0.07)\n\n# Polynomische Regression Modell definieren und anschauen\nlm_quad &lt;- lm(resp ~ pred + I(pred^2)) # lineares Modell mit quadratischem Termsummary\n\nsummary(lm_quad) # lineares Modell mit quadratischem Term anschauen\n\n\nCall:\nlm(formula = resp ~ pred + I(pred^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3866 -1.1018 -0.2027  1.3831  4.4211 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -2.239308   3.811746  -0.587  0.56777   \npred         1.330933   0.360105   3.696  0.00306 **\nI(pred^2)   -0.031587   0.007504  -4.209  0.00121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.555 on 12 degrees of freedom\nMultiple R-squared:  0.6499,    Adjusted R-squared:  0.5915 \nF-statistic: 11.14 on 2 and 12 DF,  p-value: 0.001842\n\n\n-&gt; Signifikanter Zusammenhang und viel besseres Bestimmtheitsmass (adj. R2 = 0.60)\n\n# Modelle darstellen\n\n# Vorhersagen der Modelle generieren\nxv &lt;- seq(min(pred), max(pred), length = 100) # 100 x-Werte, mit denen man die Modelle \"füttern\" kann \ny_lm1 &lt;- predict(lm_1, data.frame(pred = xv)) # Vorhersagen des quadratischen Modells für die y-Werte\ny_lm_quad &lt;- predict(lm_quad, data.frame(pred = xv)) # Vorhersagen des quadratischen Modells für die y-Werte\nModPred &lt;- data.frame(xv, y_lm1, y_lm_quad)\n\n# Modellvorhersagen plotten\n\nggplot(data, aes(x = pred, y = resp)) +\n  geom_point() +\n  geom_line(data = ModPred, aes(x = xv, y = y_lm1), color = \"red\", linetype = \"dashed\") + \n  geom_line(data = ModPred, aes(x = xv, y = y_lm_quad), color = \"blue\")\n\n\n\n\n\n\n\n\n\n# Residualplots\npar(mfrow = c(2, 2))\nplot(lm_1, main = \"Lineares Modell\")\n\n\n\n\n\n\n\nplot(lm_quad, main = \"Quadratisches  Modell\")\n\n\n\n\n\n\n\n\n-&gt; Die Plots sehen beim Modell mit quadratischem Term besser aus",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Statistik 3: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Demo.html#ancova",
    "href": "statistik/Statistik3_Demo.html#ancova",
    "title": "Statistik 3: Demo",
    "section": "ANCOVA",
    "text": "ANCOVA\nExperiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. in Abhängigkeit von der Beweidung (“Grazing” mit 2 Levels: “Grazed”, “Ungrazed”) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)\n\n# Daten einlesen und anschauen\nlibrary(\"readr\")\n\ncompensation &lt;- read_delim(\"datasets/stat/ipomopsis.csv\")\ncompensation$Grazing &lt;- as.factor(compensation$Grazing)\n\nhead(compensation)\n\n# A tibble: 6 × 3\n   Root Fruit Grazing \n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1  6.22  59.8 Ungrazed\n2  6.49  61.0 Ungrazed\n3  4.92  14.7 Ungrazed\n4  5.13  19.3 Ungrazed\n5  5.42  34.2 Ungrazed\n6  5.36  35.5 Ungrazed\n\nsummary(compensation)\n\n      Root            Fruit            Grazing  \n Min.   : 4.426   Min.   : 14.73   Grazed  :20  \n 1st Qu.: 6.083   1st Qu.: 41.15   Ungrazed:20  \n Median : 7.123   Median : 60.88                \n Mean   : 7.181   Mean   : 59.41                \n 3rd Qu.: 8.510   3rd Qu.: 76.19                \n Max.   :10.253   Max.   :116.05                \n\n# Plotten der vollständigen Daten/Information\nlibrary(\"ggplot2\")\nggplot(compensation, aes(x = Root, y = Fruit, color = Grazing)) +\n  geom_point()\n\n\n\n\n\n\n\n\n-&gt; Je grösser die Pflanze, desto grösser ihre Fruchtproduktion. -&gt; Die grössere Fruchtproduktion innerhalb der beweideten Gruppe scheint ein Resultat von unterschiedlichen Pflanzengrössen zwischen den Gruppen zu sein.\n\n# Lineare Modelle definieren und anschauen\n\naoc_1 &lt;- lm(Fruit ~ Root * Grazing, data = compensation) # Volles Modell mit Interaktion\nsummary.aov(aoc_1)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nRoot          1  16795   16795 359.968  &lt; 2e-16 ***\nGrazing       1   5264    5264 112.832 1.21e-12 ***\nRoot:Grazing  1      5       5   0.103     0.75    \nResiduals    36   1680      47                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naoc_2 &lt;- lm(Fruit ~ Grazing + Root, data = compensation) # Finales Modell ohne die (nicht signifikante) Interaktion\nsummary.aov(aoc_2) # ANOVA-Tabelle\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nGrazing      1   2910    2910   63.93 1.4e-09 ***\nRoot         1  19149   19149  420.62 &lt; 2e-16 ***\nResiduals   37   1684      46                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aoc_2) # Parameter-Tabelle\n\n\nCall:\nlm(formula = Fruit ~ Grazing + Root, data = compensation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1920  -2.8224   0.3223   3.9144  17.3290 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -127.829      9.664  -13.23 1.35e-15 ***\nGrazingUngrazed   36.103      3.357   10.75 6.11e-13 ***\nRoot              23.560      1.149   20.51  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.747 on 37 degrees of freedom\nMultiple R-squared:  0.9291,    Adjusted R-squared:  0.9252 \nF-statistic: 242.3 on 2 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Residualplots anschauen\npar(mfrow = c(2, 2)) # 2x2 Plots pro Grafik\nplot(aoc_2)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1)) # Grafik zurücksetzen\n\n-&gt; Das ANCOVA-Modell widerspiegelt die Zusammenhänge wie sie aufgrund der grafisch dargestellten Daten zu vermuten sind gut. Die Residual-Plots zeigen 3 Ausreisser (Beobachtungen 27, 34 und 37), welche “aus der Reihe tanzen”.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Statistik 3: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik4_Demo.html",
    "href": "statistik/Statistik4_Demo.html",
    "title": "Statistik 4: Demo",
    "section": "",
    "text": "Korrelation zwischen den Prädiktoren\n# Wir setzen die Schwelle bei |0.7|\n# Korrelationen rechnen details siehe: \"?cor\"\ncor(loyn[, 2:7]) \n##              AREA         AGE       DIST       LDIST       GRAZE         ALT\n## AREA   1.00000000 -0.21265343  0.2475258  0.37733668 -0.53118408  0.08935845\n## AGE   -0.21265343  1.00000000 -0.1132931  0.09930812  0.66129235 -0.27242916\n## DIST   0.24752583 -0.11329311  1.0000000  0.31814676 -0.24330458 -0.15112326\n## LDIST  0.37733668  0.09930812  0.3181468  1.00000000 -0.02373893 -0.32359264\n## GRAZE -0.53118408  0.66129235 -0.2433046 -0.02373893  1.00000000 -0.35362007\n## ALT    0.08935845 -0.27242916 -0.1511233 -0.32359264 -0.35362007  1.00000000\n# oder mit Namen der columns resp. variablen\ncor &lt;- loyn |&gt;\n  subset(select = AREA:ALT) |&gt;\n  cor()\n\n# Korrelationen Visualisieren\np_load(\"corrplot\")\ncorrplot.mixed(cor, lower = \"ellipse\", upper = \"number\", order = \"AOE\")\n\n\n\n\n\n\n\n\ncor[abs(cor)&lt;0.7] &lt;- 0\ncor\n##       AREA AGE DIST LDIST GRAZE ALT\n## AREA     1   0    0     0     0   0\n## AGE      0   1    0     0     0   0\n## DIST     0   0    1     0     0   0\n## LDIST    0   0    0     1     0   0\n## GRAZE    0   0    0     0     1   0\n## ALT      0   0    0     0     0   1\n-&gt; Keine Korrelation ist &gt;|0.7|, so können wir alle Prädiktoren “behalten”. Aber es gilt zu beachten, dass GRAZE ziemlich stark |&gt;0.6| mit AGE korreliert ist\n# Volles Modell definieren\nnames(loyn)\n## [1] \"ABUND\" \"AREA\"  \"AGE\"   \"DIST\"  \"LDIST\" \"GRAZE\" \"ALT\"\nlm_1 &lt;- lm(ABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\n\npar(mfrow = c(2, 2))\nplot(lm_1)\n-&gt; Plot sieht zwar OK aus, aber mit 6 Prädiktoren für |&lt;60| Beobachtungen ist das Modell wohl “overfitted”\n# Andere Vatiante, um korrelierte Prädiktoren zu finden (üblicher Schwellenwert VIF = 5)\np_load(\"car\")\nvif(lm_1)\n##      AGE     AREA     DIST    LDIST    GRAZE      ALT \n## 1.874993 1.763605 1.220125 1.465810 2.784577 1.346572",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistik 4: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik4_Demo.html#multimodel-inference",
    "href": "statistik/Statistik4_Demo.html#multimodel-inference",
    "title": "Statistik 4: Demo",
    "section": "Multimodel inference",
    "text": "Multimodel inference\n\np_load(\"MuMIn\")\n\nglobal_model &lt;- lm(ABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\n\noptions(na.action = \"na.fail\")\nallmodels &lt;- dredge(global_model)\nallmodels\n## Global model call: lm(formula = ABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT, \n##     data = loyn)\n## ---\n## Model selection table \n##    (Intrc)      AGE     ALT   AREA     DIST  GRAZE      LDIST df   logLik  AICc\n## 24  19.460 -0.09049 0.04249 0.1257          -1.954             6 -181.648 377.1\n## 22  27.250 -0.09295         0.1187          -2.434             5 -183.025 377.3\n## 23  20.180          0.04379 0.1120          -3.172             5 -183.186 377.6\n## 8   13.020 -0.14830 0.05448 0.1607                             5 -183.224 377.7\n## 21  28.230                  0.1045          -3.701             4 -184.568 378.0\n## 16  10.990 -0.14310 0.06018 0.1522 0.005130                    6 -182.621 379.0\n## 32  17.440 -0.09167 0.04764 0.1226 0.003705 -1.787             7 -181.328 379.1\n## 31  18.300          0.04861 0.1090 0.003463 -3.031             6 -182.922 379.6\n## 54  27.240 -0.09146         0.1257          -2.375 -4.979e-04  6 -182.936 379.7\n## 56  19.240 -0.09093 0.04375 0.1235          -1.960  1.699e-04  7 -181.638 379.7\n## 30  26.780 -0.09359         0.1170 0.001608 -2.386             6 -182.964 379.7\n## 6   21.570 -0.17130         0.1629                             4 -185.480 379.8\n## 53  28.190                  0.1137          -3.600 -6.372e-04  5 -184.429 380.1\n## 55  20.130          0.04406 0.1116          -3.174  3.620e-05  6 -183.185 380.2\n## 40  12.900 -0.14860 0.05513 0.1596                  8.597e-05  6 -183.221 380.2\n## 29  27.850                  0.1030 0.001316 -3.669             5 -184.530 380.3\n## 48  11.230 -0.14210 0.05871 0.1546 0.005306        -2.191e-04  7 -182.606 381.6\n## 38  21.780 -0.16570         0.1728                 -8.285e-04  5 -185.250 381.7\n## 14  20.930 -0.16970         0.1583 0.002886                    5 -185.298 381.8\n## 64  17.490 -0.09155 0.04731 0.1232 0.003751 -1.783 -5.331e-05  8 -181.327 381.9\n## 62  26.540 -0.09188         0.1256 0.002372 -2.285 -6.734e-04  7 -182.814 382.1\n## 63  18.450          0.04749 0.1112 0.003619 -3.012 -1.800e-04  7 -182.912 382.3\n## 61  27.540                  0.1136 0.002233 -3.521 -8.030e-04  6 -184.327 382.4\n## 17  34.250                                  -4.951             3 -188.337 383.2\n## 46  20.950 -0.16160         0.1696 0.004052        -1.107e-03  6 -184.914 383.6\n## 19  28.380          0.03376                 -4.613             4 -187.620 384.1\n## 18  34.150 -0.05604                         -4.290             4 -187.835 384.5\n## 51  24.960          0.04669                 -4.461  1.504e-03  5 -186.871 385.0\n## 49  33.610                                  -4.939  8.194e-04  4 -188.092 385.0\n## 25  33.120                         0.003213 -4.831             4 -188.133 385.1\n## 27  25.260          0.04131        0.005130 -4.345             5 -187.120 385.5\n## 20  28.540 -0.05253 0.03230                 -4.008             5 -187.169 385.6\n## 50  33.340 -0.06356                         -4.185  1.023e-03  5 -187.453 386.2\n## 52  24.690 -0.06351 0.04667                 -3.708  1.707e-03  6 -186.204 386.2\n## 26  32.900 -0.05863                0.003557 -4.126             5 -187.581 386.4\n## 28  25.270 -0.05561 0.04017        0.005404 -3.690             6 -186.605 387.0\n## 59  23.190          0.05027        0.003783 -4.287  1.273e-03  6 -186.611 387.0\n## 7    4.042          0.07835 0.1830                             4 -189.182 387.2\n## 57  32.930                         0.002318 -4.855  6.458e-04  5 -187.996 387.2\n## 15   1.681          0.08504 0.1703 0.007048                    5 -188.251 387.8\n## 60  22.870 -0.06421 0.05035        0.003888 -3.521  1.472e-03  7 -185.922 388.3\n## 58  32.630 -0.06400                0.002421 -4.092  8.429e-04  6 -187.346 388.5\n## 39   4.728          0.07497 0.1878                 -4.146e-04  5 -189.137 389.5\n## 47   2.837          0.07890 0.1788 0.007662        -8.235e-04  6 -188.077 389.9\n## 5   15.090                  0.1918                             3 -193.075 392.6\n## 37  15.990                  0.2110                 -1.793e-03  4 -192.219 393.3\n## 45  14.910                  0.2046 0.006347        -2.192e-03  5 -191.569 394.4\n## 13  14.250                  0.1847 0.004158                    4 -192.789 394.4\n## 36  12.550 -0.19070 0.07799                         2.643e-03  5 -192.165 395.6\n## 12  12.580 -0.17450 0.07003        0.010510                    5 -192.232 395.7\n## 44  10.040 -0.17890 0.08236        0.008064         2.057e-03  6 -191.118 396.0\n## 4   17.190 -0.18960 0.05881                                    4 -194.072 397.0\n## 2   26.500 -0.21500                                            3 -195.849 398.2\n## 10  24.300 -0.20710                0.008133                    4 -194.795 398.4\n## 34  25.520 -0.22070                                 1.612e-03  4 -195.130 399.1\n## 42  24.050 -0.21240                0.006624         1.084e-03  5 -194.496 400.2\n## 11   1.164          0.10260        0.013710                    4 -198.209 405.2\n## 43  -1.215          0.11370        0.011720         1.732e-03  5 -197.580 406.4\n## 35   1.436          0.11030                         2.577e-03  4 -199.349 407.5\n## 3    6.024          0.09136                                    3 -200.752 408.0\n## 9   16.550                         0.010850                    3 -202.851 412.2\n## 1   19.120                                                     2 -204.254 412.7\n## 41  16.470                         0.010600         1.941e-04  4 -202.843 414.5\n## 33  18.390                                          1.002e-03  3 -204.050 414.6\n##    delta weight\n## 24  0.00  0.127\n## 22  0.22  0.114\n## 23  0.54  0.097\n## 8   0.61  0.093\n## 21  0.87  0.082\n## 16  1.95  0.048\n## 32  2.01  0.046\n## 31  2.55  0.035\n## 54  2.58  0.035\n## 56  2.63  0.034\n## 30  2.63  0.034\n## 6   2.69  0.033\n## 53  3.03  0.028\n## 55  3.08  0.027\n## 40  3.15  0.026\n## 29  3.23  0.025\n## 48  4.56  0.013\n## 38  4.67  0.012\n## 14  4.76  0.012\n## 64  4.77  0.012\n## 62  4.98  0.011\n## 63  5.18  0.010\n## 61  5.36  0.009\n## 17  6.07  0.006\n## 46  6.53  0.005\n## 19  6.97  0.004\n## 18  7.40  0.003\n## 51  7.91  0.002\n## 49  7.92  0.002\n## 25  8.00  0.002\n## 27  8.41  0.002\n## 20  8.50  0.002\n## 50  9.07  0.001\n## 52  9.11  0.001\n## 26  9.33  0.001\n## 28  9.91  0.001\n## 59  9.93  0.001\n## 7  10.10  0.001\n## 57 10.16  0.001\n## 15 10.67  0.001\n## 60 11.20  0.000\n## 58 11.40  0.000\n## 39 12.44  0.000\n## 47 12.86  0.000\n## 5  15.55  0.000\n## 37 16.17  0.000\n## 45 17.30  0.000\n## 13 17.31  0.000\n## 36 18.50  0.000\n## 12 18.63  0.000\n## 44 18.94  0.000\n## 4  19.88  0.000\n## 2  21.10  0.000\n## 10 21.32  0.000\n## 34 21.99  0.000\n## 42 23.16  0.000\n## 11 28.15  0.000\n## 43 29.33  0.000\n## 35 30.43  0.000\n## 3  30.90  0.000\n## 9  35.10  0.000\n## 1  35.66  0.000\n## 41 37.42  0.000\n## 33 37.50  0.000\n## Models ranked by AICc(x)\n# Wir haben mehre Modelle mit einem delta AICc &lt;2, das heisst wir haben nicht ein eindeutig bestes Modell (welches wir mit der funktion \"get.models\" selektieren könnten)\n\n# Variable importance\nsw(allmodels)\n##                      AREA GRAZE AGE  ALT  DIST LDIST\n## Sum of weights:      0.97 0.76  0.66 0.58 0.27 0.23 \n## N containing models:   32   32    32   32   32   32\n\n-&gt; Auch mit dieser Sichtweise sind AREA und GRAZE die wichtigste Prädiktoren\n\n# Model averaging\navgmodel &lt;- model.avg(allmodels)\nsummary(avgmodel)\n## \n## Call:\n## model.avg(object = allmodels)\n## \n## Component model call: \n## lm(formula = ABUND ~ &lt;64 unique rhs&gt;, data = loyn)\n## \n## Component models: \n##        df  logLik   AICc delta weight\n## 1235    6 -181.65 377.08  0.00   0.13\n## 135     5 -183.02 377.30  0.22   0.11\n## 235     5 -183.19 377.62  0.54   0.10\n## 123     5 -183.22 377.70  0.61   0.09\n## 35      4 -184.57 377.95  0.87   0.08\n## 1234    6 -182.62 379.03  1.95   0.05\n## 12345   7 -181.33 379.09  2.01   0.05\n## 2345    6 -182.92 379.63  2.55   0.04\n## 1356    6 -182.94 379.66  2.58   0.03\n## 12356   7 -181.64 379.71  2.63   0.03\n## 1345    6 -182.96 379.72  2.63   0.03\n## 13      4 -185.48 379.78  2.69   0.03\n## 356     5 -184.43 380.11  3.03   0.03\n## 2356    6 -183.19 380.16  3.08   0.03\n## 1236    6 -183.22 380.23  3.15   0.03\n## 345     5 -184.53 380.31  3.23   0.03\n## 12346   7 -182.61 381.65  4.56   0.01\n## 136     5 -185.25 381.75  4.67   0.01\n## 134     5 -185.30 381.85  4.76   0.01\n## 123456  8 -181.33 381.85  4.77   0.01\n## 13456   7 -182.81 382.06  4.98   0.01\n## 23456   7 -182.91 382.26  5.18   0.01\n## 3456    6 -184.33 382.44  5.36   0.01\n## 5       3 -188.34 383.15  6.07   0.01\n## 1346    6 -184.91 383.61  6.53   0.00\n## 25      4 -187.62 384.06  6.97   0.00\n## 15      4 -187.83 384.49  7.40   0.00\n## 256     5 -186.87 384.99  7.91   0.00\n## 56      4 -188.09 385.00  7.92   0.00\n## 45      4 -188.13 385.08  8.00   0.00\n## 245     5 -187.12 385.49  8.41   0.00\n## 125     5 -187.17 385.59  8.50   0.00\n## 156     5 -187.45 386.16  9.07   0.00\n## 1256    6 -186.20 386.19  9.11   0.00\n## 145     5 -187.58 386.41  9.33   0.00\n## 1245    6 -186.61 387.00  9.91   0.00\n## 2456    6 -186.61 387.01  9.93   0.00\n## 23      4 -189.18 387.18 10.10   0.00\n## 456     5 -188.00 387.24 10.16   0.00\n## 234     5 -188.25 387.75 10.67   0.00\n## 12456   7 -185.92 388.28 11.20   0.00\n## 1456    6 -187.35 388.48 11.40   0.00\n## 236     5 -189.14 389.52 12.44   0.00\n## 2346    6 -188.08 389.94 12.86   0.00\n## 3       3 -193.07 392.63 15.55   0.00\n## 36      4 -192.22 393.25 16.17   0.00\n## 346     5 -191.57 394.39 17.30   0.00\n## 34      4 -192.79 394.39 17.31   0.00\n## 126     5 -192.16 395.58 18.50   0.00\n## 124     5 -192.23 395.71 18.63   0.00\n## 1246    6 -191.12 396.02 18.94   0.00\n## 12      4 -194.07 396.96 19.88   0.00\n## 1       3 -195.85 398.18 21.10   0.00\n## 14      4 -194.79 398.41 21.32   0.00\n## 16      4 -195.13 399.08 21.99   0.00\n## 146     5 -194.50 400.24 23.16   0.00\n## 24      4 -198.21 405.24 28.15   0.00\n## 246     5 -197.58 406.41 29.33   0.00\n## 26      4 -199.35 407.51 30.43   0.00\n## 2       3 -200.75 407.98 30.90   0.00\n## 4       3 -202.85 412.18 35.10   0.00\n## (Null)  2 -204.25 412.74 35.66   0.00\n## 46      4 -202.84 414.50 37.42   0.00\n## 6       3 -204.05 414.58 37.50   0.00\n## \n## Term codes: \n##   AGE   ALT  AREA  DIST GRAZE LDIST \n##     1     2     3     4     5     6 \n## \n## Model-averaged coefficients:  \n## (full average) \n##               Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)   \n## (Intercept)  2.125e+01  7.540e+00   7.618e+00   2.790  0.00528 **\n## AGE         -7.527e-02  7.177e-02   7.236e-02   1.040  0.29823   \n## ALT          2.811e-02  3.196e-02   3.231e-02   0.870  0.38427   \n## AREA         1.235e-01  4.745e-02   4.818e-02   2.564  0.01035 * \n## GRAZE       -2.080e+00  1.621e+00   1.633e+00   1.273  0.20292   \n## DIST         9.132e-04  3.061e-03   3.117e-03   0.293  0.76950   \n## LDIST       -4.925e-05  6.691e-04   6.839e-04   0.072  0.94259   \n##  \n## (conditional average) \n##               Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)   \n## (Intercept) 21.2504039  7.5395494   7.6177190   2.790  0.00528 **\n## AGE         -0.1132699  0.0587077   0.0597924   1.894  0.05817 . \n## ALT          0.0481783  0.0280034   0.0286744   1.680  0.09292 . \n## AREA         0.1275241  0.0425967   0.0434334   2.936  0.00332 **\n## GRAZE       -2.7514534  1.2757684   1.2968574   2.122  0.03387 * \n## DIST         0.0034010  0.0051418   0.0052641   0.646  0.51823   \n## LDIST       -0.0002128  0.0013783   0.0014092   0.151  0.87997   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Nur Esimates\nsummary(avgmodel)$coefficients\n##        (Intercept)         AGE        ALT      AREA     GRAZE         DIST\n## full       21.2504 -0.07527122 0.02811021 0.1235275 -2.079840 0.0009132418\n## subset     21.2504 -0.11326991 0.04817829 0.1275241 -2.751453 0.0034009585\n##                LDIST\n## full   -4.925166e-05\n## subset -2.128051e-04\n# Confindence intervals\nconfint(avgmodel)\n##                    2.5 %       97.5 %\n## (Intercept)  6.319949053 36.180858723\n## AGE         -0.230460847  0.003921031\n## ALT         -0.008022574  0.104379146\n## AREA         0.042396229  0.212651977\n## GRAZE       -5.293247097 -0.209659662\n## DIST        -0.006916400  0.013718317\n## LDIST       -0.002974830  0.002549220",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistik 4: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik4_Uebung.html",
    "href": "statistik/Statistik4_Uebung.html",
    "title": "Statistik 4: Übung",
    "section": "",
    "text": "Lesen Sie den Datensatz steprasen_ukraine.csv in R ein. Dieser enthält Pflanzenartenzahlen (Species.richness) von 199 10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind.\nErmitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots durch die Umweltvariablen erklärt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n) und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse geeignet?\nExplorative Datenanalyse, um zu sehen, ob die abhängige Variable in der vorliegenden Form für die Analyse geeignet ist\nDefinition eines globalen Modelles und dessen Reduktion zu einem minimal adäquaten Modell\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nZu erstellen sind (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistik 4: Übung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/0_Vorbereitung.html",
    "href": "fallstudie_s/0_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen der Fallstudie werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog der Vorbereitungsübung in Prepro1 könnt ihr mit nachstehendem Code alle noch nicht installierten Bibliotheken automatisch installieren.\n\nipak &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages &lt;- c(\n  \"readr\", \"ggplot2\", \"dplyr\", \"lubridate\", \"ggpubr\", \"PerformanceAnalytics\",\n  \"MuMIn\", \"AICcmodavg\", \"fitdistrplus\", \"lme4\", \"DHARMa\", \"blmeco\", \"sjPlot\", \"lattice\",\n   \"suncalc\", \"glmmTMB\"\n)\n\nipak(packages)\n\nZudem könnt ihr alle für die Fallstudie Profil S benötigten Daten unter auf Moodle im Abschnitt Fallstudie “Ecosystems & Biodiversity” herunterladen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html",
    "href": "fallstudie_s/1_Einführung.html",
    "title": "Einführung",
    "section": "",
    "text": "Hintergrund\nDas rund 1100 ha grosse Naturschutzgebiet Wildnispark Zürich Sihlwald, welches im periurbanen Raum südlich von Zürich liegt, gilt seit dem 1. Januar 2010 als erster national anerkannter Naturerlebnispark. Er ist Teil des Wildnisparks Zürich und wichtiges Naherholungsgebiet für die Stadt Zürich.\nDas Schutzgebiet befindet sich im Spannungsfeld zwischen Schutz und Nutzen, denn einerseits sollen die Besuchenden den Wald erleben dürfen, andererseits soll sich dieser, in der Kernzone, frei entwickeln dürfen. Im Perimeter gelten darum verschiedene Regeln. So darf z. B. nur auf bestimmten Wegen mit den Velo gefahren werden.\nDas Management braucht solide, empirisch erhobene Daten zur Natur und zu den Besuchenden damit die Ziele von Nutzen und Schürzen erreicht werden können. Das Besuchermonitoring deckt den zweiten Teil dieser notwendigen Daten ab. Im Wildnispark Zürich sind dazu mehrere automatische Zählstellen in Betrieb. Die Zählstellen erfassen stundenweise die Besuchenden auf den Wegen. Einige Zählstellen erfassen richtungsgetrennt und / oder können zwischen verschiedenen Nutzergruppen wie Personen, die zu Fuss gehen, und Velofahrenden unterscheiden.\nIm Rahmen des Moduls Research Methods werden in dieser Fallstudie mehrere dieser automatischen Zählstellen genauer untersucht. Die Daten, welche im Besitz des WPZ sind, wurden bereits kalibriert. Das heisst, Zählungen während Wartungsarbeiten, bei Felhbetrieb o.ä. wurden bereits ausgeschlossen. Dies ist eine zeitintensive Arbeit und wir dürfen hier mit einem sauber aufbereiteten “Datenschatz” arbeiten.\nPerimeter des Wildnispark Zürichs mit den ungefähren Standorten von zwei ausgewählten automatischen Zählstellen.\nHinweis:\nDer Wildnispark wertet die Zahlen auf verschiedene Weise aus. So sind z. B. Jahresgänge (an welchen Monaten herrscht besonders viel Betrieb?) und die absoluten Nutzungszahlen bekannt. Vertiefte Auswertungen, die beispielsweise den Zusammenhang zwischen Besuchszahlen und dem Wetter untersuchen, werden nicht gemacht.\nUnsere Analysen in diesem Modul helfen dem Management, ein besseres Verständnis zum Verhalten der Besuchenden zu erlangen und bilden Grundlagen für Managemententscheide in der Praxis.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#hintergrund",
    "href": "fallstudie_s/1_Einführung.html#hintergrund",
    "title": "Einführung",
    "section": "",
    "text": "Die Zähler 211 und 502 erfassen sowohl Fussgänger:innen als auch Fahrräder. Die Erfassung erfolgt richtungsgetrennt.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#ziel",
    "href": "fallstudie_s/1_Einführung.html#ziel",
    "title": "Einführung",
    "section": "Ziel",
    "text": "Ziel\nIn dieser Fallstudie zeigen wir, welche Einflüsse die Covid19-Pandemie im Frühjahr 2020 auf die täglichen Besuchszahlen im Wildnispark Zürich hatte. Dabei setzen wir den Fokus auf die Dämmerung und die Nacht, den in diesen Zeiten sind Wildtiere (u.a. Rehe) besonders sensibel gegenüber Störungen. Wir untersuchen ebenfalls, wie sich die Besuchszhalen seit der Pandemie entwickelt haben und ob sie sich wieder dem Muster von vor der Pandemie annähern. Auch dabei ist die “dunkle” Tageszeit im Fokus.\nIn unsere Analysen ziehen wir auch weitere erklärende Faktoren wie Wetter, Wochentag, Kalenderwoche und Schulferien mit ein. Die statistischen Auswertungen erlauben und somit klare Rückschlüsse auf die Effekte der Faktoren und deren Stärke zu ziehen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#grundlagen",
    "href": "fallstudie_s/1_Einführung.html#grundlagen",
    "title": "Einführung",
    "section": "Grundlagen",
    "text": "Grundlagen\nZur Verfügung stehen:\n\ndie stündlichen Zählungen von Fussgänger:innen und Velos an den Zählstellen\nMeteodaten (Temperatur, Sonnenscheindauer, Niederschlagssumme)\nR-Skripte mit Hinweisen zur Auswertung",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#aufbau-der-fallstudie",
    "href": "fallstudie_s/1_Einführung.html#aufbau-der-fallstudie",
    "title": "Einführung",
    "section": "Aufbau der Fallstudie",
    "text": "Aufbau der Fallstudie\nIn dieser Fallstudie erheben wir zuerst selbst Daten auf dem Grüntal, welche wir dann deskriptiv auswerten. Anschliessend beschäftigen wir uns mit den Daten aus dem Wildnispark Zürich, welche wir ebenfalls deskriptiv auswerten und auch sttistische Modelle damit programmieren. Diese Ergebnisse werden dann im Abschlussbericht dokumentiert.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/2_Besuchermonitoring.html",
    "href": "fallstudie_s/2_Besuchermonitoring.html",
    "title": "Monitoring",
    "section": "",
    "text": "Einführung\nEs gibt eine Vielzahl an möglichen Methoden zur Erfassung der Besuchszahlen. Automatische Zählgeräte bieten die Möglichkeit, lange und durchgehende Zeitreihen zu erfassen. Inputs dazu, wie diese ausgewertet werden können, erhält ihr in dieser Aufgabe.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Monitoring</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/2_Besuchermonitoring.html#ziele",
    "href": "fallstudie_s/2_Besuchermonitoring.html#ziele",
    "title": "Monitoring",
    "section": "Ziele",
    "text": "Ziele\n\nIhr könnt das eingesetzte Zählgerät installieren und kennt die Vor- und Nachteile verschiedener Methoden.\nIhr könnt die generierten Daten explorativ und deskriptiv auswerten.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Monitoring</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/2_Besuchermonitoring.html#grundlagen",
    "href": "fallstudie_s/2_Besuchermonitoring.html#grundlagen",
    "title": "Monitoring",
    "section": "Grundlagen",
    "text": "Grundlagen\nDie Geräte werden gemeinsam auf dem Campus Grüntal platziert.\nDatenschutz ist ein wichtiges Thema. Die eingesetzten Geräte erfassen keine Personendaten, sondern nur Bewegungen. Es handelt sich um Pyroelektrische Infrarotsensoren, welche auf den Temperaturunterschied reagieren, wenn sich eine Person vor der Linse bewegt. Insofern handelt es sich, vereinfacht gesagt, um Bewegungsmelder.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Monitoring</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "",
    "text": "Ziele\nIhr habt selbst ein (kleines) Besuchermonitoring auf dem Grüental durchgeführt und euch bereits mit dem WPZ beschäftigt. Die Aufgaben im Zusammenhang mit dem Grüental sind abgeschlossen und wir beschäftigen uns ab jetzt ausschliesslich mit dem WPZ.\nIm Rahmen unserer Analyse programmieren wir multivariate Modelle, welche den Zusammenhang zwischen der Anzahl Besuchenden und verschiedenen Einflussfaktoren beschreiben. Dank den Modellen können wir sagen, wie die Besuchenden auf die untersuchten Faktoren reagiert haben (siehe dazu auch euren Forschungsplan sowie [Einführung], Ziele).\nFür meine Analysen habe ich untenstehende Fragestellungen formuliert. Ihr könnt aber auch eure Fragestellungen aus eurem Forschungsplan übernehmen, falls ihr das möchtet und falls diese mit den zur Verügung stehenden Daten bearbeitet werden können.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#ziele",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#ziele",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "",
    "text": "Welchen Einfluss haben zeitliche Variablen (Wochentag, Ferien, Monat, Jahr, Phasen der Covid-Pandemie) und Wetterparameter (Sonnenscheindauer, Höchsttemperatur, Niederschlagssumme) auf die Besuchszahlen im WPZ?\nWie stark sind die jeweiligen Einflüsse, welche Effektrichtungen sind beobachtbar und welche der untersuchten Parameter sind signifikant?\nGibt es in den Effektrichtungen deutliche Unterschiede zwischen den Tageszeiten und wie können diese erklärt werden?\nWelches sind die mutmasslichen Auswirkungen dieser Nutzung auf das Verhalten der Rege im WPZ?\n\n\n\nJede Gruppe wertet Daten von einem Zählgerät aus. Sprecht miteinander ab, wer welchen Zähler behandelt (211 oder 502; Spezifikationen siehe [Einführung], Hinweis). Die Daten von jeder Zählstelle sollen nur von einer Gruppe ausgewertet werden!\nBezieht in eure Auswertungen den gesamten zur Verfügung stehenden Zeitraum ein.\nFür euren Zähler stehen Zahlen zu Fussgänger:innen und Fahrrädern zur Verfügung (siehe [Einführung], Hinweis). Entscheidet euch selbst, ob ihr Fussgänger:innen ODER Fahrräder auswerten wollt. Die anderen Daten dürft ihr vernachlässigen.\nIm Bericht sollen die Informationen und Erfahrungen aus dem gesamten Verlauf der Fallstudie in geeigneter Weise einfliessen. Bezüglich der Felderhebung Grüental erwarten wir keine Angaben.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#erwartungen",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#erwartungen",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "Erwartungen",
    "text": "Erwartungen\n\nStruktur / Aufbau\n\n\nAbstract / Zusammenfassung (wenn Bericht auf Deutsch, dann Zusammenfassung auch auf Deutsch). Im Abstract sollen alle Bestandteile des Berichts aufgenommen sein.\nEinleitung (hier können allenfalls Elemente / Teile aus den Forschungsplänen übernommen werden)\nFragestellung (siehe oben; die Fragestellung ist vorgegeben, darf aber für den Bericht geschärft / ausformuliert und konkretisiert werden.)\nMethoden (aufschlussreiches Kapitel mit den statistischen Analysen)\nResultate (deskriptive Statistik, multivariates Modell; kurzer Fliesstext sowie die notwendigen Tabellen und eine Auswahl möglichst informativer Grafiken)\nDiskussion (Diskussion der deskriptiven Analysen und der Modellergebnisse; dieser Abschnitt sollte die eigenen Resultate auch im Zusammenhang mit aktueller Fachliteratur reflektieren.)\nLiteraturverzeichnis (Tipp: Das Literaturverzeichnis sollte vollständig sein, sowie formal korrekt und einheitlich daherkommen. Wir erwarten speziell in der Diskussion eine Abstützung auf aktuelle Fachliteratur. Auf Moodle haben wir euch eine Auswahl relevanter Studien bereitgestellt.)\nAnhang (für alle Auswertungen relevanter R-Code in geeigneter Form)\n\n\nGesamtumfang max. 12’000 Zeichen (inkl. Leerzeichen; exkl. Tabellen, Literaturverzeichnis und Anhang)\nAbgabe am 12.1.2025 per Mail an hoce@zhaw.ch",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#bewertungskriterien",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#bewertungskriterien",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "Bewertungskriterien",
    "text": "Bewertungskriterien\n\nIst die Methode klar und verständlich formuliert?\nSind die deskriptiven Analysen klar beschrieben und geeignet visualisiert?\nIst die Variablenselektion klar beschrieben, plausibel und nachvollziehbar?\nSind die Modellresultate in Text- und Tabellenform korrekt beschrieben und geeignet visualisiert?\nIst die Diskussion klar formuliert und inhaltlich schlüssig?\nWie gut ist die Diskussion auf relevante und aktuelle Fachliteratur abgestützt?\nZusätzliche bewerten wir die inhaltliche Dichte der Arbeit und die formale Qualität (Sprache, Struktur, Aufbau, Darstellung, Literaturverzeichnis, Umgang mit Literatur im Text)",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#zusammensetzung-note-fallstudie",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#zusammensetzung-note-fallstudie",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "Zusammensetzung Note Fallstudie:",
    "text": "Zusammensetzung Note Fallstudie:\n\nFallstudie-Leistungsnachweis 1 - Forschungsplan: Testatplichtig\nFallstudie-Leistungsnachweis 2 - Multivariate Analyse: 100 %\n\nDie Note der Fallstudie fliesst in die Gesamtbewertung des Moduls Research Methods mit ein.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html",
    "href": "fallstudie_s/4_Projektierung.html",
    "title": "Projek erstellen",
    "section": "",
    "text": "Vorbereitung\nVor den eigentlichen Auswertungen müssen einige Vorbereitungen unternommen werden. Die Zeit, die man hier investiert, wird in den späteren Phasen um ein Mehrfaches eingespart.\nWie im Unterricht am Morgen empfehle auch ich mit Projekten zu arbeiten, da diese sehr einfach ausgetauscht (auf verschiedene Rechner) und somit auch reproduziert werden können. Wichtig ist, dass es keine absoluten Arbeitspfade sondern nur relative gibt. Der Datenimport (und -export) kann mithilfe dieser relativen Pfade stark vereinfacht werden. –&gt; Kurz gesagt: Projekte helfen alles am richtigen Ort zu behalten (mehr zur Arbeit mit Projekten: Link).\n–&gt; File / New Project",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#vorbereitung",
    "href": "fallstudie_s/4_Projektierung.html#vorbereitung",
    "title": "Projek erstellen",
    "section": "",
    "text": "Erstellt an einem passenden Speicherort (evtl. onedrive für das gemeinsame Arbeiten an einem Projekt) ein neues Projekt mit einem treffenden Namen:",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "title": "Projek erstellen",
    "section": "Aufgabe 1: Projektaufbau",
    "text": "Aufgabe 1: Projektaufbau\nHinweise:\nNutzt für allen Text, welcher nicht im Code integriert ist, das Symbol #. Wenn ihr den Text als Titel definieren wollt, so dass er in der Übersicht erscheint, können vor dem Wort “#” und nach dem Wort “####” eingefügt werden.\n\n# Texte, vor denen ein # und nach denen #### stehen, sind Überschriften\n\n# Ich bin eine first level Überschrift ####\n\n## Ich bin eine second level Überschrift ####\n\n# Texte, vor denen ein # steht, erklaeren den Ablauf (nicht was, sondern warum)\n\n# Dann folgen die Arbeitsschritte\n1 + 1\n\n# Wenn man auf \"Outline\" klickt (oder CTRL + SHIFT + O),\n# öffnet sich die Übersicht zu den Überschriften\n\nTipps:\n\nAlt + - = &lt;-\nCtrl + Shift + C = # vor der ausgewählten Zeile / den ausgewählten Zeilen hinzufügen oder wieder löschen\n\nAufbau eines Skripts\nZuerst immer den Titel des Projekts sowie den Autor/ die Autorin des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer Dateneigentümer ist (WPZ und Meteo Schweiz).\nIm Skript soll immer die Ordnerstruktur des Projekts genannt werden. So kann der Arbeitsvorgang auf verschiedenen Rechnern einfach reproduziert werden (ich verwende hier ein Projektordner mit den Unterordnern __scripts, data, results).\nBeschreibt zudem kurz die verwendeten Meteodaten (siehe dazu Metadata Meteodaten, –&gt; order_XXX_legend.txt)\nEin Skript kann in R eigentlich immer (mehr oder weniger) nach dem selbem Schema aufgebaut sein. Dieses Schema enthält bei uns folgende Kapitel:\n\nMetadaten und Definitionen\nDatenimport\nVorbereitung\nDeskriptive Analyse und Visualisierung\nMultifaktorielle Analyse und Visualisierung\n\nBereitet euer Skript mit diesen Kapitel vor.\n\n# .###########################################################################################\n# TITEL ####\n# Fallstudie Modul Research Methods, HS24. Autor/in ####\n# .##########################################################################################\n\n# .##########################################################################################\n# METADATA UND DEFINITIONEN ####\n# .##########################################################################################\n\n# Datenherkunft ####\n# ...\n\n# .##########################################################################################\n# 1. DATENIMPORT #####\n# .##########################################################################################\n\nIn einem professionellen Bericht ist es angebracht, wenn alle Abbildung einheitlich sind. Dafür braucht es u.a. eine Farbpalette. Ich definiere meine Auswahl bereits hier; das hat den Vorteil, dass man die Farbnamen nur einmal schreiben muss und später die selbst definierte Palette unter der Variable “mycolors” abrufen kann.\n\nmycolors &lt;- c(\"orangered\", \"gold\", \"mediumvioletred\", \"darkblue\")",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "title": "Projek erstellen",
    "section": "Aufgabe 2: Laden der Bibliotheken",
    "text": "Aufgabe 2: Laden der Bibliotheken\n\nLädt nun die nachfolgenden, benötigten Bibliotheken.\nDiese habt ihr, falls nötig, im Kapitel Vorbereitung installiert.\n\n\n# Benoetigte Bibliotheken ####\nlibrary(\"readr\") # read data into r\nlibrary(\"ggplot2\") # plot nice graphs\nlibrary(\"dplyr\") # select data\nlibrary(\"lubridate\") # Arbeiten mit Datumsformaten\nlibrary(\"suncalc\") # berechne Tageszeiten abhaengig vom Sonnenstand\nlibrary(\"ggpubr\") # to arrange multiple plots in one graph\nlibrary(\"PerformanceAnalytics\") # Plotte Korrelationsmatrix\nlibrary(\"MuMIn\") # Multi-Model Inference\nlibrary(\"AICcmodavg\") # Modellaverageing\nlibrary(\"fitdistrplus\") # Prueft die Verteilung in Daten\nlibrary(\"lme4\") # Multivariate Modelle\nlibrary(\"DHARMa\") # Modeldiagnostik\nlibrary(\"blmeco\") # Bayesian data analysis using linear models\nlibrary(\"sjPlot\") # Plotten von Modellergebnissen (tab_model)\nlibrary(\"lattice\") # einfaches plotten von Zusammenhängen zwischen Variablen\nlibrary(\"glmmTMB\")# zero-inflated model",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "title": "Projek erstellen",
    "section": "Aufgabe 3: Zeitliche Definitionen",
    "text": "Aufgabe 3: Zeitliche Definitionen\nWir lesen später zwei verschiedene Datensätze ein. Beide sollen exakt denselben Zeitraum umfassen. Definiert dazu den ersten und letzten Tag gemäss den bei euch vorhandenen Zähldaten (das unterscheidet sich von Gruppe zu Gruppe).\n\ndepo_start &lt;- as.Date(\"2017-01-01\")\ndepo_end &lt;- as.Date(\"2023-12-31\")\n\nEin Teil unserer Auswertungen ist der Einfluss der Corona-Lockdown auf das Besuchsverhalten.\n-Wir müssen also Start und Ende der beiden Lockdowns in der Schweiz definieren:\n\nlock_1_start_2020 &lt;- as.Date(\"2020-03-16\")\nlock_1_end_2020 &lt;- as.Date(\"2020-05-11\")\n\nlock_2_start_2021 &lt;- as.Date(\"2020-12-22\")\nlock_2_end_2021 &lt;- as.Date(\"2021-03-01\")\n\nEbenfalls müssen die erste und letzte Kalenderwoche der Untersuchungsfrist definiert werden. Diese werden bei wochenweisen Analysen ausgeklammert da sie i.d.R. unvollständig sind (das ist ein späterer Arbeitsschritt). Geht wie oben vor. Tipp: der Befehl isoweek() liefert euch die Kalenderwoche.\nFerienzeiten können einen grossen Einfluss auf das Besucheraufkommen haben. Die relevanten Ferienzeiträume müssen daher bekannt sein (heruntergeladen von https://www.schulferien.org/schweiz/ferien/2020/).\nLest das bereitgestellte .csv mit den Ferienzeiträumen ein und speichert es unter schulferien.\n\nschulferien &lt;- read_delim(\"datasets/fallstudie_s/ferien.csv\", \",\")\n\nNun sind alle Vorbereitungen gemacht, die Projektstruktur aufgebaut und die eigentliche Arbeit kann im nächsten Schritt beginnen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html",
    "href": "fallstudie_s/5_Datenverarbeitung.html",
    "title": "Datenverarbeitung",
    "section": "",
    "text": "Aufgabe 1: Zähldaten",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-1-zähldaten",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-1-zähldaten",
    "title": "Datenverarbeitung",
    "section": "",
    "text": "1a)\nDie Projektstruktur steht. Nun können die Daten eingelesen und die nötigen Datentypen definiert werden.\nLädt die Daten zuerst von Moodle herunter:\n\nZähldaten zu eurem Standort (211_sihlwaldstrasse_2017_2024.csv, 502_sihluferweg_2016_2024.csv)\nMeteodaten und Legende\n\nHinweis: Siehe [Einführung] für den Standort der Zähler 211 und 502.\nDie Zähldaten des WPZ wurden vorgängig bereinigt (z.B. wurden Stundenwerte entfernt, an denen am Zähler Wartungsarbeiten stattgefunden haben). Das macht es für uns einfach, denn wir können die Daten ohne vorgängige Bereinigung einlesen. Behaltet aber im Hinterkopf, dass die Datenaufbereitung, die Datenbereinigung mit viel Aufwand verbunden ist.\n\nLest die Zählaten ein, speichert ihn unter der Variable depo und sichtet den Datensatz (z.B. str(), head(), view() usw.).\n\n\n\nMusterlösung\n# Speicherort sowie Dateiname anpassen\ndepo &lt;- read_delim(\"./HIER RELATIVEN DATEIPFAD EINGEBEN\", \"HIER SEPERATOR EINGEBEN\")\n\n\nHinweis: Im Stundenformat zeigen die Werte bei 11:00 die Zähldaten zwischen 11:00 bis 12:00 Uhr.\n\n\nMusterlösung\n# lese die Daten ein\ndepo &lt;- read_delim(\"datasets/fallstudie_s/211_sihlwaldstrasse.csv\", \";\")\n\n# erstes Sichten und anpassen der Datentypen\nstr(depo)\n\n\n\n\n1b)\n\nNun muss das Datum als solches definiert werden. Ich nutze dazu as.POSIXct(). Welches Format hat das Datum im csv? Das muss im Code angepasst werden.\n\n\n\nMusterlösung\ndepo &lt;- depo |&gt;\n  mutate(\n    Datetime = as.POSIXct(DatumUhrzeit, format = \"HIER STEHT DAS DATUMSFORMAT\", tz = \"CET\"),\n    # nun schreiben wir uns das Datum in eine seperate Spalte\n    Datum = as.Date(Datetime)\n  )\n\n\n\n\nMusterlösung\n# hier der code mit dem richtigen Format\ndepo &lt;- depo |&gt;\n  mutate(\n    Datetime = as.POSIXct(as.character(Datetime), format = \"%Y%m%d%H\", tz = \"CET\"),\n    Datum = as.Date(Datetime)\n  )\n\n\n\n\n1c)\nIhr könnt selbst wählen, ob ihr Fussgänger:innen oder Fahrräder untersuchen wollt (je nachdem ob sie in eurem Datensatz vorhanden sind).\n\nEntfernt die überflüssigen Spalten aus dem Datensatz. Ich schlage vor, dass ihr dafür den Befehl dplyr::select() verwendet.\nDamit kann man entweder Spalten behalten oder eben auch Spalten entfernen (-c(SPALTENNAMEN)).\n\nHinweis: mit select() können Spalten gewählt werden, mit filter() Zeilen.\n\n\nMusterlösung\n# In dieser Auswertung werden nur Personen zu Fuss betrachtet!\n# it select werden spalten ausgewaehlt oder eben fallengelassen\ndepo &lt;- depo |&gt;\n  dplyr::select(-c(Velo_IN, Velo_OUT))\n\n\n\n\n1d)\n\nBerechnen des Totals (IN + OUT), da dieses in den Daten nicht vorhanden ist.\n\nTipp: Wenn man R sagt: “addiere mir Spalte x mit Spalte y”, dann macht R das für alle Zeilen in diesen zwei Spalten. Wenn man nun noch sagt: “speichere mir das Ergebnis dieser Addition in einer neuen Spalte namens Total”, dann hat man die Aufgabe bereits gelöst. Arbeitet mit mutate()).\n\nEntfernt nun alle NA-Werte mit na.omit().\n\n\n\nMusterlösung\n# Berechnen des Totals, da dieses in den Daten nicht vorhanden ist\ndepo &lt;- depo |&gt;\n  mutate(Total = Fuss_IN + Fuss_OUT)\n\n# Entferne die NA's in dem df.\ndepo &lt;- na.omit(depo)",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-2-meteodaten",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-2-meteodaten",
    "title": "Datenverarbeitung",
    "section": "Aufgabe 2: Meteodaten",
    "text": "Aufgabe 2: Meteodaten\n\n2a)\n\nLest die Meteodaten ein und speichert sie unter meteo.\n\n\n\nMusterlösung\n# Einlesen\nmeteo &lt;- read_delim(\"datasets/fallstudie_s/order_124839_data.txt\", \";\")\n\n\n\n\n2b)\n\nAuch hier müssen die Datentypen manuell gesetzt werden.\n\nTipp: Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewandelt werden aus dem dann das eigentliche Datum herausgelesen werden kann. Das ist mühsam - darum hier der Code.\n\n\nMusterlösung\nmeteo &lt;- mutate(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\n\nHinweise:\nDie Zeitangaben sind in UTC: 00:40 UTC = 02:40 Sommerzeit = 01:40 Winterzeit, Beispiel: 13 = beinhaltet Messperiode von 12:01 bis 13:00\nDa wir mit Tageshöchstwerten oder -summen rechnen, können wir zum Glück ignorieren, dass das nicht mit den Daten der Zählstellen übereinstimmt. Learning: es ist zentral immer die Metadaten zu checken.\nWas ist eigentlich Niederschlag:\nLink Meteo Schweiz\n\nWerden den anderen Spalten die richtigen Typen zugewiesen? Falls nicht, ändert die Datentypen.\nNun schneiden wir den Datensatz auf die Untersuchungsdauer zu.\n\n\n\nMusterlösung\n    ... |&gt;\n    filter(time &gt;= depo_start, time &lt;= depo_end)\n\n\n\nDann müssen auch hier alle nicht verfügbare Werte (NA’s) herausgefiltert werden. Macht das wieder mit na.omit()\nPrüft nun, wie die Struktur des data.frame (df) aussieht und ob alle NA Werte entfernt wurden:\n\n\n\nMusterlösung\nsum(is.na(df$Variable))\n\n\n\nStimmen alle Datentypen? str()\n\n\n\nMusterlösung\n# Die eigentlichen Messwerte sind alle nummerisch\nmeteo &lt;- meteo |&gt;\n    mutate(\n        tre200nx = as.numeric(tre200nx),\n        tre200jx = as.numeric(tre200jx),\n        rre150n0 = as.numeric(rre150n0),\n        rre150j0 = as.numeric(rre150j0),\n        sremaxdv = as.numeric(sremaxdv)\n    ) |&gt;\n    filter(time &gt;= depo_start, time &lt;= depo_end) # schneide dann auf Untersuchungsdauer\n\n# Was ist eigentlich Niederschlag:\n# https://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\n# Filtere Werte mit NA\nmeteo &lt;- na.omit(meteo)\n# Pruefe ob alles funktioniert hat\nstr(meteo)\nsum(is.na(meteo)) # zeigt die Anzahl NA's im data.frame an",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-3-datenvorverarbeitung-mutationen",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-3-datenvorverarbeitung-mutationen",
    "title": "Datenverarbeitung",
    "section": "Aufgabe 3: Datenvorverarbeitung (Mutationen)",
    "text": "Aufgabe 3: Datenvorverarbeitung (Mutationen)\n\n3a)\nJetzt fügen wir viele Convenience Variablen hinzu. Wir brauchen:\n\nWochentag; der Befehl dazu ist wday(). Danach als Faktor speichern.\nWerktag oder Wochenende, ebebfalls als Faktor.\n\nDer Code dazu könnte so aussehen:\n\n\nMusterlösung\n  ...|&gt;\n  mutate(Wochenende = ifelse(Wochentag %in% c(6,7), \"Wochenende\", \"Werktag\")) |&gt;\n  # 1 means Monday and 7 means Sunday (default)\n  mutate(Wochenende = as.factor(Wochenende)) |&gt;\n  ...\n\n\nje als Faktor:\n\nKalenderwoche: isoweek()\nMonat: month()\nJahr: year()\n\n\n\nMusterlösung\ndepo &lt;- depo |&gt;\n  # wday sortiert die Wochentage automatisch in der richtigen Reihenfolge\n  mutate(\n    Wochentag = wday(Datetime, week_start = 1),\n    Wochentag = factor(Wochentag),\n    # Werktag oder Wochenende hinzufuegen\n    Wochenende = ifelse(Wochentag %in% c(6, 7), \"Wochenende\", \"Werktag\"),\n    Wochenende = as.factor(Wochenende),\n    # Kalenderwoche hinzufuegen\n    KW = isoweek(Datetime),\n    KW = factor(KW),\n    # monat und Jahr\n    Monat = month(Datetime),\n    Monat = factor(Monat),\n    Jahr = year(Datetime),\n    Jahr = factor(Jahr))\n\n\nDies machen wir auch mit dem “meteo” Datensatz. Wiederum bitte Wochentag, Werktag oder Wochenende, Kalenderwoche, Monat und Jahr. Ebenfalls alles als Faktor speichern.\n\n\nMusterlösung\n# Wir gruppieren die Meteodaten noch nach Kalenderwoche und Werktag / Wochenende\n# Dafür brauchen wir zuerst diese als Convenience Variablen\nmeteo &lt;- meteo |&gt;\n  # wday sortiert die Wochentage automatisch in der richtigen Reihenfolge\n  mutate(\n    Wochentag = wday(time, week_start = 1),\n    Wochentag = factor(Wochentag),\n    # Werktag oder Wochenende hinzufuegen\n    Wochenende = ifelse(Wochentag %in% c(6, 7), \"Wochenende\", \"Werktag\"),\n    Wochenende = as.factor(Wochenende),\n    # Kalenderwoche hinzufuegen\n    KW = isoweek(time),\n    KW = factor(KW),\n    # monat und Jahr\n    Monat = month(time),\n    Monat = factor(Monat),\n    Jahr = year(time),\n    Jahr = factor(Jahr))\n\n\n\nSpäter werden wir nicht nur Analysen pro Tag machen, sondern auch zusammengefasst nach Woche. Dafür müssen wir nun den meteo-Datensaz gruppieren und den mean berechnen. Hier der Code dazu, wie das aussehen könnte:\n\n\n\nMusterlösung\nmeteo_day &lt;- meteo |&gt;\n  group_by(Jahr, Monat, KW, Wochenende) |&gt;\n  summarise(\n    tre200nx = mean(tre200nx),\n    tre200jx = mean(tre200jx),\n    rre150n0 = mean(rre150n0),\n    rre150j0 = mean(rre150j0),\n    sremaxdv= mean(sremaxdv))\n\n\nWieder zurück zum depo-Datensazt.\nIch mache den folgenden Punkt nachgelagert zu den voerherigen Convenience Variablen, da zu viele Operationen in einem Schritt auch schon mal etwas durcheinander erzeugen können.\nPhasen der Covid-Pandemie (Code untenstehend). Wir definieren 5 Phasen:\n\nvon Anfang Untersuchungsperiode bis vor Lockdown 1\nLockdown 1\nzwischen den Lockdowns\nLockdown 2\nEnde 2. Lockdown bis Ende Untersuchungsperiode\nWir packen alle Phasen in eine Spalte –&gt; long-format ist praktischer für das plotten als wide-format.\nSpäter im multivariaten Modell werden die Levels der Variablen per “default” alphabetisch geordnet und die Effektstärken der einzelnen Levels gegenüber dem ersten Level gerechnet. Das macht wenig Sinn, den die Levels sind nicht alphabetisch, sondern gemäss der Liste oben (später mehr dazu). Das passen wir ebenfalls an.\nHier der Code dazu:\n\n\n\nMusterlösung\ndepo &lt;- depo |&gt;\n    mutate(Phase = case_when(\n        Datetime &lt; lock_1_start ~ \"Pre\",\n        Datetime &gt;= lock_1_start & Datetime &lt;= lock_1_end ~ \"Lockdown_1\",\n        Datetime &gt; lock_1_end & Datetime &lt; lock_2_start ~ \"Inter\",\n        Datetime &gt;= lock_2_start & Datetime &lt;= lock_2_end ~ \"Lockdown_2\",\n        Datetime &gt; lock_2_end ~ \"Post\"\n    ))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\ndepo &lt;- depo |&gt;\n    # mit factor() koennen die levels direkt einfach selbst definiert werden.\n    # wichtig: speizfizieren, dass aus R base, ansonsten kommt es zu einem\n    # mix-up mit anderen packages\n    mutate(Phase = base::factor(Phase, levels = c(\"Pre\", \"Lockdown_1\", \"Inter\", \"Lockdown_2\", \"Post\")))\n\nstr(depo)\n\n\nNeben dem Lockdown können auch die Schulferien einen Einfluss auf die Besuchszahlen haben. Wir haben die Schulferien bereits als .csv eingelesen. Allerdings können wir die Schulferien nicht mit der case_when()-Funktion zuweisen, da diese mit dieser Funktion alle Vektoren im Datensatz “schulferien” verglichen werden, und nicht elementweise für jede Zeile im “depo”-Datensatz. Dies führt dazu, dass die Bedingungen nur einmal überprüft werden und dann auf den gesamten Vektor angewendet werden, anstatt Zeile für Zeile.\n\nWeil dies etwas kompliziert ist, hier eine Funktion zur Zuweisung der Ferien, welche ihr kopieren könnt:\n\n\n\nMusterlösung\n# schreibe nun eine Funktion zur zuweisung Ferien. WENN groesser als start UND kleiner als\n# ende, DANN schreibe ein 1\nfor (i in 1:nrow(schulferien)) {\n  depo$Ferien[depo$Datum &gt;= schulferien[i, \"Start\"] & depo$Datum &lt;= schulferien[i, \"Ende\"]] &lt;- 1\n}\ndepo$Ferien[is.na(depo$Ferien)] &lt;- 0\n\n# als faktor speichern\ndepo$Ferien &lt;- factor(depo$Ferien)\n\n\n\n\n3b)\n\nNun soll noch die volle Stunde als Integer im Datensatz stehen. Macht das mit dem Befehl hour()\n\n\n\nMusterlösung\n# Fuer einige Auswertungen muss auf die Stunden als nummerischer Wert zurueckgegriffen werden\ndepo$Stunde &lt;- hour(depo$Datetime)\n# hour gibt uns den integer\ntypeof(depo$Stunde)\n\n\n\n\n3c)\nDie Daten wurden durch den WPZ kalibriert (Nachkommastellen). Unser späteres Modell kann nicht mit Nachkommastellen in der abhängigen Variable umgehen (später dazu mehr).\n\nRundet die Zähldaten in der Spalte “Total” auf 0 Nachkommastellen. Der Befehl lautet round()\nDefiniert sie sicherheitshalber als Integer (= Ganzzahl)\nMacht das nun noch für IN und OUT.\n\n\n\nMusterlösung\ndepo$Total &lt;- as.integer(round(depo$Total, digits = 0))\n\ndepo$Fuss_IN &lt;- as.integer(round(depo$Fuss_IN, digits = 0))\n\ndepo$Fuss_OUT &lt;- as.integer(round(depo$Fuss_OUT, digits = 0))\n\n\n\n\n3d) Tageszeit\nWir setzen den Fokus unserer Untersuchung auf die Veränderung der Besuchszahlen in der Abend- und Morgendämmerung sowie der Nacht. Dafür müssen wir diese tageszeitliche Einteilung der Daten erst machen. Da dies über den Umfang dieser Fallstudie hinaus geht, liefere ich euch hier den Code dazu.\nDie wichtigsten Punkte:\n\nDie Tageslänge wurde für den Standort Zürich (Zeitzone CET) mit dem Package “suncalc” berechnet. Dabei wurden Sommer- und Winterzeit berücksichtigt.\nDie Einteilung der Tageszeit beruht auf dem Start und dem Ende der astronomischen Dämmerung sowie der Golden Hour. Der Morgen und der Abend wurden nach dieser Definition berechnet und um je eine Stunde Richtung Tag verlängert.\nUntenstehenden Code könnt ihr einfach kopieren.\nBeschreibt in eurem Bericht später, dass ihr die Einteilung der Tageszeit gemäss den Dämmerungszeiten in Zürich und gemäss meinem Code gemacht habt.\n\nHinweis: damit case_when() funktioniert, müsst ihr dplyr Version als 1.1.1 oder neuer haben. Das könnt ihr unter “Packages” (neben dem Reiter “Plots”, unten rechts) prüfen.\n\n\nMusterlösung\n# Einteilung Standort Zuerich\nLatitude &lt;- 47.38598\nLongitude &lt;- 8.50806\n\n# Start und das Ende der Sommerzeit:\n# https://www.schulferien.org/schweiz/zeit/zeitumstellung/\n\n\n# Welche Zeitzone haben wir eigentlich?\n# Switzerland uses Central European Time (CET) during the winter as standard time,\n# which is one hour ahead of Coordinated Universal Time (UTC+01:00), and\n# Central European Summer Time (CEST) during the summer as daylight saving time,\n# which is two hours ahead of Coordinated Universal Time (UTC+02:00).\n# https://en.wikipedia.org/wiki/Time_in_Switzerland\n\n# Was sind Astronomische Dämmerung und Golden Hour ueberhaupt?\n# https://sunrisesunset.de/sonne/schweiz/zurich-kreis-1-city/\n# https://www.rdocumentation.org/packages/suncalc/versions/0.5.0/topics/getSunlightTimes\n\n# Wir arbeiten mit folgenden Variablen:\n# \"nightEnd\" : night ends (morning astronomical twilight starts)\n# \"goldenHourEnd\" : morning golden hour (soft light, best time for photography) ends\n# \"goldenHour\" : evening golden hour starts\n# \"night\" : night starts (dark enough for astronomical observations)\n\nlumidata &lt;-\n    getSunlightTimes(\n        date = seq.Date(depo_start, depo_end, by = 1),\n        keep = c(\"nightEnd\", \"goldenHourEnd\", \"goldenHour\", \"night\"),\n        lat = Latitude,\n        lon = Longitude,\n        tz = \"CET\"\n    ) |&gt;\n    as_tibble()\n\n# jetzt haben wir alle noetigen Angaben zu Sonnenaufgang, Tageslaenge usw.\n# diese Angaben koennen wir nun mit unseren Zaehldaten verbinden:\ndepo &lt;- depo |&gt;\n    left_join(lumidata, by = c(Datum = \"date\"))\n\ndepo &lt;- depo |&gt;\n    mutate(Tageszeit = case_when(\n        Datetime &gt;= nightEnd & Datetime &lt;= goldenHourEnd ~ \"Morgen\",\n        Datetime &gt; goldenHourEnd & Datetime &lt; goldenHour ~ \"Tag\",\n        Datetime &gt;= goldenHour & Datetime &lt;= night ~ \"Abend\",\n        .default = \"Nacht\"\n    )) |&gt;\n    mutate(Tageszeit = factor(Tageszeit, levels = c(\"Morgen\", \"Tag\", \"Abend\", \"Nacht\"), ordered = TRUE))\n\n# behalte die relevanten Var\ndepo &lt;- depo |&gt; dplyr::select(-nightEnd, -goldenHourEnd, -goldenHour, -night, -lat, -lon)\n\n# Plotte zum pruefn ob das funktioniert hat\nggplot(depo, aes(y = Datetime, color = Tageszeit, x = Stunde)) +\n    geom_jitter() +\n    scale_color_manual(values = mycolors)\n\nsum(is.na(depo))\n\n# bei mir hat der Zusatz der Tageszeit noch zu einigen NA-Wertren gefueht.\n# Diese loesche ich einfach:\ndepo &lt;- na.omit(depo)\n# hat das funktioniert?\nsum(is.na(depo))",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-4-aggregierung-der-stundendaten",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-4-aggregierung-der-stundendaten",
    "title": "Datenverarbeitung",
    "section": "Aufgabe 4: Aggregierung der Stundendaten",
    "text": "Aufgabe 4: Aggregierung der Stundendaten\n\n4a)\nUnsere Daten liegen, wie ihr wisst, im Stundenformat vor. Für einige Auswertungen müssen wir aber auf ganze Tage zurückgreifen.\n\nDie Stundendaten werden zu ganzen Tagen aggregiert. Bezieht nur die Gruppierung (group_by()) Datum mit ein und speichert das Resultat unter depo_d (“_d” für “day”).\n\nHinweis: Wir gruppieren nur nach Datum, da ich mit den vielen weiteren Gruppierungen hier Probleme hatte, eine korrekte Summe zu erhalten.\n\n\nMusterlösung\ndepo_d &lt;- depo |&gt; \n  group_by(Datum) |&gt;   # Gruppieren nach der Variable Datum\n  summarise(Total = sum(Fuss_IN + Fuss_OUT),# Berechnen der gewünschten Werte\n            Fuss_IN = sum(Fuss_IN),\n            ...\n\n\n\n\nMusterlösung\n# hier werden also pro Nutzergruppe und Richtung die Stundenwerte pro Tag aufsummiert\ndepo_d &lt;- depo |&gt;\n  group_by(Datum) |&gt;\n  summarise(\n    Total = sum(Fuss_IN + Fuss_OUT),\n    Fuss_IN = sum(Fuss_IN),\n    Fuss_OUT = sum(Fuss_OUT)\n  )\n\n\n\nBerechne die Anzahl Tage bis Neujahr, wir brauchen sie später in den Modellen\n\n\n\nMusterlösung\ndepo_d &lt;- depo_d |&gt; \n  mutate(Tage_bis_Neujahr = as.numeric(difftime(ymd(paste0(year(Datum), \"-12-31\")), Datum, units = \"days\")))\n\n\n\nund füge nochmals alle Convenience Variablen gemäss oben ein:\n\n\n\nMusterlösung\ndepo_d &lt;- depo_d |&gt;\n  mutate(\n  Wochentag = wday(Datum, week_start = 1),\n  Wochentag = factor(Wochentag),\n  # Werktag oder Wochenende hinzufuegen\n  Wochenende = ifelse(Wochentag %in% c(6, 7), \"Wochenende\", \"Werktag\"),\n  Wochenende = as.factor(Wochenende),\n  # Kalenderwoche hinzufuegen\n  KW = isoweek(Datum),\n  KW = factor(KW),\n  # monat und Jahr\n  Monat = month(Datum),\n  Monat = factor(Monat),\n  Jahr = year(Datum),\n  Jahr = factor(Jahr))\n\ndepo_d &lt;- depo_d |&gt;\n  mutate(Phase = case_when(\n    Datum &lt; lock_1_start ~ \"Pre\",\n    Datum &gt;= lock_1_start & Datum &lt;= lock_1_end ~ \"Lockdown_1\",\n    Datum &gt; lock_1_end & Datum &lt; lock_2_start ~ \"Inter\",\n    Datum &gt;= lock_2_start & Datum &lt;= lock_2_end ~ \"Lockdown_2\",\n    Datum &gt; lock_2_end ~ \"Post\"\n  ))\n\ndepo_d &lt;- depo_d |&gt;\n  mutate(Phase = base::factor(Phase, levels = c(\"Pre\", \"Lockdown_1\", \"Inter\", \"Lockdown_2\", \"Post\")))\n\nfor (i in 1:nrow(schulferien)) {\n  depo_d$Ferien[depo_d$Datum &gt;= schulferien[i, \"Start\"] & depo_d$Datum &lt;= schulferien[i, \"Ende\"]] &lt;- 1\n}\ndepo_d$Ferien[is.na(depo_d$Ferien)] &lt;- 0\n\ndepo_d$Ferien &lt;- factor(depo_d$Ferien)\n\n# pruefe das df\nhead(depo_d)\n\n\n\nErstellt nun. ähnlich wie oben, einen Datensatz depo_daytime, in welchem ihr gruppiert nach:\n\n\nJahr\nMonat\nKalenderwoche\nPhase\nFerien\nWochenende oder Werktag\nTageszeit\n\n\n\nMusterlösung\ndepo_daytime &lt;- depo |&gt;\n  group_by(Jahr, Monat, KW, Phase, Ferien, Wochenende, Tageszeit) |&gt;\n  summarise(\n    Total = sum(Fuss_IN + Fuss_OUT),\n    Fuss_IN = sum(Fuss_IN),\n    Fuss_OUT = sum(Fuss_OUT))\n\n\n\nWeiter benötigen wir für das Aufzeigen der Verteilung der Besuchenden über den Tag die durchschnittliche Besucheranzahl pro Stunde (mean), aufgeteilt nach Tageszeit und Phase (group_by Tageszeit, Phase). Speichert das unter “mean_phase_d”.\n\n\n\nMusterlösung\nmean_phase_d &lt;- depo_daytime |&gt;\n  group_by(Phase, Tageszeit) |&gt;\n  summarise(\n    Total = mean(Total),\n    IN = mean(Fuss_IN),\n    OUT = mean(Fuss_OUT))\n\n\n\n\n4b)\n\nAggregiere die Stundenwerte nach dem Monat (group_by Monat, Jahr). Nun brauchen wir nur noch das Total, keine Richtungstrennung mehr. Speichert das neue df unter depo_m (“_m” für “Monat”).\n\nTipp: Braucht wiederum group_by() und summarise().\n\n\nMusterlösung\ndepo_m &lt;- depo |&gt;\n    group_by(Jahr, Monat) |&gt;\n    summarise(Total = sum(Total))\n\n\n\nFügt dem neu erstellten df depo_m eine Spalte mit Jahr + Monat hinzu.\n\nHier der fertige Code dazu (da etwas umständlich):\n\n\nMusterlösung\ndepo_m &lt;- depo_m |&gt;\n    mutate(\n        Ym = paste(Jahr, Monat), # und mache eine neue Spalte, in der Jahr und Monat sind\n        Ym = lubridate::ym(Ym)\n    ) # formatiere als Datum\n\n\n\nWiederholt diesen Schritt, diesmal aber mit der Gruppierung “Tageszeit” neben “Jahr” und “Monat” (wiederum sollen Jahr und Monat auch in einer Spalte stehen).\nSpeichert das Resultat unter “depo_m_daytime”.\n\n\n\nMusterlösung\n# Gruppiere die Werte nach Monat und TAGESZEIT\ndepo_m_daytime &lt;- depo |&gt;\n    group_by(Jahr, Monat, Tageszeit) |&gt;\n    summarise(Total = sum(Total))\n# sortiere das df aufsteigend (nur das es sicher stimmt)\n\ndepo_m_daytime &lt;- depo_m_daytime |&gt;\n    mutate(\n        Ym = paste(Jahr, Monat), # und mache eine neue Spalte, in der Jahr und Monat sind\n        Ym = lubridate::ym(Ym)\n    ) # formatiere als Datum\n\n\n\n\n4c)\nMacht euch mit den Daten vertraut. Plottet sie, seht euch die df’s an, versteht, was sie repräsentieren.\nZ.B. sind folgende Befehle und Plots wichtig:\n\nstr()\nsummarize()\nhead()\nScatterplot, x = Datum, y = Anzahl pro Zeiteinheit\nHistrogram\nusw.\n\nHinweis: Geht noch nicht zu weit mit euren Plots. Die Idee ist, dass man sich einen Überblick über die Daten verschafft und noch keine “analysierenden” Plots erstellt.\nNachdem nun alle Daten vorbereitet sind folgt im nächsten Schritt die deskriptive Analyse.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html",
    "title": "Deskriptive Analysen",
    "section": "",
    "text": "Aufgabe 1: Verlauf der Besuchszahlen nach Monat",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-1-verlauf-der-besuchszahlen-nach-monat",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-1-verlauf-der-besuchszahlen-nach-monat",
    "title": "Deskriptive Analysen",
    "section": "",
    "text": "1a)\nNachdem wir die Projektstruktur aufgebaut haben und die Daten vorbereitet (inkl. aggregiert) sind, machen wir uns an die deskriptive Analyse. Dies macht immer Sinn. Bevor mach sich an die schliessende Statistik macht, muss man ein “Gefühl” für die Daten bekommen. Dies funktioniert am einfachsten mit explorativen Analysen.\nWir interessieren uns in den Analysen für 5 Zeitabschnitte:\n\nvon Anfang Untersuchungsperiode bis vor dem 1. Lockdown (pre)\nLockdown 1\nZwischen beiden Lockdowns\nLockdown 2\nEnde 2. Lockdown bis Ende Untersuchungsperiode\n\nUnd dabei immer auch für die verschiedenen Tageszeiten (Morgen, Tag, Abend, Nacht).\n\nPlottet den Verlauf der monatlichen Besuchszahlen an eurer Zählstelle.\nAuf der x-Achse steht dabei dabei Jahr und Monat (gespeichert im df depo_m), auf der y-Achse die monatlichen Besuchszahlen.\nZeichnet auch die beiden Lockdown ein (Hinweis: rundet das Start- und Enddatum der Lockdowns auf den Monat, da im Plot die monatlichen Zahlen gezeigt werden).\n\nHaltet euch dabei an untenstehenden Plot:\n\n\n\n\n\n\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\n\nMusterlösung\nggplot(data = depo_m, mapping = aes(Ym, Total, group = 1)) + # group 1 braucht R, dass aus den Einzelpunkten ein Zusammenhang hergestellt wird\n    # zeichne Lockdown 1; ein einfaches Rechteck. bestimme mit min und max die Dimensionen\n  geom_rect(\n    mapping = aes(\n      xmin = ym(\"2020-3\"), xmax = ym(\"2020-5\"),\n      ymin = 0, ymax = max(Total + (Total / 100 * 10))\n    ), # das Rechteck soll 10 % grösser als die maximale Besuchszahl sein\n    fill = \"lightskyblue\", alpha = 0.2, colour = NA\n  ) +\n  # zeichne Lockdown 2\n  ... +\n  # zeichne die Linie\n  geom_line(...) +\n  scale_x_date(...)+\n  theme_linedraw(base_size = 15) +\n  ...\n\n\n\nExportiert euren Plot mit ggsave() nach results. Breite = 20, Höhe = 10, Einheiten = cm, dpi = 1000\n\n\n\n1b)\nDer erste Plot zeigt, wie sich die Besuchszahlen allgemein entwickelt haben. Interessant ist aber auch, wie sie während den einzelnen Monaten zueinander stehen (z.B. “Waren im Mai 2020 mehr Menschen unterwegs als im Mai 2017?”). Dies zeigt folgender Plot:\n\n\n\n\n\n\n\n\n\n\nErstellt diesen Plot und speichert ihn.\n\nHinweis: Dieser Plot wird erstellt mit dem Argument group, linetype und color innerhalb des _aes()__ Arguments. geom_line() wiederum zeichnet die Linien.\n\n\n1c)\nUnser Fokus liegt auf der dunklen Tageszeit. Wie verteilen sich die Besuchenden also auf Morgen, Tag, Abend und Nacht?\n\nErstellt und speichert folgenden Plot:\n\n\n\n\n\n\n\n\n\n\nDiese Codeschnipsel helfen euch:\n\n\nMusterlösung\nggplot(depo_m_daytime, aes(Ym, Total, fill = Tageszeit)) +\n  geom_area(...) +\n  scale_x_date(date_labels = \"%b%y\", date_breaks = \"6 months\", \n               limits = c(min(depo_m_daytime$Ym), max = max(depo_m_daytime$Ym)), expand = c(0, 0)) +\n  geom_vline(xintercept = seq(as.Date(min(depo_m_daytime$Ym)), as.Date(max(depo_m_daytime$Ym)), \n                              by = \"6 months\"), linetype = \"dashed\", color = \"black\")+\n  ...",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-2-wochengang",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-2-wochengang",
    "title": "Deskriptive Analysen",
    "section": "Aufgabe 2: Wochengang",
    "text": "Aufgabe 2: Wochengang\nNun möchten wir genauer untersuchen, wie sich die Besuchszahlen je nach Phase (Pre, Lockdown 1, Inter, Lockdown 2 und Post) auf die Wochentage und Tageszeiten verteilen.\n\n2a)\n\nErstellt dazu einen Violinplot nach untenstehender Vorgabe. Was sagt uns dieser komplexe Plot?\n\n\n\n\n\n\n\n\n\n\nHinweis:\n\nNutzt zum plotten ggplot()\nfolgende Codeschnipsel helfen euch:\n\n\n\nMusterlösung\nggplot(data = depo, aes(x = Wochentag, y = Total, fill = Tageszeit)) +\n  geom_violin() +\n  facet_grid(cols = vars(...), rows = vars(...))+\n  scale_y_log10()+ \n  ...\n\n\n\nWarum macht es Sinn, hier die y-Achse zu logarithmieren?\nGibt es alternative Darstellungsformen, welche besser geeignet wären?\nExportiert auch diesen Plot mit ggsave(). Welche Breite und Höhe passt hier?",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-3-tagesgang",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-3-tagesgang",
    "title": "Deskriptive Analysen",
    "section": "Aufgabe 3: Tagesgang",
    "text": "Aufgabe 3: Tagesgang\nVon der Übersicht ins Detail. Jetzt widmen wir uns dem Tagesgang, das heisst der Verteilung der Besuchenden auf die 24 Tagesstunden je nach Phase.\n\n3a)\n\nBerechnet zuerst den Mittelwert der totalen Besuchszahlen pro Wochentag, pro Stunde pro Phase und speichert das df unter Mean_h.\n\nVergewissert euch vor dem Plotten, dass der Datensatz im long-Format vorliegt.\n\n\n3b)\n\nPlottet den Tagesgang, unterteilt nach den 7 Wochentagen nun für die verschiedenen Phasen.\n\n\n\n\n\n\n\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\n\nMusterlösung\nggplot(Mean_h, aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag)) +\n  geom_line(...) +\n  facet_grid(...)\n...",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-4-kennzahlen",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-4-kennzahlen",
    "title": "Deskriptive Analysen",
    "section": "Aufgabe 4: Kennzahlen",
    "text": "Aufgabe 4: Kennzahlen\nBis hier hin haben wir in diesem Kapitel v.a. visuell gearbeitet. Für den Bericht kann es aber sinnvoll sein, auch einige Kennzahlen in der Hinterhand zu haben. Wir haben das bereits im Kapitel [Datenverarbeitung] berechnet.\n\nReflektiert, welche Zahlen ihr habt und was für den Bericht spannend sein könnte, resp. eure Abbildungen unterstützen oder ergänzen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/1_Vorbemerkung.html",
    "href": "fallstudie_n/1_Vorbemerkung.html",
    "title": "Vorbemerkung",
    "section": "",
    "text": "Aktuell dient diese Plattform für die BiEc Fallstudie - Profil N einzig der Bereitstellung von Aufgaben die von euch im Rahmen dieses Fallstudienprojekts erarbeitet werden sollen. Die Aufgaben werden in den meisten Fällen mit Code-Beispielen erläutert oder benötigten Code-snippets resp. Funktionen werden mitgeliefert. Im Laufe des Semesters werden hier ausserdem häppchenweise (mögliche) Lösungen zu den Aufgaben aufgeschaltet. Alles grundlegende Material und alle Unterlagen zu den theoretischen Inputs sind weiterhin und ausschliesslich im Moodlekurs Research Methods - Fallstudie BiEc zu finden. Die für die Aufgaben benötigten Datengrundlagen sind ebenfalls im entsprechenden Abschnitt auf Moodle zu finden. Frohes Schaffen!\n\nIm Rahmen der Fallstudie werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog der Vorbereitungsübung in Prepro1 könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\npacman::p_install(\"adehabitatHR\", \"bbmle\", \"car\", \"cowplot\", \"DHARMa\", \"dplyr\",\n  \"ggeffects\", \"ggplot2\", \"ggspatial\", \"glmmTMB\", \"gstat\", \"kableExtra\", \"lme4\",\n  \"MASS\", \"MuMIn\", \"pastecs\", \"performance\", \"PerformanceAnalytics\", \"psych\",\n  \"readr\", \"rms\", \"ROCR\", \"sf\", \"sjPlot\", \"sjstats\", \"terra\", character.only = TRUE,  force = FALSE)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Vorbemerkung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html",
    "href": "fallstudie_n/2_Datenverarbeitung.html",
    "title": "Daten(vor)verarbeitung",
    "section": "",
    "text": "Projektaufbau RStudio-Projekte\nVor den eigentlichen Auswertungen müssen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach untereinander ausgetauscht und somit auch reproduziert werden können; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: Link)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufbau-von-r-skripten",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufbau-von-r-skripten",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufbau von R-Skripten",
    "text": "Aufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n# .##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS24. Autor/in ####\n# .##################################################################################\n\nBeschreibt zudem folgendes:\n\nOrdnerstruktur; ich verwende hier den Projektordner mit den Unterordnern:\n\nSkripts\nData\nResults\nPlots\n\nVerwendete Daten\n\nEin Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. Dieses Schema beinhaltet (nach dem bereits erwähnten Kopf des Skripts) 4 Kapitel:\n\nDatenimport\nDatenvorverarbeitung\nAnalyse\nVisualisierung\n\nBereitet euer Skript also nach dieser Struktur vor. Nutzt für den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n# .###################################################################################\n# METADATA ####\n# .###################################################################################\n# Datenherkunft ####\n# ...\n\n# .###################################################################################\n# 1. DATENIMPORT ####\n# .###################################################################################",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#libraries-laden",
    "href": "fallstudie_n/2_Datenverarbeitung.html#libraries-laden",
    "title": "Daten(vor)verarbeitung",
    "section": "Libraries laden",
    "text": "Libraries laden\n\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"purrr\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#daten-laden",
    "href": "fallstudie_n/2_Datenverarbeitung.html#daten-laden",
    "title": "Daten(vor)verarbeitung",
    "section": "Daten laden",
    "text": "Daten laden\nHerunterladen der Daten der Feldaufnahmen von Moodle (Aufgabe3_Feldaufnahmen_alle_Gruppen.zip), Einlesen, Sichtung der Datensätze und der Datentypen.\nVerschiedene Dinge funktionierten nicht auf Anhieb:\n\nDaten Gruppe 1: leere Zeilen zwischen jedem Dateneintrag (R ist intelligent genug dies beim Einlesen zu erkennen)\nDaten Gruppe 5: leere Zeilen zwischen jedem Dateneintrag (R ist intelligent genug dies beim Einlesen zu erkennen)\nDaten Gruppe 6:\n\nExcelfile –&gt; csv daraus machen\nKoordinaten fehlen, diese werden benötigt um die Daten über die Kreise eindeutig mit den LIDAR-Daten zusammenzuführen –&gt; einfügen aus Zuteilung_Kreise_Aufnahmen_Landforst_HS24.docx\nKreise als Datentyp character, müssen numeric sein\n\nDaten Gruppe 7: Koordinaten fehlen, diese werden benötigt um die Daten über die Kreise eindeutig mit den LIDAR-Daten zusammenzuführen –&gt; einfügen aus Zuteilung_Kreise_Aufnahmen_Landforst_HS24.docx\n\nVersucht wenn möglich solche Dinge jeweils direkt mit R zu lösen, dies ist vorallem bei grösseren Datensätzen extrem hilfreich, damit die Datensätze zu einem sauberen Gesamtdatensatz zusammengefügt werden können.\n\ndf_team1 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Aufgabe_2_Team1.csv\", delim = \";\") \n\ndf_team2 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/FelderhebungenSilhwaldKreise.csv\", delim = \",\")\n\ndf_team3 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebungen Waldstruktur Team 3 pink (Gruppe 4).csv\", delim = \";\")\n\ndf_team4 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebung_Gruppe 5.csv\", delim = \";\")\n\ndf_team5 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/TEAM 5 (violett) - Felderhebungen Waldstruktur.csv\", delim = \",\")\n\ndf_team6 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebung_Team6.csv\", \n                       delim = \";\", \n                       locale = locale(encoding = \"latin1\")) %&gt;%\n  mutate(Punkt = parse_number(Punkt)) \n\n\ndf_team7 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/gr7_ground_thruth_lidar.csv\", delim = \";\")\n\n\n# hier können die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der\n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_lidar &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_LIDAR_Waldstruktur_Reh_Kreise_241011.csv\", delim = \";\")\nstr(df_lidar)\n## spc_tbl_ [305 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Anz_reh_lokalisationen: num [1:305] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ x                     : num [1:305] 684900 684900 684900 684900 684875 ...\n##  $ y                     : num [1:305] 237100 237125 237150 237175 237075 ...\n##  $ DG_us_2014            : num [1:305] 0.0903 0.2717 0.468 0.7407 0.1811 ...\n##  $ DG_os_2014            : num [1:305] 0.908 0.959 0.871 0.986 0.86 ...\n##  $ DG_us_2022            : num [1:305] 0.269 0.823 0.936 0.359 0.245 ...\n##  $ DG_os_2022            : num [1:305] 0.945 0.99 0.953 0.997 0.898 ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   Anz_reh_lokalisationen = col_double(),\n##   ..   x = col_double(),\n##   ..   y = col_double(),\n##   ..   DG_us_2014 = col_double(),\n##   ..   DG_os_2014 = col_double(),\n##   ..   DG_us_2022 = col_double(),\n##   ..   DG_os_2022 = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n# Die eingelesenen Datensätze anschauen und versuchen zu einem Gesamtdatensatz\n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt &lt;- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5,df_team6,df_team7)\nstr(df_gesamt)\n## spc_tbl_ [175 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Kreis (r 12.5m)               : num [1:175] 0 1 2 3 4 5 6 7 8 9 ...\n##  $ X                             : num [1:175] 684900 684875 684875 684875 684850 ...\n##  $ Y                             : num [1:175] 237175 237125 237175 237250 237225 ...\n##  $ Deckungsgrad Rubus sp. [%]    : num [1:175] 20 20 15 55 17 10 15 10 20 65 ...\n##  $ DG Strauchschicht [%] (0.5-3m): num [1:175] 50 35 50 60 90 75 75 80 15 27 ...\n##  $ DG Baumschicht [%] (ab 3m)    : num [1:175] 30 40 65 70 60 70 60 45 63 40 ...\n##  $ Punkt                         : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Rubus sp.                     : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Strauchschicht                : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Baumschicht                   : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Kreis                         : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ DG_Rubus_sp                   : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ DG_Strauchschicht             : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ DG_Baumschicht                : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   `Kreis (r 12.5m)` = col_double(),\n##   ..   X = col_double(),\n##   ..   Y = col_double(),\n##   ..   `Deckungsgrad Rubus sp. [%]` = col_double(),\n##   ..   `DG Strauchschicht [%] (0.5-3m)` = col_double(),\n##   ..   `DG Baumschicht [%] (ab 3m)` = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-1",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-1",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\n\n1.1 Einfügen zusätzliche Spalte pro Datensatz mit der Gruppenzugehörigkeit (Team1-7)\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensätzen gleich sind und der Gesamtdatensatz ohne Probleme zusammengefügt werden kann\n\n→ Befehle mutate und rename, mit pipes (alt: %&gt;%, neu: |&gt;) in einem Schritt möglich\n\n\n\n\nMusterlösung\n# .#################################################################################\n# 2. DATENVORVERARBEITUNG #####\n# .#################################################################################\n\ndf_team1 &lt;- df_team1 |&gt;\n  mutate(team = \"team1\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team2 &lt;- df_team2 |&gt;\n  mutate(team = \"team2\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team3 &lt;- df_team3 |&gt;\n  mutate(team = \"team3\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team4 &lt;- df_team4 |&gt;\n  mutate(team = \"team4\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team5 &lt;- df_team5 |&gt;\n  mutate(team = \"team5\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team6 &lt;- df_team6 |&gt;\n  mutate(team = \"team6\") |&gt;\n  rename(\n    KreisID = \"Punkt\",\n    DG_Rubus = \"Rubus sp.\",\n    DG_Strauchschicht = \"Strauchschicht\",\n    DG_Baumschicht = \"Baumschicht\"\n  )\n\ndf_team7 &lt;- df_team7 |&gt;\n  mutate(team = \"team7\") |&gt;\n  rename(\n    KreisID = \"Kreis\",\n    DG_Rubus = \"DG_Rubus_sp\",\n    DG_Strauchschicht = \"DG_Strauchschicht\",\n    DG_Baumschicht = \"DG_Baumschicht\"\n  )\n\n\nDas Learning aus den Vorverarbeitungen die ich für euch übernommen habe und aus Aufgabe 1 ist, dass beim Erfassen und Dokumentieren von Daten aus verschiedenen Quellen darauf geachtet werden sollte, dies möglichst einheitlich zu tun (Dateiformate, Spalten, Bezeichnungen, Datentypen, usw.), damit kann man sich viel Arbeit ersparen. Hilfreich ist in diesem Zusammenhang immer ein einheitliches Feldprotokoll resp. eine Vorlage für die Erfassung der Daten zu erstellen.",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-2",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-2",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nZusammenführen der Teildatensätze zu einem Datensatz\n\n\nMusterlösung\ndf_gesamt &lt;- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6, df_team7)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-3",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-3",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der LIDAR Variablen in den Reh-Kreisen (Aufgabe3_LIDAR_Waldstruktur_Reh_Kreise_241011.csv).\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us_2022, DG_os_2022) aus dem LIDAR-Waldstruktur-Datensatz. –&gt; Welche Art von join? Welche Spalten zum Verbinden (join_by()) der Datensätze\n\n\nMusterlösung\ndf_with_LIDAR &lt;- left_join(df_gesamt, df_lidar, join_by(X == x, Y == y))",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-4",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-4",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LiDAR-Daten (DG_xy_2022) erstellen (zusätzlich Einfärben der Gruppen und Regressionslinie darüberlegen). Korrelieren die Feldaufnahmen und die LiDAR basierte Waldstruktur?\nIm LiDAR Datensatz gibt es dieselben Variablen der Waldstruktur aus der LiDAR-Befliegung 2014. Ihr könnt untersuchen wie sich diese verändert haben und wie gut oder eben auch nicht sie mit euren Feldaufnahmen übereinstimmen.\n\n\nMusterlösung\n# .#####################################################################################\n# 4. VISUALISERUNG #####\n# .#####################################################################################\n\nggplot(df_with_LIDAR, aes(DG_us_2022, DG_Strauchschicht, color = team)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  xlim(0, 1) +\n  ylim(0, 100) +\n  geom_abline(slope = 100, intercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nMusterlösung\n\n\nwrite_delim(df_with_LIDAR, \"datasets/fallstudie_n/df_with_lidar_2024.csv\", delim = \";\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#zusatz-aufgabe-5",
    "href": "fallstudie_n/2_Datenverarbeitung.html#zusatz-aufgabe-5",
    "title": "Daten(vor)verarbeitung",
    "section": "(Zusatz-)Aufgabe 5",
    "text": "(Zusatz-)Aufgabe 5\nHerunterladen der Felderhebungsdaten von Moodle aus den vergangenen Jahren. Zusammenführen aller Datensätze. Explorative Datenanalyse zu den Veränderungen der Erhebungen ground truth LiDAR über die Zeit und zum Zusammenhang mit den LiDAR-basierten Waldstrukturvariablen aus den zwei Befliegungszeiträumen (2014 und 2022)\n!Achtung! es sind nicht alle Jahre gleich viele Teams an den Erhebungen beteiligt gewesen, daher gib es nicht für alle Teams in allen Jahren Daten.\n\n\nMusterlösung\n\nfiles &lt;- list.files(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_vergangene_Jahre\", pattern = \"*.csv\", full.names = TRUE)\n\n\ndf_ground_truth &lt;- files %&gt;%\n  map_df(~ read_delim(.x, delim = \";\") %&gt;% \n           mutate(Jahr = gsub(\"df_ground_truth_|\\\\.csv$\", \"\", basename(.x))))\n\n\ndf_without_LIDAR &lt;- df_with_LIDAR %&gt;% dplyr::select(KreisID:team) %&gt;% mutate(Jahr = \"2024\")\n\ndf_ground_truth &lt;- bind_rows(df_without_LIDAR, df_ground_truth)\n\ndf_ground_truth_with_LIDAR &lt;- left_join(df_ground_truth, df_lidar, join_by(X == x, Y == y))\n\ndf_ground_truth_with_LIDAR %&gt;%\n  ggplot(aes(KreisID, DG_Strauchschicht, color = as.numeric(Jahr), size = as.numeric(Jahr))) +\n  facet_wrap(~team, ncol = 1) +\n  geom_point() +\n  scale_color_viridis_c(option = \"plasma\") +  \n  scale_size_continuous(range = c(1, 3)) +  \n  labs(color = \"Year\", shape = \"Year\", size = \"Year\") +  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nMusterlösung\n\n\ndf_without_LIDAR &lt;- df_without_LIDAR %&gt;% dplyr::select(-Jahr)\n\nwrite_delim(df_without_LIDAR, \"datasets/fallstudie_n/df_ground_truth_2024.csv\", delim = \";\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html",
    "title": "BiEc3_N Homeranges",
    "section": "",
    "text": "Libraries laden\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"terra\")\nlibrary(\"adehabitatHR\")\nlibrary(\"ggspatial\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#daten-einlesen",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#daten-einlesen",
    "title": "BiEc3_N Homeranges",
    "section": "Daten einlesen",
    "text": "Daten einlesen\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\nRehe &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Homeranges_Rehe_landforst_20231023.csv\", delim = \";\")\n\nstr(Rehe)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-1",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-1",
    "title": "BiEc3_N Homeranges",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nIm Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\nRehe &lt;- Rehe |&gt;\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time),\n                                   format = \"%Y-%m-%d %H:%M:%S\"))",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-2",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-2",
    "title": "BiEc3_N Homeranges",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nMit den folgenden Zeilen können die GPS-Punkte visualisiert werden\n\nRehe_sf &lt;- st_as_sf(Rehe, coords = c(\"X\", \"Y\"), crs = 21781)\n\nRE13 &lt;- filter(Rehe_sf, TierID == \"RE13\")\n\nplot(RE13[\"TierID\"])\n\n\n\n\n\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\nHerumschrauben an der Ausdehnung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\n→ Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\n\nRE13_xy &lt;- st_coordinates(RE13)\n\nRE13_sp &lt;- as(RE13[\"TierID\"], \"Spatial\")\n\nsigma &lt;- 0.5 * (sd(RE13_xy[, 1]) + sd(RE13_xy[, 2]))\nn &lt;- nrow(RE13)\nhref &lt;- sigma * n ^ (-1 / 6) * 0.9\n\n# scaled reference: href * 0.9\n\nkud &lt;- kernelUD(RE13_sp, h = href, grid = 25)\n\n# Berechnung der Home Range (95% Isopleth)\n\nhomerange &lt;- getverticeshr(kud, percent = 95)\n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr &lt;- st_as_sf(homerange)\n\nst_write(\n  hr,\n  dsn = \"Results\",\n  layer = \"HR_RE13\",\n  driver = \"ESRI Shapefile\",\n  delete_layer = TRUE\n)\n## Deleting layer `HR_RE13' using driver `ESRI Shapefile'\n## Writing layer `HR_RE13' to data source `Results' using driver `ESRI Shapefile'\n## Writing 1 features with 2 fields and geometry type Polygon.\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\nggplot(hr) + \n  geom_sf(size = 1, alpha = 0.3, color = \"red\", fill=\"red\") +\n  coord_sf(datum = sf::st_crs(21781))+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n  )\n\n\n\n\n\n\n\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nggplot(hr) + \n  geom_sf(size = 1, alpha = 0.3, color = \"red\", fill=\"red\") +\n  geom_sf(data = RE13, aes(fill = \"red\")) +\n  coord_sf(datum = sf::st_crs(21781))+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n  )\n\n\n\n\n\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden.\n\npk25_wpz &lt;- rast(\"datasets/fallstudie_n/pk25_wpz.tif\")\n\nggplot(hr, aes(color = \"red\", fill = \"red\")) +\n  annotation_spatial(pk25_wpz) +\n  geom_sf(size = 1, alpha = 0.3) +\n  geom_sf(data = RE13, aes(fill = \"red\")) +\n  coord_sf(datum = sf::st_crs(21781)) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\n\nAusdehnung des Grids basiert auf hr\nCellsize des Grids: 25m\n\n\nx25 &lt;- st_make_grid(hr, 25, what = \"centers\")\ngrid_plot &lt;- st_buffer(x25, 12.5)\n\nggplot(grid_plot, color = \"black\", fill = NA) +\n  geom_sf() +\n  geom_sf(data = RE13, color = \"blue\", ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\n  coord_sf(datum = 21781) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-3",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-3",
    "title": "BiEc3_N Homeranges",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nTesten der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar den wir letzte Woche erstellt haben\nDie Theorie zu Korrelation folgt erst ab 31.10.\n\ndf_with_lidar &lt;- read_delim(\"datasets/fallstudie_n/df_with_lidar.csv\", delim = \";\")\n\ncor.test(~ DG_Strauchschicht + DG_us_2022, data = df_with_lidar, method = \"pearson\")\n## \n##  Pearson's product-moment correlation\n## \n## data:  DG_Strauchschicht and DG_us_2022\n## t = 4.8012, df = 123, p-value = 4.49e-06\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.2382939 0.5355237\n## sample estimates:\n##       cor \n## 0.3972769",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "Literaturverzeichnis",
    "section": "",
    "text": "Kovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen?\nWirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Literaturverzeichnis</span>"
    ]
  }
]