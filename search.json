[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods HS24",
    "section": "",
    "text": "Willkommen\nDas Modul „Research Methods” vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen” auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen”. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverarbeitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "PrePro.html",
    "href": "PrePro.html",
    "title": "Pre-Processing",
    "section": "",
    "text": "Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on” Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der lesson sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.\nDie Lesson vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape”, „split-apply-combine”). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser lesson lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.",
    "crumbs": [
      "Pre-Processing"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Vorbereitung.html",
    "href": "prepro/Prepro1_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "R ist ohne Zusatzpackete, sogenannte “Packages” nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Allerdings prüft R dabei nicht, ob das Package bereits vorhanden ist oder nicht: Auch bereits installierte Packages werden nochmal installiert, was unter Umständen ziemlich unpraktisch sein kann.\nAlternativ zu install.packages können Packages auch mittels der Funktion p_install installiert werden. In der Funktion p_install wird zuerst geprüft, ob das Package vorhanden ist. Ist das jeweilige Package vorhanden, wird auf eine Installation verzichtet (bei force = FALSE).\nDie Funktion p_install wird von dem Package pacman zur Verfügung gestellt. Dieses Package muss initial ganz klassisch mit install.packages installiert werden. Um die Funktion p_install aus pacman zu verwenden, muss das Package nach der installation mittels library(\"pacman\") geladen werden.\n\n\n# so werden packages klassischerweise installiert:\ninstall.packages(\"lubridate\")\n\n# so werden sie in die aktuelle Session geladen:\nlibrary(lubridate)\n\n# nun kann eine Funktion aus dem geladenen Package verwendet werden\n# (die Funktion \"now()\" war vorher nicht verfübar)\nnow()\n\n# so werden packages mit \"pacman installiert:\nlibrary(pacman)\np_install(\"dplyr\", character.only = TRUE, force = FALSE)\n\n\n\n\n\n\n\nWichtig\n\n\n\nDie häufigste Verwirrung von Einsteigern liegt in der Verwendung von Packages. Dieses Kapitel unbedingt vormerken und bei Bedarf nochmal lesen.\n\n\nIm Rahmen von Prepro 1 - 3 werden wir folgende Packages brauchen: dplyr, ggplot2, lubridate, readr und tidyr. Wir empfehlen, diese bereits vor der ersten Lektion mit pacman zu installieren (s.u.).\n\n\nlibrary(pacman)\n\npackages &lt;- c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"tidyr\")\n\nsapply(packages,\\(x) pacman::p_install(x, force = FALSE, character.only = TRUE))\n\n# character.only = TRUE: die Packages werden in Quotes angegeben\n# force = FALSE:         die Packages werden nur installiert, \n#                        wenn noch nicht vorhanden",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Demo.html",
    "href": "prepro/Prepro1_Demo.html",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "Datentypen\nDiese Demo kann hier heruntergeladen werden (Rechtsklick → speichern unter).",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Prepro 1: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#footnotes",
    "href": "prepro/Prepro1_Demo.html#footnotes",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "ordered = T kann nur bei der Funktion factor() spezifiziert werden, nicht bei as.factor(). Ansonsten sind factor() und as.factor() sehr ähnlich.↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Prepro 1: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html",
    "href": "prepro/Prepro1_Uebung.html",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Arbeiten mit RStudio “Project”\nWir empfehlen die Verwendung von “Projects” innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr hier.\nDas Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "href": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (setwd()). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt)\nAutomatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session\nFestlegen verschiedener projektspezifischer Optionen\nVerwendung von Versionsverwaltungssystemen (z.B. git)\n\n\n\n\n\n\n\nWichtig 3.1: Prüfungsrelevant\n\n\n\nDie korrekte Verwendung von RStudio Projects und relativen Pfaden wird an der praktischen Prüfung vorausgesetzt!",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nErstelle eine data.frame mit nachstehenden Daten.\n\n\nMusterlösung\ndf &lt;- data.frame(\n  Tierart = c(\"Fuchs\", \"Bär\", \"Hase\", \"Elch\"),\n  Anzahl = c(2, 5, 1, 3),\n  Gewicht = c(4.4, 40.3, 1.1, 120),\n  Geschlecht = c(\"m\", \"f\", \"m\", \"m\"),\n  Beschreibung = c(\"Rötlich\", \"Braun, gross\", \"klein, mit langen Ohren\", \"Lange Beine, Schaufelgeweih\")\n)\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\n\n\nBär\n5\n40.3\nf\nBraun, gross\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nWas für Datentypen wurden in der letzten Aufgabe automatisch angenommen? Ermittle diese mit str() und prüfe, ob diese sinnvoll sind und wandle um wo nötig.\n\n\nMusterlösung\nstr(df)\n## 'data.frame':    4 obs. of  5 variables:\n##  $ Tierart     : chr  \"Fuchs\" \"Bär\" \"Hase\" \"Elch\"\n##  $ Anzahl      : num  2 5 1 3\n##  $ Gewicht     : num  4.4 40.3 1.1 120\n##  $ Geschlecht  : chr  \"m\" \"f\" \"m\" \"m\"\n##  $ Beschreibung: chr  \"Rötlich\" \"Braun, gross\" \"klein, mit langen Ohren\" \"Lange Beine, Schaufelgeweih\"\ntypeof(df$Anzahl)\n## [1] \"double\"\n# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`.\n\ndf$Anzahl &lt;- as.integer(df$Anzahl)",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNutze die Spalte Gewicht, um die Tiere in 3 Gewichtskategorien einzuteilen:\n\nleicht: &lt; 5kg\nmittel: 5 - 100 kg\nschwer: &gt; 100kg\n\n\n\nMusterlösung\ndf$Gewichtsklasse[df$Gewicht &gt; 100] &lt;- \"schwer\"\ndf$Gewichtsklasse[df$Gewicht &lt;= 100 & df$Gewicht &gt; 5] &lt;- \"mittel\"\ndf$Gewichtsklasse[df$Gewicht &lt;= 5] &lt;- \"leicht\"\n\n\nDas Resultat:\n\n\n\n\n\n\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\nGewichtsklasse\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\nleicht\n\n\nBär\n5\n40.3\nf\nBraun, gross\nmittel\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\nleicht\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih\nschwer",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nAuf Moodle findest du ein Zip-File mit dem Namen prepro.zip. Lade das File herunter und entpacke es in deinem Projektordner. Importiere die Datei weather.csv. Falls du dafür das RStudio GUI verwendest, speichere den Import-Befehl in deinem R-Script ab. Bitte verwende einen relativen Pfad (also kein Pfad, der mit C:/, ~/ o.ä. beginnt).)\n\n\n\n\n\n\nHinweis 3.1\n\n\n\nWir nutzen readr, um csvs zu importieren, und verwenden die Funktion read_delim (mit underscore) als alternative zu read.csv oder read.delim (mit Punkt). Das ist eine persönliche Präferenz1, es ist euch überlassen, welche Funktion ihr verwendet. Beachtet, dass die beiden Funktionen leicht andere Parameter erwarten.\n\n\n\n\nMusterlösung\nlibrary(\"readr\")\n\n\nwetter &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nSchau dir die Rückmeldung von read_delim() an. Sind die Daten korrekt interpretiert worden?\n\n\nMusterlösung\n# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es\n# sich offensichtlich um Zeitangaben.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nDie Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() ein und spezifiziere sowohl format wie auch tz.\n\n\n\n\n\n\nTipp\n\n\n\n\nWenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe metadata.csv)\nas.POSIXct erwartet character: Wenn du eine Fehlermeldung hast die 'origin' must be supplied (o.ä) heisst, hast du der Funktion vermutlich einen Numeric übergeben.\n\n\n\n\n\nMusterlösung\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\", tz = \"UTC\")\n\n\n\n\n\nDie neue Tabelle sollte so aussehen\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nErstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte sowie eine geeignete Funktion aus lubridate.\n\n\nMusterlösung\nlibrary(\"lubridate\")\n\nwetter$wochentag &lt;- wday(wetter$time, label = T)\nwetter$kw &lt;- week(wetter$time)\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSat\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSat\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSat\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSat\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSat\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSat\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSat\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSat\n1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nErstelle eine neue Spalte basierend auf den Temperaturwerten mit der Einteilung “kalt” (unter Null Grad) und “warm” (über Null Grad)\n\n\nMusterlösung\nwetter$temp_kat[wetter$tre200h0 &gt; 0] &lt;- \"warm\"\nwetter$temp_kat[wetter$tre200h0 &lt;= 0] &lt;- \"kalt\"\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\ntemp_kat\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSat\n1\nkalt\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSat\n1\nkalt\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSat\n1\nkalt\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSat\n1\nkalt\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSat\n1\nkalt\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSat\n1\nkalt\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSat\n1\nkalt\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSat\n1\nkalt\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSat\n1\nkalt\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSat\n1\nkalt",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#footnotes",
    "href": "prepro/Prepro1_Uebung.html#footnotes",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Vorteile von read_delim gegenüber read.csv: https://stackoverflow.com/a/60374974/4139249↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>PrePro 1: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html",
    "href": "prepro/Prepro2_Demo.html",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "Piping\nGegeben ist ein character string (diary). Wir wollen aus diesem Text die Temperaturangabe aus dem String extrahieren, danach den Wert von Kelvin in Celsius nach der folgenden Formel umwandeln und zum Schluss den Mittelwert über all diese Werte berechnen.\n\\[°C = K - 273.15\\]\ndiary &lt;- c(\n  \"The temperature is 310° Kelvin\",\n  \"The temperature is 322° Kelvin\",\n  \"The temperature is 410° Kelvin\"\n)\n\ndiary\n## [1] \"The temperature is 310° Kelvin\" \"The temperature is 322° Kelvin\"\n## [3] \"The temperature is 410° Kelvin\"\nDazu brauchen wir die Funktion substr(), welche aus einem character einen Teil “raus schnipseln” kann.\n# Wenn die Buchstaben einzelne _Elemente_ eines Vektors wären, würden wir diese\n# folgendermassen subsetten:\n\ncharvec1 &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\")\ncharvec1[4:6]\n## [1] \"d\" \"e\" \"f\"\n\n# Aber wenn diese in einem einzigen character gespeichert sind, brauchen wir substr:\ncharvec2 &lt;- \"abcdefgh\"\nsubstr(charvec2, 4, 6)\n## [1] \"def\"\nZudem nutzen wir eine Hilfsfunktion subtrahieren, welche zwei Werte annimmt, den minuend und den subtrahend:\nsubtrahieren &lt;- function(minuend, subtrahend) {\n  minuend - subtrahend\n}\n\nsubtrahieren(10, 4)\n## [1] 6\nÜbersetzt in R-Code entsteht folgende Operation:\noutput &lt;- mean(subtrahieren(as.numeric(substr(diary, 20, 22)), 273.15))\n#                                             \\_1_/\n#                                      \\________2__________/\n#                           \\___________________3___________/\n#              \\________________________________4__________________/\n#         \\_____________________________________5____________________/\n\n# 1. Nimm diary\n# 2. Extrahiere auf jeder Zeile die Werte 20 bis 22\n# 3. Konvertiere \"character\" zu \"numeric\"\n# 4. Subtrahiere 273.15\n# 5. Berechne den Mittlwert\nDie ganze Operation liest sich etwas leichter, wenn diese sequentiell notiert wird:\ntemp &lt;- substr(diary, 20, 22)      # 2\ntemp &lt;- as.numeric(temp)           # 3\ntemp &lt;- subtrahieren(temp, 273.15) # 4\noutput &lt;- mean(temp)               # 5\nUmständlich ist dabei einfach, dass die Zwischenresultate immer abgespeichert und in der darauf folgenden Operation wieder abgerufen werden müssen. Hier kommt “piping” ins Spiel: Mit “piping” wird der Output der einen Funktion der erste Parameter der darauf folgenden Funktion.\ndiary |&gt;                  # 1\n  substr(20, 22) |&gt;       # 2\n  as.numeric() |&gt;         # 3\n  subtrahieren(273.15) |&gt; # 4\n  mean()                  # 5\n## [1] 74.18333",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "Wichtig\n\n\n\n\nder |&gt; Pipe Operator wurde erst in R 4.1 eingeführt\nNeben dem base R Pipe Operator existiert im Package magrittr ein sehr ähnlicher1 Pipe Operator: %&gt;%\nDie Tastenkombination Ctrl+Shift+M in RStudio fügt einen Pipe Operator ein.\nWelcher Pipe Operator |&gt; oder %&gt;% mit der obigen Tastenkombination eingeführt wird, kann über die RStudio Settings Tools → Global Options → Code → Häckchen setzen bei Use nativ pipe operator\nWir empfehlen die base-R Pipe |&gt; zu verwenden",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudierende &lt;- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  PLZ = c(8006, 8001, 8820)\n)\n\nstudierende\n##   Matrikel_Nr   Studi  PLZ\n## 1      100002 Patrick 8006\n## 2      100003 Manuela 8001\n## 3      200003     Eva 8820\n\nortschaften &lt;- data.frame(\n  PLZ = c(8003, 8006, 8810, 8820),\n  Ortsname = c(\"Zürich\", \"Zürich\", \"Horgen\", \"Wädenswil\")\n)\n\nortschaften\n##    PLZ  Ortsname\n## 1 8003    Zürich\n## 2 8006    Zürich\n## 3 8810    Horgen\n## 4 8820 Wädenswil\n\n\n# Load library\nlibrary(\"dplyr\")\n\ninner_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n\nleft_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      &lt;NA&gt;\n## 3      200003     Eva 8820 Wädenswil\n\nright_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n## 3          NA    &lt;NA&gt; 8003    Zürich\n## 4          NA    &lt;NA&gt; 8810    Horgen\n\nfull_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      &lt;NA&gt;\n## 3      200003     Eva 8820 Wädenswil\n## 4          NA    &lt;NA&gt; 8003    Zürich\n## 5          NA    &lt;NA&gt; 8810    Horgen\n\n\nstudierende &lt;- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Wohnort = c(8006, 8001, 8006)\n)\n\nleft_join(studierende, ortschaften, by = c(\"Wohnort\" = \"PLZ\"))\n##   Matrikel_Nr   Studi Wohnort Ortsname\n## 1      100002 Patrick    8006   Zürich\n## 2      100003 Manuela    8001     &lt;NA&gt;\n## 3      200003  Pascal    8006   Zürich",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#footnotes",
    "href": "prepro/Prepro2_Demo.html#footnotes",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "siehe https://stackoverflow.com/q/67633022/4139249↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html",
    "href": "prepro/Prepro2_Uebung_A.html",
    "title": "Prepro 2: Übung A",
    "section": "",
    "text": "Aufgabe 1\nLese die Wetterdaten von letzer Woche weather.csv (Quelle MeteoSchweiz) in R ein. Sorge dafür, dass die Spalten korrekt formatiert sind (stn als factor, time als POSIXct, tre200h0 als numeric.)\nMusterlösung\nlibrary(\"readr\")\n\nwetter &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\nwetter$stn &lt;- as.factor(wetter$stn)\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\", tz = \"UTC\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nLese den Datensatz metadata.csv ebenfalls als csv ein.\n\n\n\n\n\n\nTipp\n\n\n\nWenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. das è in Genève), hat das vermutlich mit der Zeichencodierung zu tun. Das File ist aktuell in UTF-8 codiert. Wenn Umlaute nicht korrekt dargestellt werden, hat R diese Codierung nicht erkannt und sie muss in der Import-Funktion spezifitiert werden. Dies wird je nach verwendete import Funktion unterschiedlich gemacht:\n\nFunktionen aus dem Package readr: locale = locale(encoding = \"UTF-8\")\nBase-R Funktionen: fileEncoding = \"UTF-8\"\n\nWenn ihr die Codierung eines Files nicht kennt, könnt ihr wie folgt vorgehen: Anleitung für Windows, für Mac und für Linux.\n\n\n\n\nMusterlösung\nmetadata &lt;- read_delim(\"datasets/prepro/metadata.csv\", locale = locale(encoding = \"UTF-8\"))",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNun wollen wir den Datensatz wetter mit den Informationen aus metadata anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Selektiere diese Spalten.\n\n\nMusterlösung\nmetadata &lt;- metadata[, c(\"stn\", \"Name\", \"x\", \"y\", \"Meereshoehe\")]",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nJetzt kann metadata mit dem Datensatz wetter verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir “joinen” können.\nNutze die Join-Möglichkeiten von dplyr (Hilfe via ?dplyr::join), um die Datensätze wetter und metadata zu verbinden.\n\n\nMusterlösung\nlibrary(\"dplyr\")\nwetter &lt;- left_join(wetter, metadata, by = \"stn\")\n\n# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.\n# Attribut: \"stn\"",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nErstelle eine neue Spalte month, welche den jeweiligen Monat (aus time) beinhaltet. Nutze dafür die Funktion lubridate::month().\n\n\nMusterlösung\nlibrary(\"lubridate\")\n\nwetter$month &lt;- month(wetter$time)",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nBerechne mit der Spalte month die Durchschnittstemperatur pro Monat.\n\n\nMusterlösung\nmean(wetter$tre200h0[wetter$month == 1])\n## [1] -1.963239\nmean(wetter$tre200h0[wetter$month == 2])\n## [1] 0.3552632\nmean(wetter$tre200h0[wetter$month == 3])\n## [1] 2.965054\n\n# usw. für alle 12 Monate",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Übung A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html",
    "href": "prepro/Prepro2_Uebung_B.html",
    "title": "Prepro 2: Übung B",
    "section": "",
    "text": "Aufgabe 1\nGegeben sind die Daten von drei Sensoren (sensor1.csv, sensor2.csv, sensor3.csv). Lese die Datensätze ein.\nMusterlösung\nlibrary(\"readr\")\n\nsensor1 &lt;- read_delim(\"datasets/prepro/sensor1.csv\")\nsensor2 &lt;- read_delim(\"datasets/prepro/sensor2.csv\")\nsensor3 &lt;- read_delim(\"datasets/prepro/sensor3.csv\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle aus den 3 Dataframes einen einzigen Dataframe, welcher aussieht wie unten dargestellt. Nutze dafür zwei joins aus dplyr, um 3 data.frames miteinander zu verbinden. Bereinige im Anschluss die Spaltennamen (wie geht das?).\n\n\nMusterlösung\nlibrary(\"dplyr\")\n\nsensor1_2 &lt;- full_join(sensor1, sensor2, \"Datetime\")\n\nsensor1_2 &lt;- rename(sensor1_2, sensor1 = Temp.x, sensor2 = Temp.y)\n\nsensor_all &lt;- full_join(sensor1_2, sensor3, by = \"Datetime\")\n\nsensor_all &lt;- rename(sensor_all, sensor3 = Temp)\n\n\n\n\n\n\n\nDatetime\nsensor1\nsensor2\nsensor3\n\n\n\n\n16102017_1800\n23.5\n13.5\n26.5\n\n\n17102017_1800\n25.4\n24.4\n24.4\n\n\n18102017_1800\n12.4\n22.4\n13.4\n\n\n19102017_1800\n5.4\n12.4\n7.4\n\n\n23102017_1800\n23.5\n13.5\nNA\n\n\n24102017_1800\n21.3\n11.3\nNA",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Datei sensor_fail.csv in R.\nsensor_fail.csv hat eine Variabel SensorStatus: 1 bedeutet der Sensor misst, 0 bedeutet der Sensor misst nicht. Fälschlicherweise wurde auch dann der Messwert Temp = 0 erfasst, wenn Sensorstatus = 0. Richtig wäre hier NA (not available). Korrigiere den Datensatz entsprechend.\n\n\nMusterlösung\nsensor_fail &lt;- read_delim(\"datasets/prepro/sensor_fail.csv\")\n\n# mit base-R:\nsensor_fail$Temp_correct[sensor_fail$SensorStatus == 0] &lt;- NA\nsensor_fail$Temp_correct[sensor_fail$SensorStatus != 0] &lt;- sensor_fail$Temp #Warnmeldung kann ignoriert werden.\n\n# das gleiche mit dplyr:\nsensor_fail &lt;- sensor_fail |&gt;\n  mutate(Temp_correct = ifelse(SensorStatus == 0, NA, Temp))\n\n\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\nTemp_correct\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n0.6\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n0.3\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n0.0\n\n\nSen102\n0.0\n86\n19102017_1800\n0\nNA\n\n\nSen102\n0.0\n98\n23102017_1800\n0\nNA\n\n\nSen102\n0.0\n98\n24102017_1800\n0\nNA\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n0.0\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n-0.3\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n-0.7\n\n\nSen103\n-1.2\n98\n28102017_1800\n1\n-1.2",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWarum spielt es eine Rolle, ob 0 oder NA erfasst wird? Berechne die Mittlere der Temperatur / Feuchtigkeit nach der Korrektur.\n\n\nMusterlösung\n# Mittelwerte der falschen Sensordaten: 0 fliesst in die Berechnung\n# ein und verfälscht den Mittelwert\nmean(sensor_fail$Temp)\n## [1] -0.13\n\n# Mittelwerte der korrigierten Sensordaten: mit na.rm = TRUE werden\n# NA-Werte aus der Berechnung entfernt.\nmean(sensor_fail$Temp_correct, na.rm = TRUE)\n## [1] -0.1857143",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Übung B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html",
    "href": "prepro/Prepro3_Demo.html",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "Split-Apply-Combine\nIn dieser Demo möchten wir weitere Werkzeuge aus dem Tidyverse vorstellen und mit Beispielen illustrieren. Die tidyverse-Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem “must have” im Umgang mit Daten in R entwickelt.\nWir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf weitere wichtige Komponenten und zeigen zusätzliche Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch Wickham und Grolemund (2017) beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online1 , das vollständige eBook kann über die Bibliothek bezogen werden2.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "Daten laden\nWir laden die Wetterdaten (Quelle MeteoSchweiz) der letzten Übung.\n\nlibrary(\"readr\")\n\nwetter &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\n\nwetter$stn &lt;- as.factor(wetter$stn)\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\")\n\n\n\nKennwerte berechnen\nWir möchten den Mittelwert aller gemessenen Temperaturwerten berechnen. Dazu könnten wir folgenden Befehl verwenden:\n\nmean(wetter$tre200h0, na.rm = TRUE)\n## [1] 6.324744\n\nDie Option na.rm = T bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen.\nMit derselben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (max()), Minimum (min()), Median (median()) u.v.m.).\nDiese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über alle Beobachtungen für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur pro Monat berechnen wollen.\n\n\nConvenience Variablen\nUm diese Aufgabe zu lösen, muss zuerst der Monat extrahiert werden (der Monat ist die convenience variable). Hierfür brauchen wir die Funktion lubridate::month().\nNun kann kann die convenience Variable “Month” erstellt werden. Ohne dpylr wird eine neue Spalte folgendermassen hinzugefügt.\n\nlibrary(\"lubridate\")\n\nwetter$month &lt;- month(wetter$time)\n\nMit dplyr (siehe 3) sieht der gleiche Befehl folgendermassen aus:\n\nlibrary(\"dplyr\")\n\nwetter &lt;- mutate(wetter, month = month(time))\n\nDer grosse Vorteil von dplyr ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.\n\n\nKennwerte nach Gruppen berechnen\nUm mit base R den Mittelwert pro Monat zu berechnen, kann man zuerst ein Subset mit [] erstellen und davon den Mittelwert berechnen, z.B. folgendermassen:\n\nmean(wetter$tre200h0[wetter$month == 1], na.rm = TRUE)\n## [1] -1.963239\n\nDies müssen wir pro Monat wiederholen, was natürlich sehr umständlich ist. Deshalb nutzen wir das package dplyr. Damit geht die Aufgabe (Temperaturmittel pro Monat berechnen) folgendermassen:\n\nsummarise(group_by(wetter, month), temp_mittel = mean(tre200h0, na.rm = TRUE))\n## # A tibble: 12 × 2\n##    month temp_mittel\n##    &lt;dbl&gt;       &lt;dbl&gt;\n##  1     1      -1.96 \n##  2     2       0.355\n##  3     3       2.97 \n##  4     4       4.20 \n##  5     5      11.0  \n##  6     6      12.4  \n##  7     7      13.0  \n##  8     8      15.0  \n##  9     9       9.49 \n## 10    10       8.79 \n## 11    11       1.21 \n## 12    12      -0.898\n\n\n\nVerketten vs. verschachteln\nAuf Deutsch übersetzt heisst die obige Operation folgendermassen:\n\nnimm den Datensatz wetter\nBilde Gruppen pro Jahr (group_by(wetter,year))\nBerechne das Temperaturmittel (mean(tre200h0))\n\nDiese Übersetzung R -&gt; Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch verkettet ausgesprochen wird (Operation 1-&gt;2-&gt;3) während der Computer verschachtelt liest 3(2(1)). Um R näher an die gesprochene Sprache zu bringen, kann man den |&gt;-Operator verwenden (siehe 4).\n\n# 1 nimm den Datensatz \"wetter\"\n# 2 Bilde Gruppen pro Monat\n# 3 berechne das Temperaturmittel\n\nsummarise(group_by(wetter, month), temp_mittel = mean(tre200h0))\n#                  \\_1_/\n#         \\__________2_________/\n# \\__________________3_______________________________________/\n\n# wird zu:\n\nwetter |&gt;                                 # 1\n  group_by(month) |&gt;                      # 2\n  summarise(temp_mittel = mean(tre200h0)) # 3\n\nDieses Verketten mittels |&gt; (genannt “pipe”) macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Die “pipe” wird mit dem package magrittr bereitgestellt und mit dplyr mitinstalliert.\nZu dplyr gibt es etliche Tutorials online (siehe5), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in dpylr: mutate() und summarise().\n\nsummarise() fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der “summarise” Funktion spezifiziert wurde (z.B. temp_mittel).\nmit mutate wird ein data.frame vom Umfang her belassen, es werden lediglich zusätzliche Variablen (Spalten) hinzugefügt (siehe Beispiel unten).\n\n\n# Maximal und minimal Temperatur pro Kalenderwoche\nweather_summary &lt;- wetter |&gt;                # 1) nimm den Datensatz \"wetter\"\n  filter(month == 1) |&gt;                     # 2) filter auf den Monat Januar\n  mutate(day = day(time)) |&gt;                # 3) erstelle eine neue Spalte \"day\"\n  group_by(day) |&gt;                          # 4) Nutze die neue Spalte um Gruppen zu bilden\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), # 5) Berechne das Maximum\n    temp_min = min(tre200h0, na.rm = TRUE)  # 6) Berechne das Minimum\n  )\n\nweather_summary\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#reshaping-data",
    "href": "prepro/Prepro3_Demo.html#reshaping-data",
    "title": "Prepro 3: Demo",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nBreit → lang\nDie Umformung von Tabellen breit→lang erfolgt mittels tidyr(siehe 6). Auch dieses Package funktioniert wunderbar mit piping (|&gt;).\n\nlibrary(\"tidyr\")\nweather_summary |&gt;\n  pivot_longer(c(temp_max, temp_min))\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nIm Befehl pivot_longer() müssen wir festlegen, welche Spalten zusammengefasst werden sollen (hier: temp_max,temp_min,temp_mean). Alternativ können wir angeben, welche Spalten wir nicht zusammenfassen wollen:\n\nweather_summary |&gt;\n  pivot_longer(-day)\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nWenn wir die Namen neuen Spalten festlegen wollen (anstelle von name und value) erreichen wir dies mit names_to bzw. values_to:\n\nweather_summary_long &lt;- weather_summary |&gt;\n  pivot_longer(-day, names_to = \"Messtyp\", values_to = \"Messwert\")\n\nDie ersten 6 Zeilen von weather_summary_long:\n\n\n\n\n\nday\nMesstyp\nMesswert\n\n\n\n\n1\ntemp_max\n5.8\n\n\n1\ntemp_min\n-4.4\n\n\n2\ntemp_max\n2.8\n\n\n2\ntemp_min\n-4.3\n\n\n3\ntemp_max\n4.2\n\n\n3\ntemp_min\n-3.1\n\n\n\n\n\nDie ersten 6 Zeilen von wetter_sry:\n\n\n\n\n\nday\ntemp_max\ntemp_min\n\n\n\n\n1\n5.8\n-4.4\n\n\n2\n2.8\n-4.3\n\n\n3\n4.2\n-3.1\n\n\n4\n4.7\n-2.8\n\n\n5\n11.4\n-0.6\n\n\n6\n6.7\n-1.6\n\n\n\n\n\nBeachte: weather_summary_long umfasst 62 Beobachtungen (Zeilen), das sind doppelt soviel wie weather_summary, da wir ja zwei Spalten zusammengefasst haben.\n\nnrow(weather_summary)\n## [1] 31\nnrow(weather_summary_long)\n## [1] 62\n\nLange Tabellen sind in verschiedenen Situationen praktischer. Beispielsweise ist das Visualisieren mittels ggplot2 (dieses Package werdet ihr im Block “InfoVis” kennenlernen) mit long tables wesentlich einfacher.\n\n\nlibrary(\"ggplot2\")\nggplot(weather_summary_long, aes(day, Messwert, colour = Messtyp)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\nLang → breit\nDas Gegenstück zu pivot_longer ist pivot_wider. Mit dieser Funktion können wir eine lange Tabelle in eine breite überführen. Dazu müssen wir in names_from angeben, aus welcher Spalte die neuen Spaltennamen erstellt werden sollen (names_from) und aus welcher Spalte die Werte entstammen sollen (values_from):\n\nweather_summary_long |&gt;\n  pivot_wider(names_from = Messtyp, values_from = Messwert)\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows\n\nZum Vergleich: mit einer wide table müssen wir in ggplot2 jede Spalte einzeln plotten. Dies ist bei wenigen Variabeln wie hier noch nicht problematisch, aber bei einer hohen Anzahl wird dies schnell mühsam.\n\nggplot(weather_summary) +\n  geom_line(aes(day, temp_max)) +\n  geom_line(aes(day, temp_min))\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, und Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#footnotes",
    "href": "prepro/Prepro3_Demo.html#footnotes",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "http://r4ds.had.co.nz/↩︎\nhttps://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093↩︎\nWickham und Grolemund (2017), Kapitel 10 / http://r4ds.had.co.nz/transform.html↩︎\nWickham und Grolemund (2017), Kapitel 14 / http://r4ds.had.co.nz/pipes.html↩︎\nWickham und Grolemund (2017), Kapitel 10 / http://r4ds.had.co.nz/transform.html, oder Hands-on dplyr tutorial..↩︎\nhttps://r4ds.had.co.nz/tidy-data.html#pivoting↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html",
    "href": "prepro/Prepro3_Uebung.html",
    "title": "Prepro 3: Übung",
    "section": "",
    "text": "Aufgabe 1\nGegeben sei ein Datensatz sensors_combined.csv, mit den Temperaturwerten von drei verschiedenen Sensoren. Importiere ihn als csv in R (als sensors_combined).\nFormatiere die Datetime Spalte in POSIXct um. Verwende dazu die Funktion as.POSIXct (lies mit ?strftime() nochmal nach, wie du das spezifische Format (die “Schablone”) festlegen kannst.\nMusterlösung\nlibrary(\"readr\")\n\nsensors_combined &lt;- read_delim(\"datasets/prepro/sensors_combined.csv\", \",\")\n\nsensors_combined$Datetime &lt;- as.POSIXct(sensors_combined$Datetime, format = \"%d%m%Y_%H%M\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nÜberführe die Tabelle in ein langes Format (verwende dazu die Funktion pivot_longer aus tidyr) und speichere den output als sensors_long.\nTipp:\n\nim Argument cols kannst du entweder die Spalten auflisten, die “pivotiert” werden sollen.\nAlternativ kannst du (mit vorangestelltem Minuszeichen, -) die Spalte bezeichnen, die nicht pivotiert werden soll.\nIn beiden Fällen musst du die Spalten weder mit Anführungs- und Schlusszeichen noch mit dem $-Zeichen versehen.\n\n\n\nMusterlösung\nlibrary(\"tidyr\")\n\n# Variante 1 (Spalten abwählen)\nsensors_long &lt;- pivot_longer(sensors_combined, -Datetime) \n\n# Variante 2 (Spalten anwählen)\nsensors_long &lt;- pivot_longer(sensors_combined, c(sensor1:sensor3))",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nGruppiere sensors_long nach der neuen Spalte, wo die Sensor-Information enthalten ist (default: name) mit group_by und berechne den Mittelwert der Temperatur pro Sensor (summarise). Hinweis: Beide Funktionen sind Teil des Packages dplyr.\nDer Output sieht folgendermassen aus:\n\n\nMusterlösung\nlibrary(\"dplyr\")\n\nsensors_long |&gt;\n  group_by(name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 3 × 2\n##   name    temp_mean\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 sensor1      14.7\n## 2 sensor2      12.0\n## 3 sensor3      14.4",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nErstelle für sensors_long eine neue convenience Variabel month, welche den Monat beinhaltet (Tipp: verwende dazu die Funktion month aus lubridate). Gruppiere nun nach month und Sensor und berechne den Mittelwert der Temperatur.\n\n\nMusterlösung\nlibrary(\"lubridate\")\n\nsensors_long |&gt;\n  mutate(month = month(Datetime)) |&gt;\n  group_by(month, name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 6 × 3\n## # Groups:   month [2]\n##   month name    temp_mean\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n## 1    10 sensor1     14.7 \n## 2    10 sensor2     12.7 \n## 3    10 sensor3     14.4 \n## 4    11 sensor1    NaN   \n## 5    11 sensor2      8.87\n## 6    11 sensor3    NaN",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nLade jetzt nochmal den Datensatz weather.csv (Quelle MeteoSchweiz) herunter und importiere ihn als CSV mit den korrekten Spaltentypen (stn als factor, time als POSIXct, tre200h0 als double).\n\n\nMusterlösung\nweather &lt;- read_delim(\"datasets/prepro/weather.csv\")\n\nweather$stn &lt;- as.factor(weather$stn)\n\nweather$time &lt;- as.POSIXct(as.character(weather$time), format = \"%Y%m%d%H\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nErstelle nun eine convenience Variable für die Kalenderwoche pro Messung (lubridate::week). Berechne im Anschluss den Mittelwert der Temperatur pro Kalenderwoche.\n\n\nMusterlösung\nweather_summary &lt;- weather |&gt;\n  mutate(week = week(time)) |&gt;\n  group_by(week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\nVisualisiere im Anschluss das Resultat:\n\nMusterlösung\nplot(weather_summary$week, weather_summary$temp_mean, type = \"l\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nIn der vorherigen Aufgabe haben wir den Mittelwert der Temperatur pro Kalenderwoche über alle Jahre (2000 und 2001) berechnet. Wenn wir die Jahre aber miteinander vergleichen wollen, müssen wir das Jahr als zusätzliche convenience Variable erstellen und danach gruppieren. Versuche dies mit den Wetterdaten und visualisiere den Output anschliessend.\n\n\nMusterlösung\nweather_summary2 &lt;- weather |&gt;\n  mutate(\n    week = week(time),\n    year = year(time)\n    ) |&gt;\n  group_by(year, week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\n\n\nMusterlösung\nplot(weather_summary2$week, weather_summary2$temp_mean, type = \"l\")\n\n\n\n\n\n\n\n\nAbbildung 8.1: baseplot mag keine long tables und macht aus den beiden Jahren eine kontinuierliche Linie",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nÜberführe den Output aus der letzten Übung in eine wide table. Nun lassen sich die beiden Jahre viel besser miteinander vergleichen.\n\n\nMusterlösung\nweather_summary2 &lt;- weather_summary2 |&gt;\n  pivot_wider(names_from = year, values_from = temp_mean,names_prefix = \"year\")\n\n\n\n\nMusterlösung\nplot(weather_summary2$week, weather_summary2$year2000, type = \"l\",col = \"blue\")\nlines(weather_summary2$week, weather_summary2$year2001, type = \"l\",col = \"red\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Übung</span>"
    ]
  },
  {
    "objectID": "InfoVis.html",
    "href": "InfoVis.html",
    "title": "InfoVis",
    "section": "",
    "text": "Infovis 1\nDie konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.",
    "crumbs": [
      "InfoVis"
    ]
  },
  {
    "objectID": "InfoVis.html#infovis-2",
    "href": "InfoVis.html#infovis-2",
    "title": "InfoVis",
    "section": "Infovis 2",
    "text": "Infovis 2\nDie Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser lesson eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.",
    "crumbs": [
      "InfoVis"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Vorbereitung.html",
    "href": "infovis/Infovis1_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von InfoVis 1 - 2 werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog Vorbereitung könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\n\npackages &lt;- c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \n  \"scales\", \"tidyr\")\n\nsapply(packages,\\(x) pacman::p_install(x, force = FALSE, character.only = TRUE))\n\nZudem könnt ihr die Daten für die Übungen auf Moodle herunterladen.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html",
    "href": "infovis/Infovis1_Demo.html",
    "title": "Infovis 1: Demo A",
    "section": "",
    "text": "Ggplot2\nAls erstes laden wir den Datensatz temperature_SHA_ZER.csv ein. Es handelt sich dabei um eine leicht modifizierte Variante der Daten aus PrePro1 und PrePro2.\nIn ggplot wird durch den Befehl ggplot() initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (data =), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (mapping = aes()).\n# Datensatz: \"temperature\" | Beeinflussende Variabeln: \"time\" und \"temp\"\nggplot(data = temperature, mapping = aes(time, SHA))\nWeiter braucht es mindestens ein “Layer”, der beschreibt, wie die Daten dargestellt werden sollen (z.B. geom_point()). Anders als bei “Piping” (|&gt;) wird ein Layer mit + hinzugefügt.\nggplot(data = temperature, mapping = aes(time, SHA)) +\n  # Layer: \"geom_point\" entspricht Punkten in einem Scatterplot\n  geom_point()\nDa ggplot die Eingaben in der Reihenfolge data = und dann mapping = erwartet, können wir diese Spezifizierungen auch weglassen.\nggplot(temperature, aes(time, SHA)) +\n  geom_point()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nWie wir in PrePro 2 bereits erwähnt haben, ist ggplot2 auf long tables ausgelegt. Wir überführen deshalb an dieser Stelle die breite in eine lange Tabelle:\n\ntemperature_long &lt;- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nNun wollen wir die Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in aes().\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWir können noch einen Layer mit Linien hinzufügen:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point() +\n  geom_line()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "href": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "title": "Infovis 1: Demo A",
    "section": "Beschriftungen (labels)",
    "text": "Beschriftungen (labels)\nWeiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (geom_point()) nun weg, da mir diese nicht gefallen.\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  )",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIm obigen Plot fällt auf, dass stündliche Werte eine zu hohe Auflösung haben, wenn wir die Daten über 2 Jahre visualisieren. Mit Split Apply Combine (PrePro 3) können wir die Auflösung unserer Daten verändern:\n\ntemperature_day &lt;- temperature_long |&gt;\n  mutate(time = as.Date(time))\n\ntemperature_day\n## # A tibble: 35,088 × 3\n##    time       station  temp\n##    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n##  1 2000-01-01 SHA       0.2\n##  2 2000-01-01 ZER      -8.8\n##  3 2000-01-01 SHA       0.3\n##  4 2000-01-01 ZER      -8.7\n##  5 2000-01-01 SHA       0.3\n##  6 2000-01-01 ZER      -9  \n##  7 2000-01-01 SHA       0.3\n##  8 2000-01-01 ZER      -8.7\n##  9 2000-01-01 SHA       0.4\n## 10 2000-01-01 ZER      -8.5\n## # ℹ 35,078 more rows\n\ntemperature_day &lt;- temperature_day |&gt;\n  group_by(station, time) |&gt;\n  summarise(temp = mean(temp))\n\ntemperature_day\n## # A tibble: 1,462 × 3\n## # Groups:   station [2]\n##    station time        temp\n##    &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;\n##  1 SHA     2000-01-01  1.25\n##  2 SHA     2000-01-02  1.73\n##  3 SHA     2000-01-03  1.59\n##  4 SHA     2000-01-04  1.78\n##  5 SHA     2000-01-05  4.66\n##  6 SHA     2000-01-06  3.49\n##  7 SHA     2000-01-07  3.87\n##  8 SHA     2000-01-08  3.28\n##  9 SHA     2000-01-09  3.24\n## 10 SHA     2000-01-10  3.24\n## # ℹ 1,452 more rows",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "href": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "title": "Infovis 1: Demo A",
    "section": "X/Y-Achse anpassen",
    "text": "X/Y-Achse anpassen\nMan kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat ggplot eine Annahme auf der Basis der Daten getroffen).\nBei unserer y-Achse handelt es sich um numerische Daten, ggplot nennt diese: scale_y_continuous(). Unter ggplot2.tidyverse.org findet man noch andere x/y-Achsentypen (scale_x_irgenwas bzw. scale_y_irgendwas).\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) # y-Achsenabschnitt bestimmen\n\n\n\n\n\n\n\n\nDas gleiche Spiel kann man für die x-Achse betreiben. Bei unserer x-Achse handelt es sich ja um Datumsangaben. ggplot nennt diese: scale_x_date().\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  )",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nSehr praktisch sind auch die Funktionen für “Small multiples”. Dies erreicht man mit facet_wrap() (oder facet_grid(), mehr dazu später). Man muss mit einem Tilde-Symbol “~” nur festlegen, welche Variable für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll.\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station)\n\n\n\n\n\n\n\n\nAuch facet_wrap kann man auf seine Bedürfnisse anpassen: Beispielweise kann man mit ncol = die Anzahl facets pro Zeile bestimmen.\nZudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb theme(legend.position=\"none\")\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#plot-exportieren",
    "href": "infovis/Infovis1_Demo.html#plot-exportieren",
    "title": "Infovis 1: Demo A",
    "section": "Plot exportieren",
    "text": "Plot exportieren\nFolgendermassen kann ich den letzten Plot als png-File abspeichern:\n\nggsave(filename = \"plot.png\")",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html",
    "href": "infovis/Infovis1_Uebung.html",
    "title": "Infovis 1: Übung",
    "section": "",
    "text": "Aufgabe 1\nIn dieser Übung geht es darum, die Grafiken aus dem Blog-post Kovic (2014) zu rekonstruieren. Der urspüngliche Blogpost ist nicht mehr verfügbar, wir haben deshalb eine Kopie auf folgender Website gehostet:\nhttps://researchmethods-zhaw.github.io/blog.tagesanzeiger.ch/\nSchau dir die Grafiken in dem Blogpost durch. Freundlicherweise wurden im Blogbeitrag die ggplot2 Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen.\nImportiere als erstes den Datensatz tagi_data_kanton.csv.\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_kanton.csv Datensatz:\nTipp:\nMusterlösung\n# Lösung zu Aufgabe 1\n\nlibrary(\"ggplot2\")\nplot1 &lt;- ggplot(kanton, aes(auslanderanteil, ja_anteil)) +\n  geom_point() +\n  coord_fixed(1) +\n  scale_y_continuous(breaks = c(0, 0.1, 0.3, 0.5, 0.7), limits = c(0, 0.7)) +\n  scale_x_continuous(breaks = c(0, 0.1, 0.3, 0.5, 0.7), limits = c(0, 0.7)) +\n  labs(y = \"Anteil Ja-Stimmen\", x = \"Ausländeranteil\")\n\nplot1",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "title": "Infovis 1: Übung",
    "section": "",
    "text": "Nutze ggplot(kanton, aes(auslanderanteil, ja_anteil)), um den ggplot zu initiieren. Füge danach einen Punkte-Layer hinzu (geom_point())\nNutze coord_fixed(), um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).\nOptional:\n\nSetze die Achsen Start- und Endwerte mittels scale_y_continuous bzw. scale_x_continuous.\nSetze analog Kovic (2014) die breaks (0.0, 0.1…0.7) manuell (innerhalb scale_*_continuous)\nNutze labs() für die Beschriftung der Achsen",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot:\nTipp:\n\nNutze geom_smooth\n\n\n\nMusterlösung\n# Lösung zu Aufgabe 2\n\nplot1 +\n  geom_smooth()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Gemeindedaten tagi_data_gemeinden.csv.\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_point()\nNutze labs()\nNutze coord_fixed()\n\n\n\nMusterlösung\n# Lösung zu Aufgabe 3\n\ngemeinde &lt;- read_delim(\"datasets/infovis/tagi_data_gemeinden.csv\", \",\")\n\nplot2 &lt;- ggplot(gemeinde, aes(anteil_ausl, anteil_ja)) +\n  geom_point() +\n  labs(x = \"Ausländeranteil\", y = \"Anteil Ja-Stimmen\") +\n  coord_fixed(1) +\n  lims(x = c(0, 1), y = c(0, 1))\n\nplot2",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\nMusterlösung\n# Lösung zu Aufgabe 4\n\nplot2 +\n  geom_smooth()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap um einen Plot pro Kanton darzustellen.\n\n\n\nMusterlösung\n# Lösung zu Aufgabe 5\n\nplot3 &lt;- plot2 +\n  facet_wrap(~kanton)\nplot3",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\nMusterlösung\n# Lösung zu Aufgabe 6\n\nplot3 +\n  geom_smooth()",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap\n\n\n\nMusterlösung\n# Lösung zu Aufgabe 7\n\nplot4 &lt;- plot2 +\n  facet_wrap(~quantile)\nplot4",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nRekonstruiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\nMusterlösung\n# Lösung zu Aufgabe 8\n\nplot4 +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. „Je weniger Ausländer, desto mehr Ja-Stimmen? Wirklich?“ Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Übung</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: Script EDA",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"scales\")\n\n# create some data about age and height of people\npeople &lt;- data.frame(\n  ID = c(1:30),\n  age = c(\n    5.0, 7.0, 6.5, 9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,\n    63.5, 65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,\n    15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3\n  ),\n  height = c(\n    0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,\n    1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,\n    1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80\n  ),\n  weight = c(\n    45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,\n    84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,\n    48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3\n  )\n)\n\n# build a scatterplot for a first inspection\nggplot(people, aes(x = age, y = height)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -&gt; Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 25593\n##   10 | 037\n##   12 | 523\n##   14 | 19556\n##   16 | 255789916\n##   18 | 04774\nstem(people$height, scale = 2)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 2559\n##    9 | 3\n##   10 | \n##   11 | 037\n##   12 | 5\n##   13 | 23\n##   14 | 19\n##   15 | 556\n##   16 | 2557899\n##   17 | 16\n##   18 | 0477\n##   19 | 4\n\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    5.00    8.70   30.20   59.14   65.15  512.30\nboxplot(people$age)\n\n\n\n\n\n\n\n\n\nsummary(people$height)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.820   1.190   1.555   1.455   1.690   1.940\nboxplot(people$height)\n\n\n\n\n\n\n\n\n\n# explore data with a histgram\nggplot(people, aes(x = age)) +\n  geom_histogram(binwidth = 20)\n\n\n\n\n\n\n\n\n\ndensity(x = people$height)\n## \n## Call:\n##  density.default(x = people$height)\n## \n## Data: people$height (30 obs.);   Bandwidth 'bw' = 0.1576\n## \n##        x                y           \n##  Min.   :0.3472   Min.   :0.001593  \n##  1st Qu.:0.8636   1st Qu.:0.102953  \n##  Median :1.3800   Median :0.510601  \n##  Mean   :1.3800   Mean   :0.483553  \n##  3rd Qu.:1.8964   3rd Qu.:0.722660  \n##  Max.   :2.4128   Max.   :1.216350\n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes\n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n\n# logarithmic axis: respond to skewness in the data, e.g. log10\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean &lt;- people |&gt;\n  filter(ID != 27) |&gt; # Diese Person war zu klein.\n  filter(age &lt; 100) # Fehler in der Erhebung des Alters\n\n\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10)\n\n\n\n\n\n\n\n\n\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10) +\n  theme_bw() # specifying the theme\n\n\n\n\n\n\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5) +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age &lt; 15) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# filter \"teenies\": No trend\npeopleClean |&gt;\n  filter(age &gt; 55) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[, 2:4], panel = panel.smooth)\n\n\n\n\n\n\n\n\n\npairs(peopleClean[, 2:4], panel = panel.smooth)",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Script EDA</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html",
    "href": "infovis/Infovis2_Uebung_A.html",
    "title": "Infovis 2: Übung A",
    "section": "",
    "text": "Aufgabe 1\nFür die heutige Übung brauchst du den Datensatz temperature_2005.csv. Dabei handelt es sich wieder um Teperaturwerte verschiedener Stationen, diesmal aus dem Jahr 2005. Das Datum ist so formatiert, dass R (isbesondere read_delim) es korrekt als datetime erkennen und als POSIXct einlesen sollte.\nGleichzeitig wollen wir euch heute Quarto näher bringen. Wir werden dafür Quarto im Unterricht demonstieren. Zudem kann man in Wickham, Çetinkaya-Rundel, und Grolemund (2023) (Kapitel Quarto) sowie auf der Quarto Webseite mehr über Quarto erfahren.\nMache aus der wide table eine long table, die wie folgt aussieht.\nMusterlösung\nlibrary(\"tidyr\")\ntemperature_long &lt;- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temperature\")\n\nknitr::kable(head(temperature_long))\n\n\n\n\n\ntime\nstation\ntemperature\n\n\n\n\n2005-01-01\nALT\n1.3\n\n\n2005-01-01\nBUS\n1.5\n\n\n2005-01-01\nGVE\n1.1\n\n\n2005-01-01\nINT\n0.2\n\n\n2005-01-01\nOTL\n2.2\n\n\n2005-01-01\nLUG\n1.7\nImportiere anschliessend den Datensatz temperature_2005_metadata.csv und verbinde die beiden Datensätze mit einem left_join via station (bzw. stn).\nMusterlösung\nmetadata &lt;- read_delim(\"datasets/infovis/temperature_2005_metadata.csv\", \",\")\n\nlibrary(\"dplyr\")\n\ntemperature_long &lt;- left_join(temperature_long, metadata, by = c(station = \"stn\"))",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle ein Scatterplot (time vs. temperature), wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot (scale_color_gradient). Verkleinere die Punkte, um übermässiges Überplotten der Punkten zu vermeiden (size =). Weiter sollen auf der x-Achse im Abstand von 3 Monaten der jeweilige Monat vermerkt sein (date_breaks bzw. date_labels von scale_x_datetime()).\n\n\nMusterlösung\n# Musterlösung\nlibrary(\"ggplot2\")\nggplot(temperature_long, aes(time, temperature, color = Meereshoehe)) +\n  geom_point(size = 0.5) +\n  labs(x = \"\", y = \"Temperatur in ° Celsius\") +\n  scale_x_datetime(date_breaks = \"3 months\", date_labels = \"%b\") +\n  scale_color_gradient(low = \"blue\", high = \"red\")",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nErstelle eine Zusatzvariabel Date mit dem Datum der jeweiligen Messung ( mit as.Date). Nutze diese Spalte, um die Tagesmitteltemperatur pro Station zu berechnen (mit summarise()).\nUm die Metadaten (Name, Meereshoehe, x, y) nicht zu verlieren, kannst du den Join aus der ersten Übung wieder ausführen. Alternativ (schneller aber auch schwerer zu verstehen) kannst du diese Variabeln innerhalb deines group_by verwenden.\n\n\nMusterlösung\ntemperature_long &lt;- temperature_long |&gt;\n  mutate(time = as.Date(time)) |&gt;\n  group_by(time, station, Name, Meereshoehe, x, y) |&gt;\n  summarise(temperature = mean(temperature))",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWiederhole nun den Plot aus der ersten Aufgabe mit den aggregierten Daten aus der vorherigen Aufgabe. Um die labels korrekt zu setzen, musst du scale_x_datetime mit scale_x_date ersetzen.\n\n\nMusterlösung\np &lt;- ggplot(temperature_long, aes(time, temperature, color = Meereshoehe)) +\n  geom_point(size = 0.5) +\n  labs(x = \"\", y = \"Temperatur in ° Celsius\") +\n  scale_x_date(date_breaks = \"3 months\", date_labels = \"%b\") +\n  scale_color_gradient(low = \"blue\", high = \"red\")\np",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nFüge am obigen Plot eine schwarze, gestrichelte Trendlinie hinzu.\n\n\nMusterlösung\n# Musterlösung\np &lt;- p +\n  stat_smooth(colour = \"black\", lty = 2)\np",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nPositioniere die Legende oberhalb des Plots (nutze dazu theme() mit legend.position).\n\n\nMusterlösung\n# Musterlösung\np &lt;- p +\n  theme(legend.position = \"top\")\np",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 7 (optional, fortgeschritten)",
    "text": "Aufgabe 7 (optional, fortgeschritten)\nFüge den Temperaturwerten auf der y-Ache ein °C hinzu (siehe unten und studiere diesen Tipp zur Hilfe).\n\n\nMusterlösung\n# Musterlösung\np &lt;- p +\n  scale_y_continuous(labels = function(x) paste0(x, \"°C\")) +\n  labs(x = \"Kalenderwoche\", y = \"Temperatur\")\np",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nJetzt verlassen wir den Scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein.\n\nBeachte den Unterschied zwischen colour = und fill =\nBeachte den Unterschied zwischen facet_wrap() und facet_grid()\nfacet_grid() braucht übrigens noch einen Punkt (.) zur Tilde (~).\nBeachte den Unterschied zwischen “.~” und “~.” bei facet_grid()\nverschiebe nach Bedarf die Legende\n\n\n\nMusterlösung\n# Musterlösung\nlibrary(\"lubridate\")\ntemperature_long &lt;- mutate(temperature_long, monat = month(time, label = T, abbr = F))\n\nggplot(temperature_long, aes(monat, temperature, fill = Meereshoehe)) +\n  geom_boxplot() +\n  labs(x = \"Station\", y = \"Temperatur\") +\n  facet_wrap(~station) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 9",
    "text": "Aufgabe 9\nAls letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm geom_histogram() mit den Temperaturwerten. Teile dazu die Stationen in verschiedene Höhenlagen ein (Tieflage [&lt; 400 m], Mittellage [400 - 600 m] und Hochlage [&gt; 600 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen mit einem Histogramm.\nTip: Nutze cut um die Stationen in die drei Gruppen aufzuteilen\n\n\nMusterlösung\n# Musterlösung\ntemperature_long &lt;- temperature_long |&gt;\n  mutate(lage = cut(Meereshoehe, c(0, 400, 600, 1000), labels = c(\"Tieflage\", \"Mittellage\", \"Hochlage\")))\n\nggplot(temperature_long, aes(temperature)) +\n  geom_histogram() +\n  facet_grid(~lage) +\n  labs(x = \"Lage\", y = \"Temperatur\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, und Garrett Grolemund. 2023. R for data science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 2: Übung A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html",
    "href": "infovis/Infovis2_Uebung_B.html",
    "title": "Infovis 2: Übung optional",
    "section": "",
    "text": "Aufgabe 1: Parallel coordinate plots\nIn dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man hier oder mit der Hilfe ?datasets.\nDazu verwenden wir nach wie vor ggplot2, aber mit einigen Tricks.\nErstelle einen parallel coordinate plot. Dafür eignet sich der integrierte Datensatz mtcars. Extrahiere die Fahrzeugnamen mit rownames_to_column.\nZudem müssen die Werte jeweiles auf eine gemeinsame Skala normalisiert werden. Hierfür kannst du die Funktion scales::rescale verwenden.\nMusterlösung\n\nlibrary(\"tidyr\")\nmtcars2 &lt;- mtcars |&gt;\n  tibble::rownames_to_column(\"car\") |&gt;\n  pivot_longer(-car)\n\nlibrary(\"dplyr\")\nmtcars2 &lt;- mtcars2 |&gt;\n  group_by(name) |&gt;\n  mutate(value_scaled = scales::rescale(value))\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\nSo sieht der fertige Plot aus:\nMusterlösung\nmtcars2 &lt;- mtcars2 |&gt;\n  group_by(car) |&gt;\n  mutate(gear = value[name == \"gear\"])\n\nlibrary(\"ggplot2\")\n\nggplot(mtcars2, aes(name, value_scaled, group = car, color = factor(gear))) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.title.y = element_blank())",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Infovis 2: Übung optional</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "title": "Infovis 2: Übung optional",
    "section": "Aufgabe 2: Polar Plot mit Biber Daten",
    "text": "Aufgabe 2: Polar Plot mit Biber Daten\nPolar Plots eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:\n\nbeaver1 und beaver2\nAirPassenger\n\nBeide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.\nWenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen.\n\n\nMusterlösung\nbeaver1_new &lt;- beaver1 |&gt;\n  mutate(beaver = \"nr1\")\n\nbeaver2_new &lt;- beaver2 |&gt;\n  mutate(beaver = \"nr2\")\n\nbeaver_new &lt;- rbind(beaver1_new, beaver2_new)\n\n\nZudem müssen wir die Zeitangabe noch anpassen: Gemäss der Datenbeschreibung handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als “0330” notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln.\n\n\nMusterlösung\nbeaver_new &lt;- beaver_new |&gt;\n  mutate(\n    hour_dec = (time / 100) %/% 1, # Ganze Stunden (mittels ganzzaliger Division)\n    min_dec = (time / 100) %% 1 / 0.6, # Dezimalminuten (15 min wird zu 0.25, via Modulo)\n    hour_min_dec = hour_dec + min_dec # Dezimal-Zeitangabe (03:30 wird zu 3.5)\n  )\n\n\nSo sieht der fertige Plot aus:\n\n\nMusterlösung\n# Lösung Aufgabe 2\n\nbeaver_new |&gt;\n  ggplot(aes(hour_min_dec, temp, color = beaver)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 23, 2)) +\n  coord_polar() +\n  theme_minimal() +\n  theme(axis.title = element_blank())",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Infovis 2: Übung optional</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "title": "Infovis 2: Übung optional",
    "section": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren",
    "text": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren\nAnalog Aufgabe 2, dieses Mal mit dem Datensatz AirPassengers\nAirPassengers kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein data.frame oder eine matrix, ist aber von der Klasse ts.\n\n\nMusterlösung\nAirPassengers\n##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 1949 112 118 132 129 121 135 148 148 136 119 104 118\n## 1950 115 126 141 135 125 149 170 170 158 133 114 140\n## 1951 145 150 178 163 172 178 199 199 184 162 146 166\n## 1952 171 180 193 181 183 218 230 242 209 191 172 194\n## 1953 196 196 236 235 229 243 264 272 237 211 180 201\n## 1954 204 188 235 227 234 264 302 293 259 229 203 229\n## 1955 242 233 267 269 270 315 364 347 312 274 237 278\n## 1956 284 277 317 313 318 374 413 405 355 306 271 306\n## 1957 315 301 356 348 355 422 465 467 404 347 305 336\n## 1958 340 318 362 348 363 435 491 505 404 359 310 337\n## 1959 360 342 406 396 420 472 548 559 463 407 362 405\n## 1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nclass(AirPassengers)\n## [1] \"ts\"\n\n\nDamit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine matrix umwandeln. Wie das geht habe ich hier erfahren.\n\n\nMusterlösung\nAirPassengers2 &lt;- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)\n\nAirPassengers2\n##       month\n## year   Apr Aug Dec Feb Jan Jul Jun Mar May Nov Oct Sep\n##   1949 129 148 118 118 112 148 135 132 121 104 119 136\n##   1950 135 170 140 126 115 170 149 141 125 114 133 158\n##   1951 163 199 166 150 145 199 178 178 172 146 162 184\n##   1952 181 242 194 180 171 230 218 193 183 172 191 209\n##   1953 235 272 201 196 196 264 243 236 229 180 211 237\n##   1954 227 293 229 188 204 302 264 235 234 203 229 259\n##   1955 269 347 278 233 242 364 315 267 270 237 274 312\n##   1956 313 405 306 277 284 413 374 317 318 271 306 355\n##   1957 348 467 336 301 315 465 422 356 355 305 347 404\n##   1958 348 505 337 318 340 491 435 362 363 310 359 404\n##   1959 396 559 405 342 360 548 472 406 420 362 407 463\n##   1960 461 606 432 391 417 622 535 419 472 390 461 508\n\n\nAus der matrix muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen.\n\n\nMusterlösung\nAirPassengers3 &lt;- AirPassengers2 |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"year\") |&gt;\n  pivot_longer(-year, names_to = \"month\", values_to = \"n\") |&gt;\n  mutate(\n    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln\n    month = factor(month, levels = month.abb, ordered = T),\n    month_numb = as.integer(month),\n    year = as.integer(year)\n  )\n\n\nSo sieht der fertige Plot aus:\n\n\nMusterlösung\nggplot(AirPassengers3, aes(month, year, fill = n)) +\n  geom_raster() +\n  scale_y_reverse() +\n  scale_fill_viridis_c(guide = guide_colourbar(barwidth = 15, title.position = \"top\")) +\n  theme_minimal() +\n  labs(fill = \"Anzahl Passagiere\") +\n  coord_equal() +\n  theme(axis.title = element_blank(), legend.position = \"bottom\")",
    "crumbs": [
      "InfoVis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Infovis 2: Übung optional</span>"
    ]
  },
  {
    "objectID": "Statistik.html",
    "href": "Statistik.html",
    "title": "Statistik",
    "section": "",
    "text": "Statistik 1\nIn Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-2",
    "href": "Statistik.html#statistik-2",
    "title": "Statistik",
    "section": "Statistik 2",
    "text": "Statistik 2\nIn Statistik 2 lernen die Studierenden die Voraussetzungen und die praktische Anwendung “einfacher” linearer Modelle in R (sowie teilweise ihrer “nicht-parametrischen” bzw. “robusten” Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-3",
    "href": "Statistik.html#statistik-3",
    "title": "Statistik",
    "section": "Statistik 3",
    "text": "Statistik 3\nIn Statistik 3 beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlauben, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Dann besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet. Abschliessend fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-4",
    "href": "Statistik.html#statistik-4",
    "title": "Statistik",
    "section": "Statistik 4",
    "text": "Statistik 4\nIn Statistik 4 geht es um multiple Regressionen, die versuchen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-5",
    "href": "Statistik.html#statistik-5",
    "title": "Statistik",
    "section": "Statistik 5",
    "text": "Statistik 5\nIn Statistik 5 geht es um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-6",
    "href": "Statistik.html#statistik-6",
    "title": "Statistik",
    "section": "Statistik 6",
    "text": "Statistik 6\nIn Statistik 6 lernen die Studierenden Lösungen kennen, welche die diversen Limitierungen von linearen Modellen überwinden. Während generalized linear models (GLMs) aus Statistik 4 bekannt sind, geht es jetzt um linear mixed effect models (LMMs) und generalized linear mixed effect models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den Beobachtungen. Einfachere Fälle von LMMs, wie split-plot und repeated-measures ANOVAs, lassen sich noch mit dem aov-Befehl in Base R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen Abhängigkeiten, erlauben.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-7",
    "href": "Statistik.html#statistik-7",
    "title": "Statistik",
    "section": "Statistik 7",
    "text": "Statistik 7\nStatistik 7 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit mehreren abhängigen und mehrenen unabhängigen Variablen zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren. Ordinationen dienen dazu, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion zu visualisieren. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Neben der Beschreibung der Datenstruktur in komplexen Datensätzen kann eine PCA auch dazu dienen, aus diesen unabhängie Variablen zu generieren, die anschliessend in einer multiplen Regression als Prädiktoren genutzt werden können.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "Statistik.html#statistik-8",
    "href": "Statistik.html#statistik-8",
    "title": "Statistik",
    "section": "Statistik 8",
    "text": "Statistik 8\nIn Statistik 8 lernen die Studierenden Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre Technik der deskriptiven Statistik multivariater Datensätze kennen. Es gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer gehen wir auf die k-means Clusteranalyse (eine Partitionierung) ein.\nIm Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "statistik/Statistik0_Vorbereitung.html",
    "href": "statistik/Statistik0_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von Statistik 1 - 8 werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog Vorbereitung könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\n\npackages &lt;- c(\"agricolae\", \"car\", \"corrplot\", \"DHARMa\", \"doBy\", \"factoextra\",\n\"FSA\", \"glmmTMB\", \"lme4\", \"MASS\", \"MuMIn\", \"performance\",\n\"performance\", \"relaimpo\", \"sjPlot\", \"tidyverse\", \"vegan\")\n\n\nsapply(packages,\\(x) pacman::p_install(x, force = FALSE, character.only = TRUE))",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html",
    "href": "statistik/Statistik1_Demo.html",
    "title": "Statistik 1: Demo",
    "section": "",
    "text": "t-Test\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html#t-test",
    "href": "statistik/Statistik1_Demo.html#t-test",
    "title": "Statistik 1: Demo",
    "section": "",
    "text": "Daten generieren und anschauen\n\n# Je 10 Messwerte für Sorte a und b zu einem Data Frame im long-Format verbinden \nMesswerte_a &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14) # Messwerte von Cultivar a\nMesswerte_b &lt;- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10) # Messwerte von Cultivar b\ncultivar &lt;- as.factor( c(rep(\"a\", 10), rep(\"b\", 10))) # Bezeichnug der Cultivare in der Tabelle\nblume &lt;- data.frame(\"cultivar\" = cultivar, \"size\" = c(Messwerte_a, Messwerte_b)) # Data frame erstellen \n\n\n# Boxplots\n\nlibrary(ggplot2)\n\nggplot(blume, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot() + # Boxplots\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) # Datenpunkte darstellen\n\n\n\n\n\n\n\n\n\n# Histogramme\n\nggplot(blume, aes(x = size, fill = cultivar)) +\n  geom_histogram(binwidth = 2) +\n  facet_wrap(~ cultivar)\n\n\n\n\n\n\n\n\n\n\nZweiseitiger t-Test\n\n# Links der Tilde (\"~\") steht immer die abhängige Variable, rechsts die erklärende(n) Variable(n)\n# Alternativ kann man die Werte auch direkt in die t.test()-Funktion eigeben:\n# t.test(Messwerte_a, Messwerte_b)\n\nt.test(size ~ cultivar, data = blume) # Zweiseitig \"Test auf a ≠ b\" (default)\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n\n\n\nEinseitiger t-Test\n\nt.test(size ~ cultivar, alternative = \"greater\",  data = blume) # Einseitig \"Test auf a &gt; b\"\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.02827\nalternative hypothesis: true difference in means between group a and group b is greater than 0\n95 percent confidence interval:\n 0.5954947       Inf\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\nt.test(size ~ cultivar, alternative = \"less\",  data = blume) # Einseitig \"Test auf a &lt; b\"\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.9717\nalternative hypothesis: true difference in means between group a and group b is less than 0\n95 percent confidence interval:\n     -Inf 7.204505\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n\n\n\nKlassischer t-Test vs. Welch Test\n\n# Varianzen gleich: klassischer t-Test\nt.test(size ~ cultivar, var.equal = TRUE, data = blume)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n# Varianzen ungleich: Welch's t-Test (siehe Titelzeile des R-Outputs!)\nt.test(size ~ cultivar, data = blume) # dasselbe wie var.equal = FALSE\n\n\n    Welch Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n\n\n\nGepaarter t-Test\n\n# Gepaarter t-Test: erster Wert von a wird mit erstem Wert von\n# b gepaart, zweiter Wert von a mit zweitem von b ect.\n\nt.test(Messwerte_a, Messwerte_b, paired = TRUE) # für gepaarten t-Test funktioniert Notation \"size ~ cultivar\" nicht\n\n\n    Paired t-test\n\ndata:  Messwerte_a and Messwerte_b\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.366339 6.433661\nsample estimates:\nmean difference \n            3.9",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html#binomialtest",
    "href": "statistik/Statistik1_Demo.html#binomialtest",
    "title": "Statistik 1: Demo",
    "section": "Binomialtest",
    "text": "Binomialtest\nIn Klammern übergibt man die Anzahl der Erfolge und die Stichprobengrösse\n\nbinom.test(84, 200) # Anzahl Frauen im Nationalrat (≙ 42.0 %; Stand 2019)\n\n\n    Exact binomial test\n\ndata:  84 and 200\nnumber of successes = 84, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.3507439 0.4916638\nsample estimates:\nprobability of success \n                  0.42 \n\nbinom.test(116, 200) # Anzahl Männer im Nationalrat (≙ 58.0 %; Stand 2019)\n\n\n    Exact binomial test\n\ndata:  116 and 200\nnumber of successes = 116, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5083362 0.6492561\nsample estimates:\nprobability of success \n                  0.58 \n\nbinom.test(3, 7) # Anzahl Frauen im Bundesrat (≙ 42.9 %; Stand 2019)\n\n\n    Exact binomial test\n\ndata:  3 and 7\nnumber of successes = 3, number of trials = 7, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.09898828 0.81594843\nsample estimates:\nprobability of success \n             0.4285714",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "href": "statistik/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "title": "Statistik 1: Demo",
    "section": "Chi-Quadrat-Test & Fishers Test",
    "text": "Chi-Quadrat-Test & Fishers Test\nErmitteln des kritischen Wertes\n\nDirekter Test in R (dazu Werte als Matrix nötig)\n\n# Matrix mit Haarfarbe&Augenfarbe-Kombinationen erstellen\n# 38 blond&blau, 14 dunkel&blau, 11 blond&braun, 51 dunkel&braun\ncount &lt;- matrix(c(38, 14, 11, 51), nrow = 2)\ncount # Check\n\n     [,1] [,2]\n[1,]   38   11\n[2,]   14   51\n\nrownames(count) &lt;- c(\"blond\", \"dunkel\") # Benennen für Übersicht\ncolnames(count) &lt;- c(\"blau\", \"braun\") #  Benennen für Übersicht\ncount # Check\n\n       blau braun\nblond    38    11\ndunkel   14    51\n\n# Tests durchführen\nchisq.test(count)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  count\nX-squared = 33.112, df = 1, p-value = 8.7e-09\n\nfisher.test(count)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  count\np-value = 2.099e-09\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  4.746351 34.118920\nsample estimates:\nodds ratio \n  12.22697",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistik 1: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html",
    "href": "statistik/Statistik1_Uebung.html",
    "title": "Statistik 1: Übung",
    "section": "",
    "text": "Anleitung zu den Übungen\nZu erstellen sind:\nWichtige Hinweise zur Ausgestaltung:",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html#aufgabe-1.1-assoziationstest",
    "href": "statistik/Statistik1_Uebung.html#aufgabe-1.1-assoziationstest",
    "title": "Statistik 1: Übung",
    "section": "Aufgabe 1.1: Assoziationstest",
    "text": "Aufgabe 1.1: Assoziationstest\nFührt einen Assoziationstest zweier kategorialer Variablen (mit je zwei Ausprägungen) durch. Dazu erhebt ihr selbst die Daten (wozu ihr euch auch in Teams zusammenschliessen könnt). Ihr könnt z.B. eine Datenerhebung unter Mitstudierenden durchführen (etwa Nutzung Mac/Windows vs. männlich/weiblich). Bitte formuliert vor der Datenerhebung eine Hypothese, d.h. eine Erwartungshaltung, ob und welche Assoziation vorliegt und wenn ja warum. Beachtet, dass ihr für diese Form des Assoziationstests genau zwei binäre Variablen benötigt. Wenn ihr also kategoriale Variablen mit mehr als zwei Ausprägungen habt, so könnt ihr entweder Ausprägungen sinnvoll zusammenfassen oder seltene Ausprägungen im Test unberücksichtigt lassen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "href": "statistik/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "title": "Statistik 1: Übung",
    "section": "Aufgabe 1.2: t-Test",
    "text": "Aufgabe 1.2: t-Test\n**T-Test mit Datensatz *Novanimal_Weeks.csv**\nIm Forschungsprojekt NOVANIMAL wurde u.a. in der ZHAW-Mensa untersucht, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren. Dazu wurden in sogenannten Interventionswochen mehr vegane und vegetarische Gerichte (zu Lasten kleinerer Fleischgerichtsauswahl) angeboten. Eine erste grundsätzliche Frage war, ob die unterschiedliche Gerichtsauswahl zu unterschiedlichen Verkaufszahlen führten. Beantwortet dazu mit dem Datensatz und geeigneten Tests folgende Frage:\nWurden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?\n\nSchau die Daten an: Verstehen und ggf. plotten.\nDefiniere die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)).\nWelche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart?\nFühre einen t-Test durch.\nWie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)?\nStelle deine Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle\n\n\n\n\n\n\n\nStatistik 1: Lösung\n\n\n\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n\nLösungstext als Download\n\n\nVorebreitung R-Session\n\n# lade Packages\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\n\n## definiert mytheme für ggplot2 (verwendet dabei theme_classic())\nmytheme &lt;-\n  theme_classic() +\n  theme(\n    axis.line = element_line(color = \"black\"),\n    axis.text = element_text(size = 12, color = \"black\"),\n    axis.title = element_text(size = 12, color = \"black\"),\n    axis.ticks = element_line(size = .75, color = \"black\"),\n    axis.ticks.length = unit(.5, \"cm\")\n  )\n\n\n\nMusterlösung Übung 1.1: Assoziationstest\nIn diesem Beispiel soll einem Klischee auf den Grund gegagen werden: Sind Aargauer überdurchschnittlich mit weissen Socken assoziiert? Die Datenerhebung basiert auf männlichen Studenten bei denen folgende beiden binären Variablen erhoben wurden:\n\nSockenfarbe: weiss, nicht-weiss\nSelbstdeklarierte Kantonsangehörigkeit: AG, nicht-AG\n\nDie Hypothese ist: Das Klischee trifft zu, weisse Socken sind überdurchschnittlich häufig mit Aargauer Studenten assoziiert.\nDie Datenerhebung unter 35 Studenten ergab folgende Datengrundlage:\n\nWeisssockige Aargauer: 4\nNicht-weisssockige Aargauer: 2\nWeissockige nicht-Aargauer: 7\nNicht-weisssockige nicht-Aargauer: 22\n\n\n# Matrix erstellen\nAargauer &lt;- c(4, 2)\nnames(Aargauer) &lt;- c(\"Weiss\", \"NotWeiss\")\nNotAargauer &lt;- c(7, 22)\nnames(NotAargauer) &lt;- c(\"Weiss\", \"NotWeiss\")\nAGsocks &lt;- data.frame(Aargauer, NotAargauer)\nAGsocks &lt;- as.matrix(AGsocks)\nAGsocks\n\n         Aargauer NotAargauer\nWeiss           4           7\nNotWeiss        2          22\n\n# Daten anschauen mit erstem Google-Ergebnis für \"Assoziation Plot r\"\nassocplot(AGsocks) # Interpretation des Plots mit dem Befehl ?assocplot\n\n\n\n\n\n\n\n\nDer Assoziationsplot zeigt, dass in den Daten weisse Socken bei den Aargauern überverterten und bei den Nicht-Aargauern untervertreten sind.\nFür kleine Erwartungswerte in den Zellen (&lt; 5) ist der Chi-Quadrat-Test nicht zuverlässig (siehe “Warning message”). Darum wird mit Fishers exaktem Test gearbeitet.\n\n# Tests durchführen\nchisq.test(AGsocks) # Chi-Quadrat-Test nur zum anschauen.\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AGsocks\nX-squared = 2.4323, df = 1, p-value = 0.1189\n\nfisher.test(AGsocks) # \"Fisher's Exact Test for Count Data\"\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  AGsocks\np-value = 0.06323\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.6811227 78.4336189\nsample estimates:\nodds ratio \n  5.897263 \n\n\n\nErgebnisse\nIn den erhobenen Daten konnte keine signifikante Assoziation zwischen Kantonangehörigkeit (AG, nicht-AG) und Sockenfarbe (weiss, nicht-weiss) festgestellt werden. Der p-Wert von Fishers exaktem Test war nur marginal signifikant (p = 0.063). Das nicht-signfikante Resultat überrascht auf den ersten Blick, denn der “odds ratio” im Datensatz ist mit 5.9 relativ hoch und 67 % der Aargauer trugen weisse Socken während nur 24 % der Nicht-Aargauer weisse Socken trugen. Doch war der Anteil von nur 6 Aargauer in der nur 35 Männer umfassenden Stichprobe relativ klein, um ein verlässliches Bild der Sockenpräferenzen der Aargauer zu machen. Insofern leuchtet es ein, das bei dieser kleinen und unausgewogenen Stichprobe die “Power” des satistischen Tests (um die Nullhypothese zu verwerfen) relativ klein ist.\n\n\n\nMusterlösung Übung 1.2: t-Test\n\nNull- und Alternativhypothese\n\\(H_0\\): Es gibt keinen Unterschied in den Verkaufszahlen zwischen den Basis- und den Interventionswochen.\n\\(H_1\\): Es gibt einen Unterschied in den Verkaufszahlen zwischen den Basis- und den Interventionswochen.\n\n# lade Daten\ndf &lt;- read_delim(\"datasets/stat/Novanimal_Weeks.csv\", delim = \";\")\ndf # Daten anschauen\n\n# A tibble: 12 × 3\n    week condit       tot_sold\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1    40 Basis            2188\n 2    41 Intervention     2004\n 3    42 Basis            2113\n 4    43 Intervention     2160\n 5    44 Basis            2290\n 6    45 Intervention     2165\n 7    46 Intervention     2272\n 8    47 Basis            2198\n 9    48 Intervention     2293\n10    49 Basis            2343\n11    50 Intervention     2228\n12    51 Basis            2086\n\n# überprüft die Voraussetzungen für einen t-Test\nggplot(df, aes(x = condit, y = tot_sold)) + # achtung 0 Punkt fehlt\n  geom_boxplot(fill = \"white\", color = \"black\", size = 1) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.3) +\n  labs(x = \"\\nBedingungen\", y = \"Durchschnittlich verkaufte Gerichte pro Woche\\n\") +\n  mytheme\n\n\n\n\n\n\n\n# Auf den ersten Blick scheint es keine starken Abweichungen zu einer\n# Normalverteilung zu geben resp. es sind keine extremen schiefen Verteilungen\n# ersichtlich\n\n\n# führt einen t-Tests durch;\n# es wird angenommen, dass die Verkaufszahlen zwischen den Bedingungen\n# unabhängig sind\n\nt_test &lt;- t.test(tot_sold ~ condit, data = df)\nt_test\n\n\n    Welch Two Sample t-test\n\ndata:  tot_sold by condit\nt = 0.27168, df = 9.9707, p-value = 0.7914\nalternative hypothesis: true difference in means between group Basis and group Intervention is not equal to 0\n95 percent confidence interval:\n -115.2743  147.2743\nsample estimates:\n       mean in group Basis mean in group Intervention \n                      2203                       2187 \n\n# alternative Formulierung\nt.test(\n  df[df$condit == \"Basis\", ]$tot_sold,\n  df[df$condit == \"Intervention\", ]$tot_sold\n)\n\n\n    Welch Two Sample t-test\n\ndata:  df[df$condit == \"Basis\", ]$tot_sold and df[df$condit == \"Intervention\", ]$tot_sold\nt = 0.27168, df = 9.9707, p-value = 0.7914\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -115.2743  147.2743\nsample estimates:\nmean of x mean of y \n     2203      2187 \n\n\n\n\nMethoden\nUnser Ziel bestand darin, einen Vergleich der aggregierten Verkaufszahlen zwischen den Interventions- und Basiswochen durchzuführen. Wir gingen davon aus, dass die wöchentlichen Verkaufszahlen unabhängig voneinander sind, weshalb wir die Unterschiede zwischen den Verkaufszahlen pro Woche in den beiden Bedingungen mittels eines t-Tests untersuchten. Obwohl bei der visuellen Prüfung der Modellvoraussetzungen keine schwerwiegenden Verletzungen festgestellt wurden (abgesehen von einem Ausreißer), entschieden wir uns dennoch für einen Welch t-Test. Es sei angemerkt, dass die Gruppengröße mit jeweils n = 6 Wochen eher klein war. Trotzdem erzielten die T-Tests verlässliche Ergebnisse. Für weitere Informationen zu diesem Thema verweisen wir auf die verlinkte Studie.\n\n\nErgebnisse\nIn den Basiswochen werden wöchentlich mehr Gerichte verkauft als in den Interventionswochen, wie in Abbildung 1 ersichtlich ist. Allerdings ergab der Welch t-Test, dass es keine signifikanten Unterschiede in den wöchentlichen Verkaufszahlen zwischen den beiden Bedingungen (t(10) = 0.272 , p = 0.791). m die Ergebnisse weiter zu bestätigen, könnte eine \\(\\chi^2\\)-Test durchgeführt werden, da die Gruppengröße mit n = 6 als eher klein betrachtet werden kann.\n\n\n\n\n\n\n\n\nAbbildung 19.1: Die wöchentlichen Verkaufszahlen für die Interventions- und Basiswochen unterscheiden sich nicht signifikant.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html#vorebreitung-r-session",
    "href": "statistik/Statistik1_Uebung.html#vorebreitung-r-session",
    "title": "Statistik 1: Übung",
    "section": "Vorebreitung R-Session",
    "text": "Vorebreitung R-Session\n\n# lade Packages\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\n\n## definiert mytheme für ggplot2 (verwendet dabei theme_classic())\nmytheme &lt;-\n  theme_classic() +\n  theme(\n    axis.line = element_line(color = \"black\"),\n    axis.text = element_text(size = 12, color = \"black\"),\n    axis.title = element_text(size = 12, color = \"black\"),\n    axis.ticks = element_line(size = .75, color = \"black\"),\n    axis.ticks.length = unit(.5, \"cm\")\n  )",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html#musterlösung-übung-1.1-assoziationstest",
    "href": "statistik/Statistik1_Uebung.html#musterlösung-übung-1.1-assoziationstest",
    "title": "Statistik 1: Übung",
    "section": "Musterlösung Übung 1.1: Assoziationstest",
    "text": "Musterlösung Übung 1.1: Assoziationstest\nIn diesem Beispiel soll einem Klischee auf den Grund gegagen werden: Sind Aargauer überdurchschnittlich mit weissen Socken assoziiert? Die Datenerhebung basiert auf männlichen Studenten bei denen folgende beiden binären Variablen erhoben wurden:\n\nSockenfarbe: weiss, nicht-weiss\nSelbstdeklarierte Kantonsangehörigkeit: AG, nicht-AG\n\nDie Hypothese ist: Das Klischee trifft zu, weisse Socken sind überdurchschnittlich häufig mit Aargauer Studenten assoziiert.\nDie Datenerhebung unter 35 Studenten ergab folgende Datengrundlage:\n\nWeisssockige Aargauer: 4\nNicht-weisssockige Aargauer: 2\nWeissockige nicht-Aargauer: 7\nNicht-weisssockige nicht-Aargauer: 22\n\n\n# Matrix erstellen\nAargauer &lt;- c(4, 2)\nnames(Aargauer) &lt;- c(\"Weiss\", \"NotWeiss\")\nNotAargauer &lt;- c(7, 22)\nnames(NotAargauer) &lt;- c(\"Weiss\", \"NotWeiss\")\nAGsocks &lt;- data.frame(Aargauer, NotAargauer)\nAGsocks &lt;- as.matrix(AGsocks)\nAGsocks\n\n         Aargauer NotAargauer\nWeiss           4           7\nNotWeiss        2          22\n\n# Daten anschauen mit erstem Google-Ergebnis für \"Assoziation Plot r\"\nassocplot(AGsocks) # Interpretation des Plots mit dem Befehl ?assocplot\n\n\n\n\n\n\n\n\nDer Assoziationsplot zeigt, dass in den Daten weisse Socken bei den Aargauern überverterten und bei den Nicht-Aargauern untervertreten sind.\nFür kleine Erwartungswerte in den Zellen (&lt; 5) ist der Chi-Quadrat-Test nicht zuverlässig (siehe “Warning message”). Darum wird mit Fishers exaktem Test gearbeitet.\n\n# Tests durchführen\nchisq.test(AGsocks) # Chi-Quadrat-Test nur zum anschauen.\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AGsocks\nX-squared = 2.4323, df = 1, p-value = 0.1189\n\nfisher.test(AGsocks) # \"Fisher's Exact Test for Count Data\"\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  AGsocks\np-value = 0.06323\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.6811227 78.4336189\nsample estimates:\nodds ratio \n  5.897263 \n\n\n\nErgebnisse\nIn den erhobenen Daten konnte keine signifikante Assoziation zwischen Kantonangehörigkeit (AG, nicht-AG) und Sockenfarbe (weiss, nicht-weiss) festgestellt werden. Der p-Wert von Fishers exaktem Test war nur marginal signifikant (p = 0.063). Das nicht-signfikante Resultat überrascht auf den ersten Blick, denn der “odds ratio” im Datensatz ist mit 5.9 relativ hoch und 67 % der Aargauer trugen weisse Socken während nur 24 % der Nicht-Aargauer weisse Socken trugen. Doch war der Anteil von nur 6 Aargauer in der nur 35 Männer umfassenden Stichprobe relativ klein, um ein verlässliches Bild der Sockenpräferenzen der Aargauer zu machen. Insofern leuchtet es ein, das bei dieser kleinen und unausgewogenen Stichprobe die “Power” des satistischen Tests (um die Nullhypothese zu verwerfen) relativ klein ist.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik1_Uebung.html#musterlösung-übung-1.2-t-test",
    "href": "statistik/Statistik1_Uebung.html#musterlösung-übung-1.2-t-test",
    "title": "Statistik 1: Übung",
    "section": "Musterlösung Übung 1.2: t-Test",
    "text": "Musterlösung Übung 1.2: t-Test\n\nNull- und Alternativhypothese\n\\(H_0\\): Es gibt keinen Unterschied in den Verkaufszahlen zwischen den Basis- und den Interventionswochen.\n\\(H_1\\): Es gibt einen Unterschied in den Verkaufszahlen zwischen den Basis- und den Interventionswochen.\n\n# lade Daten\ndf &lt;- read_delim(\"datasets/stat/Novanimal_Weeks.csv\", delim = \";\")\ndf # Daten anschauen\n\n# A tibble: 12 × 3\n    week condit       tot_sold\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1    40 Basis            2188\n 2    41 Intervention     2004\n 3    42 Basis            2113\n 4    43 Intervention     2160\n 5    44 Basis            2290\n 6    45 Intervention     2165\n 7    46 Intervention     2272\n 8    47 Basis            2198\n 9    48 Intervention     2293\n10    49 Basis            2343\n11    50 Intervention     2228\n12    51 Basis            2086\n\n# überprüft die Voraussetzungen für einen t-Test\nggplot(df, aes(x = condit, y = tot_sold)) + # achtung 0 Punkt fehlt\n  geom_boxplot(fill = \"white\", color = \"black\", size = 1) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.3) +\n  labs(x = \"\\nBedingungen\", y = \"Durchschnittlich verkaufte Gerichte pro Woche\\n\") +\n  mytheme\n\n\n\n\n\n\n\n# Auf den ersten Blick scheint es keine starken Abweichungen zu einer\n# Normalverteilung zu geben resp. es sind keine extremen schiefen Verteilungen\n# ersichtlich\n\n\n# führt einen t-Tests durch;\n# es wird angenommen, dass die Verkaufszahlen zwischen den Bedingungen\n# unabhängig sind\n\nt_test &lt;- t.test(tot_sold ~ condit, data = df)\nt_test\n\n\n    Welch Two Sample t-test\n\ndata:  tot_sold by condit\nt = 0.27168, df = 9.9707, p-value = 0.7914\nalternative hypothesis: true difference in means between group Basis and group Intervention is not equal to 0\n95 percent confidence interval:\n -115.2743  147.2743\nsample estimates:\n       mean in group Basis mean in group Intervention \n                      2203                       2187 \n\n# alternative Formulierung\nt.test(\n  df[df$condit == \"Basis\", ]$tot_sold,\n  df[df$condit == \"Intervention\", ]$tot_sold\n)\n\n\n    Welch Two Sample t-test\n\ndata:  df[df$condit == \"Basis\", ]$tot_sold and df[df$condit == \"Intervention\", ]$tot_sold\nt = 0.27168, df = 9.9707, p-value = 0.7914\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -115.2743  147.2743\nsample estimates:\nmean of x mean of y \n     2203      2187 \n\n\n\n\nMethoden\nUnser Ziel bestand darin, einen Vergleich der aggregierten Verkaufszahlen zwischen den Interventions- und Basiswochen durchzuführen. Wir gingen davon aus, dass die wöchentlichen Verkaufszahlen unabhängig voneinander sind, weshalb wir die Unterschiede zwischen den Verkaufszahlen pro Woche in den beiden Bedingungen mittels eines t-Tests untersuchten. Obwohl bei der visuellen Prüfung der Modellvoraussetzungen keine schwerwiegenden Verletzungen festgestellt wurden (abgesehen von einem Ausreißer), entschieden wir uns dennoch für einen Welch t-Test. Es sei angemerkt, dass die Gruppengröße mit jeweils n = 6 Wochen eher klein war. Trotzdem erzielten die T-Tests verlässliche Ergebnisse. Für weitere Informationen zu diesem Thema verweisen wir auf die verlinkte Studie.\n\n\nErgebnisse\nIn den Basiswochen werden wöchentlich mehr Gerichte verkauft als in den Interventionswochen, wie in Abbildung 1 ersichtlich ist. Allerdings ergab der Welch t-Test, dass es keine signifikanten Unterschiede in den wöchentlichen Verkaufszahlen zwischen den beiden Bedingungen (t(10) = 0.272 , p = 0.791). m die Ergebnisse weiter zu bestätigen, könnte eine \\(\\chi^2\\)-Test durchgeführt werden, da die Gruppengröße mit n = 6 als eher klein betrachtet werden kann.\n\n\n\n\n\n\n\n\nAbbildung 19.1: Die wöchentlichen Verkaufszahlen für die Interventions- und Basiswochen unterscheiden sich nicht signifikant.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistik 1: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html",
    "href": "statistik/Statistik2_Demo.html",
    "title": "Statistik 2: Demo",
    "section": "",
    "text": "ANOVA\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#anova",
    "href": "statistik/Statistik2_Demo.html#anova",
    "title": "Statistik 2: Demo",
    "section": "",
    "text": "t-test als ANOVA\n\n# Dieselben Daten wie für die t-Tests in Statistik 1\nMesswerte_a &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14) # Messwerte von Cultivar a\nMesswerte_b &lt;- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10) # Messwerte von Cultivar b\ncultivar &lt;- as.factor( c(rep(\"a\", 10), rep(\"b\", 10))) # Bezeichnug der Cultivare in der Tabelle\nblume &lt;- data.frame(\"cultivar\" = cultivar, \"size\" = c(Messwerte_a, Messwerte_b)) # Data frame erstellen\n\n# Daten anschauen\nlibrary(ggplot2)\n\nggplot(blume, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot() + # Boxplots\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) # Datenpunkte darstellen\n\n\n\n\n\n\n\n# Klassischer t-Test ausführen\nt.test(size ~ cultivar, var.equal = TRUE, data = blume)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n# ANOVA ausführen\naov(size ~ cultivar, data = blume)\n\nCall:\n   aov(formula = size ~ cultivar, data = blume)\n\nTerms:\n                cultivar Residuals\nSum of Squares     76.05    316.50\nDeg. of Freedom        1        18\n\nResidual standard error: 4.193249\nEstimated effects may be unbalanced\n\nsummary( aov(size ~ cultivar, data = blume))\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ncultivar     1   76.0   76.05   4.325 0.0521 .\nResiduals   18  316.5   17.58                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm( aov(size ~ cultivar, data = blume))\n\n\nCall:\naov(formula = size ~ cultivar, data = blume)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -2.575 -0.350  2.925  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.300      1.326   11.54 9.47e-10 ***\ncultivarb     -3.900      1.875   -2.08   0.0521 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.193 on 18 degrees of freedom\nMultiple R-squared:  0.1937,    Adjusted R-squared:  0.1489 \nF-statistic: 4.325 on 1 and 18 DF,  p-value: 0.05212\n\n\n\n\nEchte ANOVA\n\n# Ein weiterer Cultivar hinzufügen\nMesswerte_c &lt;-c(30, 19, 31, 23, 18, 25, 26, 24, 17, 20)\ncultivar &lt;- as.factor( c(rep(\"a\", 10), rep(\"b\", 10), rep(\"c\", 10))) # Bezeichnug der Cultivare in der Tabelle\nblume2 &lt;- data.frame(\"cultivar\" = cultivar, \"size\" = c(Messwerte_a, Messwerte_b, Messwerte_c)) # Data frame erstellen\n\n\n# Daten als Boxplots anschauen\nggplot(blume2, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot()  + # Boxplots\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) # Datenpunkte darstellen\n\n\n\n\n\n\n\n# Kennzahlen der Daten anschauen\n\nlibrary(\"dplyr\")\n\nblume2 |&gt; \n  group_by(cultivar) |&gt;\n  summarise(\n    Mean = mean(size), \n    SD = sd(size),\n    Min = min(size),\n    Max = max(size)\n  )\n\n# A tibble: 3 × 5\n  cultivar  Mean    SD   Min   Max\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a         15.3  5.21     8    25\n2 b         11.4  2.84     7    16\n3 c         23.3  4.85    17    31\n\n# ANOVA durchführen\naov1 &lt;- aov(size ~ cultivar, data = blume2)\naov1\n\nCall:\n   aov(formula = size ~ cultivar, data = blume2)\n\nTerms:\n                cultivar Residuals\nSum of Squares  736.0667  528.6000\nDeg. of Freedom        2        27\n\nResidual standard error: 4.424678\nEstimated effects may be unbalanced\n\nsummary(aov1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov1)\n\n\nCall:\naov(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n# Direkt als lineares Modell\nlm1 &lt;- lm(size ~ cultivar, data = blume2)\nsummary(lm1)\n\n\nCall:\nlm(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n\n\n\nTukeys Posthoc-Test\n\n# Load library\nlibrary(\"agricolae\")\n\n# Sorten mit ihren Namen bezeichnen, damit keine Verwechslung mit den Post-Hoc-Labels entsteht\nblume2n &lt;- blume2 \nblume2n$cultivar &lt;- recode(blume2n$cultivar, \"a\" = \"Andro\", \"b\" = \"Bulli\", \"c\" = \"Chroma\")\n\n# ANOVA und Posthoc-Test durchführen\naov1 &lt;- aov(size ~ cultivar, data = blume2n)\nposthoc &lt;- HSD.test(aov1, \"cultivar\", console = TRUE)\n\n\nStudy: aov1 ~ \"cultivar\"\n\nHSD Test for size \n\nMean Square Error:  19.57778 \n\ncultivar,  means\n\n       size      std  r       se Min Max   Q25  Q50   Q75\nAndro  15.3 5.207900 10 1.399206   8  25 11.50 14.5 18.75\nBulli  11.4 2.836273 10 1.399206   7  16 10.00 11.5 12.75\nChroma 23.3 4.854551 10 1.399206  17  31 19.25 23.5 25.75\n\nAlpha: 0.05 ; DF Error: 27 \nCritical Value of Studentized Range: 3.506426 \n\nMinimun Significant Difference: 4.906213 \n\nTreatments with the same letter are not significantly different.\n\n       size groups\nChroma 23.3      a\nAndro  15.3      b\nBulli  11.4      b\n\nposthoc\n\n$statistics\n   MSerror Df     Mean       CV      MSD\n  19.57778 27 16.66667 26.54807 4.906213\n\n$parameters\n   test   name.t ntr StudentizedRange alpha\n  Tukey cultivar   3         3.506426  0.05\n\n$means\n       size      std  r       se Min Max   Q25  Q50   Q75\nAndro  15.3 5.207900 10 1.399206   8  25 11.50 14.5 18.75\nBulli  11.4 2.836273 10 1.399206   7  16 10.00 11.5 12.75\nChroma 23.3 4.854551 10 1.399206  17  31 19.25 23.5 25.75\n\n$comparison\nNULL\n\n$groups\n       size groups\nChroma 23.3      a\nAndro  15.3      b\nBulli  11.4      b\n\nattr(,\"class\")\n[1] \"group\"\n\n# Darstellung der Ergebnisse mit Post-Hoc-Labels über Boxplots\n\n# Labels des Posthoc-Tests extrahieren\nlabels &lt;- posthoc$groups\nlabels$cultivar &lt;- rownames(labels)\n\n#In Plot darstellen\nggplot(blume2n, aes(x = cultivar, y = size, fill = cultivar)) +\n  geom_boxplot() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5) +\n  geom_text(data = labels, aes(x = cultivar, y = 33, label = groups)) \n\n\n\n\n\n\n\n\n\n\n2-faktorielle ANOVA\n\n# Daten generieren\nMesswerte_d &lt;- c(10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\nMesswerte_e &lt;- c(15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nMesswerte_f &lt;- c(10, 12, 11, 13, 10, 9, 2, 4, 7, 13)\n\n\nblume3 &lt;- data.frame(\n    cultivar = c(rep(\"a\", 20), rep(\"b\", 20), rep(\"c\", 20)),\n    house = as.factor( c(rep(c(rep(\"yes\", 10), rep(\"no\", 10)), 3))),\n    size = c(Messwerte_a, Messwerte_b, Messwerte_c, Messwerte_d, Messwerte_e, Messwerte_f)\n)\n\nblume3\n\n   cultivar house size\n1         a   yes   20\n2         a   yes   19\n3         a   yes   25\n4         a   yes   10\n5         a   yes    8\n6         a   yes   15\n7         a   yes   13\n8         a   yes   18\n9         a   yes   11\n10        a   yes   14\n11        a    no   12\n12        a    no   15\n13        a    no   16\n14        a    no    7\n15        a    no    8\n16        a    no   10\n17        a    no   12\n18        a    no   11\n19        a    no   13\n20        a    no   10\n21        b   yes   30\n22        b   yes   19\n23        b   yes   31\n24        b   yes   23\n25        b   yes   18\n26        b   yes   25\n27        b   yes   26\n28        b   yes   24\n29        b   yes   17\n30        b   yes   20\n31        b    no   10\n32        b    no   12\n33        b    no   11\n34        b    no   13\n35        b    no   10\n36        b    no   25\n37        b    no   12\n38        b    no   30\n39        b    no   26\n40        b    no   13\n41        c   yes   15\n42        c   yes   13\n43        c   yes   18\n44        c   yes   11\n45        c   yes   14\n46        c   yes   25\n47        c   yes   39\n48        c   yes   38\n49        c   yes   28\n50        c   yes   24\n51        c    no   10\n52        c    no   12\n53        c    no   11\n54        c    no   13\n55        c    no   10\n56        c    no    9\n57        c    no    2\n58        c    no    4\n59        c    no    7\n60        c    no   13\n\n\n\n# Daten mit Boxplots anschauen\n# Base-R-Variante wäre: boxplot(size ~ cultivar + house, data = blume3)\nggplot(blume3, aes(x = interaction(cultivar, house), y = size, fill = cultivar)) +\n  geom_boxplot() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", alpha = 0.5)\n\n\n\n\n\n\n\nsummary( aov(size ~ cultivar + house, data = blume3))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  417.1   208.5   5.005     0.01 *  \nhouse        1  992.3   992.3  23.815 9.19e-06 ***\nResiduals   56 2333.2    41.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary( aov(size ~ cultivar + house + cultivar:house, data = blume3))\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar        2  417.1   208.5   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Kurzschreibweise: \"*\" bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird\nsummary( aov(size ~ cultivar * house, data = blume3))\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar        2  417.1   208.5   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm( aov(size ~ cultivar + house, data = blume3))\n\n\nCall:\naov(formula = size ~ cultivar + house, data = blume3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.733 -4.696 -1.050  2.717 19.133 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    9.283      1.667   5.570 7.52e-07 ***\ncultivarb      6.400      2.041   3.135  0.00273 ** \ncultivarc      2.450      2.041   1.200  0.23509    \nhouseyes       8.133      1.667   4.880 9.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.455 on 56 degrees of freedom\nMultiple R-squared:  0.3766,    Adjusted R-squared:  0.3432 \nF-statistic: 11.28 on 3 and 56 DF,  p-value: 6.848e-06\n\n# Interaktionsplots\n\ninteraction.plot(blume3$cultivar, blume3$house, blume3$size)\n\n\n\n\n\n\n\ninteraction.plot(blume3$house, blume3$cultivar, blume3$size)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#modelldiagnostik",
    "href": "statistik/Statistik2_Demo.html#modelldiagnostik",
    "title": "Statistik 2: Demo",
    "section": "Modelldiagnostik",
    "text": "Modelldiagnostik\n\nBeispiel Modelldiagnostik\n\n# Beispiel mit den blume Daten\npar(mfrow = c(2, 2)) # 4 Plots in einem Fenster\nplot( lm(Messwerte_b ~ Messwerte_a))\n\n\n\n\n\n\n\n\n\n\nBeispiel Modelldiagnostik nicht ok\n\n# Daten erstellen\ng &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nh &lt;- c(12, 15, 10, 7, 8, 10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\nBsp &lt;- data.frame(g, h)\n\n# Daten betrachten\nggplot(Bsp, aes(x = g, y = h)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(lm(h ~ g))",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "href": "statistik/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "title": "Statistik 2: Demo",
    "section": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind",
    "text": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind\n\n# Nicht-parametrische Alternative zu t-Test\nwilcox.test(Messwerte_a, Messwerte_b)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Messwerte_a and Messwerte_b\nW = 73, p-value = 0.08789\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#kruskal-wallis-test-bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "href": "statistik/Statistik2_Demo.html#kruskal-wallis-test-bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "title": "Statistik 2: Demo",
    "section": "Kruskal-Wallis-Test bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen",
    "text": "Kruskal-Wallis-Test bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen\n\n# Zum Vergleich normale ANOVA noch mal\nsummary(aov(size ~ cultivar, data = blume2))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Kruskal-Wallis-Test\nkruskal.test(size ~ cultivar, data = blume2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  size by cultivar\nKruskal-Wallis chi-squared = 16.686, df = 2, p-value = 0.0002381\n\n\n\n# Load library\nlibrary(\"FSA\")\n\n# Post-Hoc mit korrigierten p-Werte nach Bejamini-Hochberg\ndunnTest(size ~ cultivar, method = \"bh\", data = blume2)\n\n  Comparison         Z      P.unadj        P.adj\n1      a - b  1.526210 1.269575e-01 0.1269575490\n2      a - c -2.518247 1.179407e-02 0.0176911039\n3      b - c -4.044457 5.244459e-05 0.0001573338",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Demo.html#welch-test-bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "href": "statistik/Statistik2_Demo.html#welch-test-bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "title": "Statistik 2: Demo",
    "section": "Welch-Test bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen",
    "text": "Welch-Test bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen\n\n# Welch-Test\noneway.test(size ~ cultivar, var.equal = F, data = blume2)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  size and cultivar\nF = 21.642, num df = 2.000, denom df = 16.564, p-value = 2.397e-05",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistik 2: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Uebung.html",
    "href": "statistik/Statistik2_Uebung.html",
    "title": "Statistik 2: Übung",
    "section": "",
    "text": "Aufgabe ANOVA mit Kormoran-Daten\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistik 2: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik2_Uebung.html#kommentierter-lösungsweg",
    "href": "statistik/Statistik2_Uebung.html#kommentierter-lösungsweg",
    "title": "Statistik 2: Übung",
    "section": "Kommentierter Lösungsweg",
    "text": "Kommentierter Lösungsweg\n\nlibrary(\"readr\")\n\n# Daten laden\nkormoran &lt;- read_delim(\"datasets/stat/kormoran.csv\", \";\", col_types = cols(\"Unterart\" = \"f\", \"Jahreszeit\" = \"f\")) \n\n# Ueberpruefen, ob Einlesen richtig funktioniert hat und welche Datenstruktur vorliegt\nstr(kormoran)\n\nspc_tbl_ [40 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Obs       : num [1:40] 1 2 3 4 5 6 7 8 9 10 ...\n $ Tauchzeit : num [1:40] 9.5 11.9 13.4 13.8 15.3 15.5 15.6 16.7 16.8 18.7 ...\n $ Unterart  : Factor w/ 2 levels \"C\",\"S\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Jahreszeit: Factor w/ 4 levels \"F\",\"S\",\"H\",\"W\": 1 1 1 1 1 2 2 2 2 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Obs = col_double(),\n  ..   Tauchzeit = col_double(),\n  ..   Unterart = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n  ..   Jahreszeit = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE)\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(kormoran)\n\n      Obs          Tauchzeit     Unterart Jahreszeit\n Min.   : 1.00   Min.   : 9.50   C:20     F:10      \n 1st Qu.:10.75   1st Qu.:13.38   S:20     S:10      \n Median :20.50   Median :16.75            H:10      \n Mean   :20.50   Mean   :17.40            W:10      \n 3rd Qu.:30.25   3rd Qu.:20.77                      \n Max.   :40.00   Max.   :30.40                      \n\n\nMan erkennt, dass es sich um einen Dataframe mit einer metrischen (Tauchzeit) und zwei kategorialen (Unterart, Jahreszeit) Variablen handelt. Die adäquate Analyse (1 metrische Abhängige vs. 2 kategoriale Unabhängige) ist damit eine zweifaktorielle ANOVA. Die Sortierung der Jahreszeiten (default: alphabetisch) ist inhaltlich aber nicht sinnvoll und sollte angepasst werden.\n\n# Umsortieren der Faktoren, damit sie in den Boxplots eine sinnvolle Reihung haben\nkormoran$Jahreszeit &lt;- ordered(kormoran$Jahreszeit, levels = c(\"F\", \"S\", \"H\", \"W\"))\nkormoran$Jahreszeit\n\n [1] F F F F F S S S S S H H H H H W W W W W F F F F F S S S S S H H H H H W W W\n[39] W W\nLevels: F &lt; S &lt; H &lt; W\n\n\n\n# Explorative Datenanalyse\n# (Check auf Normalverteilung der Residuen und Varianzhomogenitaet)\nboxplot(Tauchzeit ~ Jahreszeit * Unterart, data = kormoran)\n\n\n\n\n\n\n\n\nWegen der Asymmetrie der Boxen könnte man eine log-Transformation ausprobieren.\n\nboxplot(log10(Tauchzeit) ~ Jahreszeit * Unterart, data = kormoran)\n\n\n\n\n\n\n\n\nDa die Transformation keine klare Verbesserung bringt, bleiben wir im Folgenden bei den untransformierten Daten, da diese leichter (direkter) interpretiert werden können. Auch angesichts der sehr gerigen Anzahl an Datenpunkte (5 Tauchgänge pro Art-Jahreszeit-Kombination) sind asymmwetrische Boxplots wenig überraschend. Das Gesamtbild ist noch OK für parametrische Verfahren (Box ziemlich symmetrisch um Median, Whisker etwas asymmetrisch aber nicht kritisch).\n\n# Vollständiges Modell mit Interaktion\naov_1 &lt;- aov(Tauchzeit ~ Unterart * Jahreszeit, data = kormoran)\naov_1\n\nCall:\n   aov(formula = Tauchzeit ~ Unterart * Jahreszeit, data = kormoran)\n\nTerms:\n                Unterart Jahreszeit Unterart:Jahreszeit Residuals\nSum of Squares   106.929    756.170              11.009    84.992\nDeg. of Freedom        1          3                   3        32\n\nResidual standard error: 1.629724\nEstimated effects may be unbalanced\n\nsummary(aov_1)\n\n                    Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nUnterart             1  106.9  106.93  40.259 4.01e-07 ***\nJahreszeit           3  756.2  252.06  94.901 5.19e-16 ***\nUnterart:Jahreszeit  3   11.0    3.67   1.382    0.266    \nResiduals           32   85.0    2.66                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov_1)\n\n\nCall:\naov(formula = Tauchzeit ~ Unterart * Jahreszeit, data = kormoran)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.280 -0.905 -0.290  0.945  4.580 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            19.03500    0.36442  52.234  &lt; 2e-16 ***\nUnterartS              -3.27000    0.51536  -6.345 4.01e-07 ***\nJahreszeit.L            9.69112    0.72883  13.297 1.40e-14 ***\nJahreszeit.Q            0.53000    0.72883   0.727   0.4724    \nJahreszeit.C            0.08497    0.72883   0.117   0.9079    \nUnterartS:Jahreszeit.L -2.02141    1.03073  -1.961   0.0586 .  \nUnterartS:Jahreszeit.Q -0.10000    1.03073  -0.097   0.9233    \nUnterartS:Jahreszeit.C -0.55454    1.03073  -0.538   0.5943    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.63 on 32 degrees of freedom\nMultiple R-squared:  0.9114,    Adjusted R-squared:  0.892 \nF-statistic: 47.02 on 7 and 32 DF,  p-value: 4.638e-15\n\n# p-Wert der Interaktion ist 0.266\n\n\n# Modelvalidierung mit Diagnostikplots\npar(mfrow = c(2, 2))\nplot(aov_1)\n\n\n\n\n\n\n\n\nDas volle (maximale) Modell zeigt, dass es keine signifikante Interaktion zwischen Jahreszeit und Unterart gibt. Wir können das Modell also vereinfachen, indem wir die Interaktion herausnehmen (+ statt * in der Modellspezifikation)\n\n# Modellvereinfachung\naov_2 &lt;- aov(Tauchzeit ~ Unterart + Jahreszeit, data = kormoran)\naov_2\n\nCall:\n   aov(formula = Tauchzeit ~ Unterart + Jahreszeit, data = kormoran)\n\nTerms:\n                Unterart Jahreszeit Residuals\nSum of Squares   106.929    756.170    96.001\nDeg. of Freedom        1          3        35\n\nResidual standard error: 1.656166\nEstimated effects may be unbalanced\n\nsummary(aov_2)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nUnterart     1  106.9  106.93   38.98 3.69e-07 ***\nJahreszeit   3  756.2  252.06   91.89  &lt; 2e-16 ***\nResiduals   35   96.0    2.74                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIm so vereinfachten Modell sind alle verbleibenden Terme signifikant, wir sind also beim „minimal adäquaten Modell“ angelangt\n\n# Anderer Weg, um zu pruefen, ob man das komplexere Modell mit Interaktion behalten soll\nanova(aov_1, aov_2)\n\nAnalysis of Variance Table\n\nModel 1: Tauchzeit ~ Unterart * Jahreszeit\nModel 2: Tauchzeit ~ Unterart + Jahreszeit\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     32 84.992                           \n2     35 96.001 -3   -11.009 1.3817 0.2661\n\n# In diesem Fall bekommen wir den gleichen p-Wert wie oben (0.266)\n\n# Modelldiagnostik\npar(mfrow = c(2, 2)) # alle vier Abbildungen in einem 2 x 2 Raster\nplot(aov_2)\n\n\n\n\n\n\n\n\n\ninfluence.measures(aov_2) #\n# kann man sich zusätzlich zum \"plot\" ansehen, um herauszufinden,\n# ob es evtl. sehr einflussreiche Werte mit Cook's D von 1 oder grösser gibt\n\nLinks oben ist alles bestens, d. h. keine Hinweise auf Varianzheterogenität („Keil“) oder Nichtlinearität („Banane“) Rechts oben ganz gut, allerdings weichen Punkte 1 und 20 deutlich von der optimalen Gerade ab -&gt; aus diesem Grund können wir es doch noch mal mit der log10-Transformation versuchen (s.u.) Rechts unten: kein Punkt hat einen problematischen Einfluss (die roten Linien für Cook’s D &gt; 0.5 und &gt; 1 sind noch nicht einmal im Bildausschnitt.\n\n# Alternative mit log10\naov_3 &lt;- aov(log10(Tauchzeit) ~ Unterart + Jahreszeit, data = kormoran)\naov_3\n\nCall:\n   aov(formula = log10(Tauchzeit) ~ Unterart + Jahreszeit, data = kormoran)\n\nTerms:\n                 Unterart Jahreszeit Residuals\nSum of Squares  0.0627004  0.4958434 0.0562031\nDeg. of Freedom         1          3        35\n\nResidual standard error: 0.04007247\nEstimated effects may be unbalanced\n\nsummary(aov_3)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nUnterart     1 0.0627 0.06270   39.05 3.64e-07 ***\nJahreszeit   3 0.4958 0.16528  102.93  &lt; 2e-16 ***\nResiduals   35 0.0562 0.00161                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(aov_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRechts oben: Punkt 20 jetzt auf der Linie, aber Punkt 1 weicht umso deutlicher ab -&gt; keine Verbesserung -&gt; wir bleiben bei den untransformierten Daten. Da wir keine Interaktion zwischen Unterart und Jahreszeit festgestellt haben, brauchen wir auch keinen Interaktionsplot (unnötig kompliziert), statt dessen können wir die Ergebnisse am besten mit zwei getrennten Plots für die beiden Faktoren darstellen. Bitte die Achsenbeschriftungen und den Tukey post-hoc-Test nicht vergessen.\n\npar(mfrow = c(1, 1)) # Zurückschalten auf Einzelplots\nlibrary(\"multcomp\")\n\nboxplot(Tauchzeit ~ Unterart, data = kormoran)\n\n\n\n\n\n\n\nletters &lt;- cld(glht(aov_2, linfct = mcp(Jahreszeit = \"Tukey\")))\nboxplot(Tauchzeit ~ Jahreszeit, data = kormoran)\nmtext(letters$mcletters$Letters, at = 1:4)\n\n\n\n\n\n\n\n\nJetzt brauchen wir noch die Mittelwerte bzw. Effektgroessen\nFür den Ergebnistext brauchen wir auch noch Angaben zu den Effektgrössen. Hier sind zwei Möglichkeiten, um an sie zu gelangen.\n\nlibrary(\"dplyr\")\n\nkormoran |&gt;\n  group_by(Jahreszeit) |&gt;\n  summarise(Tauchzeit = mean(Tauchzeit))\n\n# A tibble: 4 × 2\n  Jahreszeit Tauchzeit\n  &lt;ord&gt;          &lt;dbl&gt;\n1 F               11.9\n2 S               15.1\n3 H               19.2\n4 W               23.4\n\nkormoran |&gt;\n  group_by(Unterart) |&gt;\n  summarise(Tauchzeit = mean(Tauchzeit))\n\n# A tibble: 2 × 2\n  Unterart Tauchzeit\n  &lt;fct&gt;        &lt;dbl&gt;\n1 C             19.0\n2 S             15.8\n\nsummary(lm(Tauchzeit ~ Jahreszeit, data = kormoran))\n\n\nCall:\nlm(formula = Tauchzeit ~ Jahreszeit, data = kormoran)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.820 -1.617 -0.145  1.587  6.980 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   17.4000     0.3754  46.351  &lt; 2e-16 ***\nJahreszeit.L   8.6804     0.7508  11.562 1.12e-13 ***\nJahreszeit.Q   0.4800     0.7508   0.639    0.527    \nJahreszeit.C  -0.1923     0.7508  -0.256    0.799    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.374 on 36 degrees of freedom\nMultiple R-squared:  0.7884,    Adjusted R-squared:  0.7708 \nF-statistic: 44.72 on 3 and 36 DF,  p-value: 3.156e-12\n\nsummary(lm(Tauchzeit ~ Unterart, data = kormoran))\n\n\nCall:\nlm(formula = Tauchzeit ~ Unterart, data = kormoran)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.535 -3.585 -0.335  3.760 11.365 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   19.035      1.059  17.976   &lt;2e-16 ***\nUnterartS     -3.270      1.498  -2.184   0.0352 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 38 degrees of freedom\nMultiple R-squared:  0.1115,    Adjusted R-squared:  0.08811 \nF-statistic: 4.768 on 1 and 38 DF,  p-value: 0.03523",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistik 2: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Demo.html",
    "href": "statistik/Statistik3_Demo.html",
    "title": "Statistik 3: Demo",
    "section": "",
    "text": "Korrelation vs. Regression\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n## Korrelationen und Regressionen\n\n# Datensatz zum Einfluss von Stickstoffdepositionen auf den Pflanzenartenreichtum\nlibrary(readr)\nlibrary(ggplot2)\n\ndf &lt;- read_delim(\"datasets/stat/Nitrogen.csv\", \";\")\n\nsummary(df)\n\n  N_deposition   Species_richness\n Min.   : 2.00   Min.   :12.0    \n 1st Qu.: 9.00   1st Qu.:17.5    \n Median :20.00   Median :21.0    \n Mean   :20.53   Mean   :20.2    \n 3rd Qu.:30.50   3rd Qu.:23.0    \n Max.   :55.00   Max.   :28.0    \n\n# Plotten der Beziehung\nggplot(df, aes(x = N_deposition, y = Species_richness)) +\n  geom_point()\n# Pearson Korrelation\n# zuerst Species_richness dann N_deposition\ncor.test(df$Species_richness, df$N_deposition, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  df$Species_richness and df$N_deposition\nt = -5.2941, df = 13, p-value = 0.0001453\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9405572 -0.5450218\nsample estimates:\n       cor \n-0.8265238 \n\n# Pearson Korrelation\n# zuerst Species_richness dann N_deposition\ncor.test(df$N_deposition, df$Species_richness, method = \"pearson\") # zuerst N_deposition dann Species_richness  \n\n\n    Pearson's product-moment correlation\n\ndata:  df$N_deposition and df$Species_richness\nt = -5.2941, df = 13, p-value = 0.0001453\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9405572 -0.5450218\nsample estimates:\n       cor \n-0.8265238 \n\n# Rang-Korrelation Spearman\ncor.test(df$Species_richness, df$N_deposition, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  df$Species_richness and df$N_deposition\nS = 1015.5, p-value = 0.0002259\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.8133721 \n\n# Rang-Korrelation Kendall\ncor.test(df$Species_richness, df$N_deposition, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  df$Species_richness and df$N_deposition\nz = -3.308, p-value = 0.0009398\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n-0.657115 \n\n# Jetzt als Regression\nlm1 &lt;- lm(Species_richness ~ N_deposition, data = df) # zuerst Species_richness dann N_deposition\nlm1\n\n\nCall:\nlm(formula = Species_richness ~ N_deposition, data = df)\n\nCoefficients:\n (Intercept)  N_deposition  \n     25.6050       -0.2632  \n\nlm2 &lt;- lm(N_deposition ~ Species_richness, data = df) # zuerst N_deposition dann Species_richness  \nlm2\n\n\nCall:\nlm(formula = N_deposition ~ Species_richness, data = df)\n\nCoefficients:\n     (Intercept)  Species_richness  \n          72.957            -2.595  \n\nanova(lm1) # ANOVA-Tabelle, 1. Möglichkeit\n\nAnalysis of Variance Table\n\nResponse: Species_richness\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nN_deposition  1 233.91 233.908  28.028 0.0001453 ***\nResiduals    13 108.49   8.346                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.aov(lm1) # ANOVA-Tabelle, 2. Möglichkeit\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nN_deposition  1  233.9  233.91   28.03 0.000145 ***\nResiduals    13  108.5    8.35                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm1) # Regressionskoeffizienten\n\n\nCall:\nlm(formula = Species_richness ~ N_deposition, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9184 -1.9992  0.4493  2.0015  4.6081 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.60502    1.26440  20.251 3.25e-11 ***\nN_deposition -0.26323    0.04972  -5.294 0.000145 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.889 on 13 degrees of freedom\nMultiple R-squared:  0.6831,    Adjusted R-squared:  0.6588 \nF-statistic: 28.03 on 1 and 13 DF,  p-value: 0.0001453\n\n# Signifikantes Ergebnis visualisieren\n\nggplot(df, aes(x = N_deposition, y = Species_richness)) +\n  geom_point() +\n  geom_abline(intercept = lm1$coefficients[1], slope = lm1$coefficients[2], color = \"blue\")\n\n\n\n\n\n\n\n#BaseR Variante plot(Species_richness ~ N_deposition, data = df) + abline(lm1)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Statistik 3: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Demo.html#einfache-und-polynomische-regression",
    "href": "statistik/Statistik3_Demo.html#einfache-und-polynomische-regression",
    "title": "Statistik 3: Demo",
    "section": "Einfache und Polynomische Regression",
    "text": "Einfache und Polynomische Regression\n\n# Daten generieren \npred &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24) # \"pred\" sei unsere unabhängige Variable\nresp &lt;- c(12, 15, 10, 7, 2, 10, 12, 11, 13, 10, 9, 2, 4, 7, 13) # \"resp\" sei unsere abhängige Variable\ndata &lt;- data.frame(pred, resp) # Dataframe erstellen\n\n# Daten anschauen\nggplot(data, aes(x = pred, y = resp)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# Modell definieren ud anschauen\nlm_1 &lt;- lm(resp ~ pred) # Einfaches lineares Modell\nsummary(lm_1) # Modell anschauen\n\n\nCall:\nlm(formula = resp ~ pred)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0549 -1.7015  0.5654  2.0617  5.6406 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  12.2879     2.4472   5.021 0.000234 ***\npred         -0.1541     0.1092  -1.412 0.181538    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 13 degrees of freedom\nMultiple R-squared:  0.1329,    Adjusted R-squared:  0.06622 \nF-statistic: 1.993 on 1 and 13 DF,  p-value: 0.1815\n\n\n-&gt; kein signifikanter Zusammenhang im einfachen linearen Modell und entsprechend kleines Bestimmtheitsmass (adj. R2 = 0.07)\n\n# Polynomische Regression Modell definieren und anschauen \nlm_quad &lt;- lm(resp ~ pred + I(pred^2)) # lineares Modell mit quadratischem Termsummary\n\nsummary(lm_quad) # lineares Modell mit quadratischem Term anschauen\n\n\nCall:\nlm(formula = resp ~ pred + I(pred^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3866 -1.1018 -0.2027  1.3831  4.4211 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -2.239308   3.811746  -0.587  0.56777   \npred         1.330933   0.360105   3.696  0.00306 **\nI(pred^2)   -0.031587   0.007504  -4.209  0.00121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.555 on 12 degrees of freedom\nMultiple R-squared:  0.6499,    Adjusted R-squared:  0.5915 \nF-statistic: 11.14 on 2 and 12 DF,  p-value: 0.001842\n\n\n-&gt; Signifikanter Zusammenhang und viel besseres Bestimmtheitsmass (adj. R2 = 0.60)\n\n# Modelle darstellen\n\n# Vorhersagen der Modelle generieren \nxv &lt;- seq(min(pred), max(pred), length = 100) # 100 x-Werte, mit denen man die Modelle \"füttern\" kann \ny_lm1 &lt;- predict(lm_1, data.frame(pred = xv)) # Vorhersagen des quadratischen Modells für die y-Werte\ny_lm_quad &lt;- predict(lm_quad, data.frame(pred = xv)) # Vorhersagen des quadratischen Modells für die y-Werte\nModPred &lt;- data.frame(xv, y_lm1, y_lm_quad)\n\n# Modellvorhersagen plotten\n\nggplot(data, aes(x = pred, y = resp)) +\n  geom_point() +\n  geom_line(data = ModPred, aes(x = xv, y = y_lm1), color = \"red\", linetype = \"dashed\") + \n  geom_line(data = ModPred, aes(x = xv, y = y_lm_quad), color = \"blue\")\n\n\n\n\n\n\n\n# Alternativ kann man die Modelle Modellvorhersagen auch direkt in ggplot rechnen\n# + geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) # Einfache Lineare Regression\n# + geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE) # Mit quadratischem Term\n\n\n# Residualplots\npar(mfrow = c(2, 2))\nplot(lm_1, main = \"Lineares Modell\")\n\n\n\n\n\n\n\nplot(lm_quad, main = \"Quadratisches  Modell\")\n\n\n\n\n\n\n\n\n-&gt; Die Plots sehen beim Modell mit quadratischem Term besser aus",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Statistik 3: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Demo.html#ancova",
    "href": "statistik/Statistik3_Demo.html#ancova",
    "title": "Statistik 3: Demo",
    "section": "ANCOVA",
    "text": "ANCOVA\nExperiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. in Abhängigkeit von der Beweidung (“Grazing” mit 2 Levels: “Grazed”, “Ungrazed”) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)\n\n# Daten einlesen und anschauen\nlibrary(\"readr\")\n\ncompensation &lt;- read_delim(\"datasets/stat/ipomopsis.csv\")\ncompensation$Grazing &lt;- as.factor(compensation$Grazing)\n\nhead(compensation)\n\n# A tibble: 6 × 3\n   Root Fruit Grazing \n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1  6.22  59.8 Ungrazed\n2  6.49  61.0 Ungrazed\n3  4.92  14.7 Ungrazed\n4  5.13  19.3 Ungrazed\n5  5.42  34.2 Ungrazed\n6  5.36  35.5 Ungrazed\n\nsummary(compensation)\n\n      Root            Fruit            Grazing  \n Min.   : 4.426   Min.   : 14.73   Grazed  :20  \n 1st Qu.: 6.083   1st Qu.: 41.15   Ungrazed:20  \n Median : 7.123   Median : 60.88                \n Mean   : 7.181   Mean   : 59.41                \n 3rd Qu.: 8.510   3rd Qu.: 76.19                \n Max.   :10.253   Max.   :116.05                \n\n# Plotten der vollständigen Daten/Information\nlibrary(\"ggplot2\")\nggplot(compensation, aes(x = Root, y = Fruit, color = Grazing)) +\n  geom_point()\n\n\n\n\n\n\n\n\n-&gt; Je grösser die Pflanze, desto grösser ihre Fruchtproduktion. -&gt; Die grössere Fruchtproduktion innerhalb der beweideten Gruppe scheint ein Resultat von unterschiedlichen Pflanzengrössen zwischen den Gruppen zu sein.\n\n# Lineare Modelle definieren und anschauen\n\naoc_1 &lt;- lm(Fruit ~ Root * Grazing, data = compensation) # Volles Modell mit Interaktion\nsummary.aov(aoc_1)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nRoot          1  16795   16795 359.968  &lt; 2e-16 ***\nGrazing       1   5264    5264 112.832 1.21e-12 ***\nRoot:Grazing  1      5       5   0.103     0.75    \nResiduals    36   1680      47                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naoc_2 &lt;- lm(Fruit ~ Grazing + Root, data = compensation) # Finales Modell ohne die (nicht signifikante) Interaktion\nsummary.aov(aoc_2) # ANOVA-Tabelle\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nGrazing      1   2910    2910   63.93 1.4e-09 ***\nRoot         1  19149   19149  420.62 &lt; 2e-16 ***\nResiduals   37   1684      46                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aoc_2) # Parameter-Tabelle\n\n\nCall:\nlm(formula = Fruit ~ Grazing + Root, data = compensation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1920  -2.8224   0.3223   3.9144  17.3290 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -127.829      9.664  -13.23 1.35e-15 ***\nGrazingUngrazed   36.103      3.357   10.75 6.11e-13 ***\nRoot              23.560      1.149   20.51  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.747 on 37 degrees of freedom\nMultiple R-squared:  0.9291,    Adjusted R-squared:  0.9252 \nF-statistic: 242.3 on 2 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Residualplots anschauen\npar(mfrow = c(2, 2)) # 2x2 Plots pro Grafik\nplot(aoc_2)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1)) # Grafik zurücksetzen\n\n-&gt; Das ANCOVA-Modell widerspiegelt die Zusammenhänge wie sie aufgrund der grafisch dargestellten Daten zu vermuten sind gut. Die Residual-Plots zeigen 3 Ausreisser (Beobachtungen 27, 34 und 37), welche “aus der Reihe tanzen”.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Statistik 3: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Uebung.html",
    "href": "statistik/Statistik3_Uebung.html",
    "title": "Statistik 3: Übung",
    "section": "",
    "text": "Lineare und polynomische Regression\nAufgabenbeschreibung\nIn R gibt es zahlreiche eingebaute Datensätze. Diese sind direkt in R verfügbar und können ohne zusätzliche Downloads oder Installationen genutzt werden. In dieser Übung verwenden wir den Datensatz LifeCycleSavings. Du kannst den Datensatz z.B. mit folgendem Befehl als Objekt laden:\nDataUebung3 &lt;- LifeCycleSavings\nDer Datensatz enthält Informationen über die Sparquoten in 50 Ländern:\nDu sollst nun untersuchen, ob und wie Sparquote von den anderen drei Variablen abhängt. Teste für jede erklärende Variable einzeln auf lineare und quadratische Zusammenhänge. Stelle die Ergebnisse grafisch dar und verfasse einen ausformulierten Methoden- und Ergebnisteil.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistik 3: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Uebung.html#lineare-und-polynomische-regression",
    "href": "statistik/Statistik3_Uebung.html#lineare-und-polynomische-regression",
    "title": "Statistik 3: Übung",
    "section": "Modeldiagnostik",
    "text": "sr Sparquote (Savings Ratio; Anteil des Einkommens der gespart wird in Prozent)\npop15 Prozentsatz der Bevölkerung unter 15 Jahren\npop75 Prozentsatz der Bevölkerung über 75 Jahren\ndpi Pro-Kopf-Einkommen\n\n\n\n\n\n\n\n\nLösung Übung 3\n\n\n\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n\nLösungstext als Download\n\n\nR-Session vorbereiten\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\nDaten anschauen\n\np1 &lt;- ggplot(LifeCycleSavings, aes(x = pop15, y = sr)) +\n  geom_point()\np2 &lt;- ggplot(LifeCycleSavings, aes(x = pop75, y = sr)) +\n  geom_point()\np3 &lt;- ggplot(LifeCycleSavings, aes(x = dpi, y = sr)) +\n  geom_point()\n\np1+p2+p3\n\n\n\n\n\n\n\n\n-&gt; Zusammenhänge sind zu erahnen, aber die Streuung ist gross.\n\nModelle erstellen\n\nlm_pop15 &lt;- lm(sr ~ pop15 , data = LifeCycleSavings ) # Einfaches lineares Modell\nlm_q_pop15 &lt;- lm(sr ~ pop15 + I(pop15^2),  data = LifeCycleSavings) # Modell mit quadratischem Term\n\n\nlm_pop75 &lt;- lm(sr ~ pop75, data = LifeCycleSavings ) # Einfaches lineares Modell\nlm_q_pop75 &lt;- lm(sr ~ pop75 + I(pop75^2),  data = LifeCycleSavings) # Modell mit quadratischem Term\n\nlm_dpi &lt;- lm(sr ~ dpi , data = LifeCycleSavings ) # Einfaches lineares Modell\nlm_q_dpi &lt;- lm(sr ~ dpi + I(dpi^2),  data = LifeCycleSavings) # Modell mit quadratischem Term\n\n\n\nModell-Outputs anschauen\n\nlm_pop15 |&gt; summary()\n\n\nCall:\nlm(formula = sr ~ pop15, data = LifeCycleSavings)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.637 -2.374  0.349  2.022 11.155 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.49660    2.27972   7.675 6.85e-10 ***\npop15       -0.22302    0.06291  -3.545 0.000887 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.03 on 48 degrees of freedom\nMultiple R-squared:  0.2075,    Adjusted R-squared:  0.191 \nF-statistic: 12.57 on 1 and 48 DF,  p-value: 0.0008866\n\nlm_q_pop15 |&gt; summary()\n\n\nCall:\nlm(formula = sr ~ pop15 + I(pop15^2), data = LifeCycleSavings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2918 -2.6062  0.2732  1.8186 11.0642 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 22.399363  13.358692   1.677    0.100\npop15       -0.522473   0.806247  -0.648    0.520\nI(pop15^2)   0.004268   0.011455   0.373    0.711\n\nResidual standard error: 4.067 on 47 degrees of freedom\nMultiple R-squared:  0.2098,    Adjusted R-squared:  0.1762 \nF-statistic: 6.241 on 2 and 47 DF,  p-value: 0.003946\n\n\n-&gt; Ein linearer negativer Zusammenhang ist vorhanden.\n-&gt; Kein quadratischer Zusammenhang.\n\nlm_pop75 |&gt; summary() \n\n\nCall:\nlm(formula = sr ~ pop75, data = LifeCycleSavings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2657 -3.2295  0.0543  2.3336 11.8498 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.1517     1.2475   5.733  6.4e-07 ***\npop75         1.0987     0.4753   2.312   0.0251 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.294 on 48 degrees of freedom\nMultiple R-squared:  0.1002,    Adjusted R-squared:  0.08144 \nF-statistic: 5.344 on 1 and 48 DF,  p-value: 0.02513\n\nlm_q_pop75 |&gt; summary() \n\n\nCall:\nlm(formula = sr ~ pop75 + I(pop75^2), data = LifeCycleSavings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.6125 -3.2081  0.1644  2.2641 11.5023 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   5.9658     2.4752   2.410   0.0199 *\npop75         2.3997     2.3879   1.005   0.3201  \nI(pop75^2)   -0.2608     0.4690  -0.556   0.5808  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.325 on 47 degrees of freedom\nMultiple R-squared:  0.1061,    Adjusted R-squared:  0.06803 \nF-statistic: 2.788 on 2 and 47 DF,  p-value: 0.07172\n\n\n-&gt; Ein linearer postiviver Zusammenhang ist vorhanden.\n-&gt; Kein quadratischer Zusammenhang.\n\nlm_dpi |&gt; summary()\n\n\nCall:\nlm(formula = sr ~ dpi, data = LifeCycleSavings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.1915 -3.6215  0.4418  2.8304 11.2790 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8.5682306  0.9414690   9.101 5.04e-12 ***\ndpi         0.0009964  0.0006366   1.565    0.124    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.416 on 48 degrees of freedom\nMultiple R-squared:  0.04856,   Adjusted R-squared:  0.02874 \nF-statistic:  2.45 on 1 and 48 DF,  p-value: 0.1241\n\nlm_q_dpi |&gt; summary() \n\n\nCall:\nlm(formula = sr ~ dpi + I(dpi^2), data = LifeCycleSavings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.663  -2.488   0.032   2.535  11.071 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.786e+00  1.207e+00   5.623 9.98e-07 ***\ndpi          5.263e-03  2.008e-03   2.620   0.0118 *  \nI(dpi^2)    -1.344e-06  6.028e-07  -2.230   0.0305 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.243 on 47 degrees of freedom\nMultiple R-squared:  0.1396,    Adjusted R-squared:  0.103 \nF-statistic: 3.813 on 2 and 47 DF,  p-value: 0.02919\n\n\n-&gt; Kein einfacher linearer Zusammenhang.\n-&gt; Ein quadratischer Zusammenhang ist vorhanden.\n\n\nModeldiagnostik\n\n# Residualplots\npar(mfrow = c(2, 2))\nplot(lm_pop15)\n\n\n\n\n\n\n\nplot(lm_q_pop15)\n\n\n\n\n\n\n\nplot(lm_q_dpi)\n\n\n\n\n\n\n\n\n\n\nDarstellung der Ergebnisse\n\n# Erstellen der Plots\np1 &lt;- ggplot(LifeCycleSavings, aes(x = pop15, y = sr)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  labs(x = \"% der Bevölkerung &lt; 15 Jahre\",\n       y = \"Sparquote\") +\n  theme_classic()\n\np2 &lt;- ggplot(LifeCycleSavings, aes(x = pop75, y = sr)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  labs(x = \"% der Bevölkerung &gt; 75 Jahren\",\n       y = \"Sparquote\") +\n  theme_classic()\n\np3 &lt;- ggplot(LifeCycleSavings, aes(x = dpi, y = sr)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE) +\n  labs(x = \"Pro-Kopf-Einkommen\",\n       y = \"Sparquote\") +\n  theme_classic()\n\np1 + p2 + p3",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistik 3: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Uebung.html#modeldiagnostik",
    "href": "statistik/Statistik3_Uebung.html#modeldiagnostik",
    "title": "Statistik 3: Übung",
    "section": "",
    "text": "# Residualplots\npar(mfrow = c(2, 2))\nplot(lm_pop15)\n\n\n\n\n\n\n\nplot(lm_q_pop15)\n\n\n\n\n\n\n\nplot(lm_q_dpi)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistik 3: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik3_Uebung.html#darstellung-der-ergebnisse",
    "href": "statistik/Statistik3_Uebung.html#darstellung-der-ergebnisse",
    "title": "Statistik 3: Übung",
    "section": "Darstellung der Ergebnisse",
    "text": "Darstellung der Ergebnisse\n\n# Erstellen der Plots\np1 &lt;- ggplot(LifeCycleSavings, aes(x = pop15, y = sr)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  labs(x = \"% der Bevölkerung &lt; 15 Jahre\",\n       y = \"Sparquote\") +\n  theme_classic()\n\np2 &lt;- ggplot(LifeCycleSavings, aes(x = pop75, y = sr)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  labs(x = \"% der Bevölkerung &gt; 75 Jahren\",\n       y = \"Sparquote\") +\n  theme_classic()\n\np3 &lt;- ggplot(LifeCycleSavings, aes(x = dpi, y = sr)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE) +\n  labs(x = \"Pro-Kopf-Einkommen\",\n       y = \"Sparquote\") +\n  theme_classic()\n\np1 + p2 + p3",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistik 3: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik4_Demo.html",
    "href": "statistik/Statistik4_Demo.html",
    "title": "Statistik 4: Demo",
    "section": "",
    "text": "Multiple lineare Regression\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n# Daten einlesen und anschauen\nlibrary(pacman)\np_load(\"readr\")\n\nloyn &lt;- read_delim(\"datasets/stat/loyn.csv\", delim = \";\")\n\nstr(loyn)\n\nspc_tbl_ [54 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ABUND: num [1:54] 5.3 2 1.5 17.1 13.8 14.1 3.8 2.2 3.3 3 ...\n $ AREA : num [1:54] 0.1 0.5 0.5 1 1 1 1 1 1 1 ...\n $ AGE  : num [1:54] 16 64 84 18 66 19 29 64 19 84 ...\n $ DIST : num [1:54] 39 234 104 66 246 234 467 284 156 311 ...\n $ LDIST: num [1:54] 39 234 311 66 246 ...\n $ GRAZE: num [1:54] 2 5 5 3 5 3 5 5 4 5 ...\n $ ALT  : num [1:54] 160 60 140 160 140 130 90 60 130 130 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ABUND = col_double(),\n  ..   AREA = col_double(),\n  ..   AGE = col_double(),\n  ..   DIST = col_double(),\n  ..   LDIST = col_double(),\n  ..   GRAZE = col_double(),\n  ..   ALT = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(loyn)\n\n     ABUND            AREA             AGE             DIST       \n Min.   : 1.50   Min.   :  0.10   Min.   : 8.00   Min.   :  26.0  \n 1st Qu.:11.80   1st Qu.:  2.00   1st Qu.:18.00   1st Qu.:  93.0  \n Median :20.40   Median :  7.00   Median :21.50   Median : 221.0  \n Mean   :19.12   Mean   : 21.02   Mean   :34.31   Mean   : 236.9  \n 3rd Qu.:27.75   3rd Qu.: 25.00   3rd Qu.:57.50   3rd Qu.: 311.0  \n Max.   :39.60   Max.   :144.00   Max.   :94.00   Max.   :1427.0  \n     LDIST            GRAZE            ALT       \n Min.   :  26.0   Min.   :1.000   Min.   : 60.0  \n 1st Qu.: 156.8   1st Qu.:2.000   1st Qu.:120.0  \n Median : 338.5   Median :3.000   Median :140.0  \n Mean   : 729.8   Mean   :3.056   Mean   :143.3  \n 3rd Qu.: 854.0   3rd Qu.:4.000   3rd Qu.:175.0  \n Max.   :4426.0   Max.   :5.000   Max.   :220.0",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistik 4: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik4_Demo.html#multiple-lineare-regression",
    "href": "statistik/Statistik4_Demo.html#multiple-lineare-regression",
    "title": "Statistik 4: Demo",
    "section": "",
    "text": "Korrelation zwischen den Prädiktoren\n\n# Wir setzen die Schwelle bei |0.7|\n# Korrelationen rechnen details siehe: \"?cor\"\ncor(loyn[, 2:7]) \n\n             AREA         AGE       DIST       LDIST       GRAZE         ALT\nAREA   1.00000000 -0.21265343  0.2475258  0.37733668 -0.53118408  0.08935845\nAGE   -0.21265343  1.00000000 -0.1132931  0.09930812  0.66129235 -0.27242916\nDIST   0.24752583 -0.11329311  1.0000000  0.31814676 -0.24330458 -0.15112326\nLDIST  0.37733668  0.09930812  0.3181468  1.00000000 -0.02373893 -0.32359264\nGRAZE -0.53118408  0.66129235 -0.2433046 -0.02373893  1.00000000 -0.35362007\nALT    0.08935845 -0.27242916 -0.1511233 -0.32359264 -0.35362007  1.00000000\n\n# oder mit Namen der columns resp. variablen\ncor1 &lt;- \n  loyn |&gt;\n  subset(select = AREA:ALT) |&gt;\n  cor()\n\n# Korrelationen Visualisieren\np_load(\"corrplot\")\ncorrplot.mixed(cor1, lower = \"ellipse\", upper = \"number\", order = \"AOE\")\n\n\n\n\n\n\n\ncor1[abs(cor1)&lt;0.7] &lt;- 0\ncor1\n\n      AREA AGE DIST LDIST GRAZE ALT\nAREA     1   0    0     0     0   0\nAGE      0   1    0     0     0   0\nDIST     0   0    1     0     0   0\nLDIST    0   0    0     1     0   0\nGRAZE    0   0    0     0     1   0\nALT      0   0    0     0     0   1\n\n\n-&gt; Keine Korrelation ist &gt;|0.7|, so können wir alle Prädiktoren “behalten”. Aber es gilt zu beachten, dass GRAZE ziemlich stark |&gt;0.6| mit AGE korreliert ist\n\n# Volles Modell definieren\nnames(loyn)\n\n[1] \"ABUND\" \"AREA\"  \"AGE\"   \"DIST\"  \"LDIST\" \"GRAZE\" \"ALT\"  \n\nlm_1 &lt;- lm(ABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\n\npar(mfrow = c(2, 2))\nplot(lm_1)\n\n\n\n\n\n\n\n\n-&gt; Plot sieht zwar OK aus, aber mit 6 Prädiktoren für |&lt;60| Beobachtungen ist das Modell wohl “overfitted”\n\n# Andere Variante, um korrelierte Prädiktoren zu finden (üblicher Schwellenwert VIF = 5)\np_load(\"car\")\nvif(lm_1)\n\n     AGE     AREA     DIST    LDIST    GRAZE      ALT \n1.874993 1.763605 1.220125 1.465810 2.784577 1.346572 \n\n\n\n\nModellselektion\n\nsummary(lm_1)\n\n\nCall:\nlm(formula = ABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT, \n    data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.8828  -4.4751   0.5753   4.5738  18.1946 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  1.749e+01  6.599e+00   2.650   0.0109 * \nAGE         -9.155e-02  5.430e-02  -1.686   0.0985 . \nAREA         1.232e-01  4.173e-02   2.953   0.0049 **\nDIST         3.751e-03  5.083e-03   0.738   0.4642   \nLDIST       -5.331e-05  1.335e-03  -0.040   0.9683   \nGRAZE       -1.783e+00  1.181e+00  -1.510   0.1378   \nALT          4.731e-02  2.900e-02   1.631   0.1095   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.451 on 47 degrees of freedom\nMultiple R-squared:  0.5722,    Adjusted R-squared:  0.5176 \nF-statistic: 10.48 on 6 and 47 DF,  p-value: 2.344e-07\n\ndrop1(lm_1, test = \"F\")\n\nSingle term deletions\n\nModel:\nABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT\n       Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)   \n&lt;none&gt;              2609.5 223.41                    \nAGE     1    157.79 2767.3 224.58  2.8419 0.098466 . \nAREA    1    484.08 3093.6 230.60  8.7187 0.004904 **\nDIST    1     30.24 2639.8 222.03  0.5447 0.464169   \nLDIST   1      0.09 2609.6 221.41  0.0016 0.968322   \nGRAZE   1    126.56 2736.1 223.97  2.2794 0.137794   \nALT     1    147.76 2757.3 224.38  2.6612 0.109504   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Prädiktor mit grösstem p-Wert entfernen\nlm_2 &lt;- lm(ABUND ~ AGE + AREA + DIST  + GRAZE + ALT, data = loyn)\n# oder\nlm_2 &lt;- update(lm_1, ~ . - LDIST) \n\n# Oben beschriebener Schritt wiederholten bis nur noch signifikante Prädiktoren im Modell sind\ndrop1(lm_2, test = \"F\") \n\nSingle term deletions\n\nModel:\nABUND ~ AGE + AREA + DIST + GRAZE + ALT\n       Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)   \n&lt;none&gt;              2609.6 221.41                    \nAGE     1    158.71 2768.3 222.60  2.9192 0.093989 . \nAREA    1    563.32 3172.9 229.97 10.3614 0.002309 **\nDIST    1     31.10 2640.7 220.05  0.5721 0.453139   \nGRAZE   1    127.97 2737.6 222.00  2.3539 0.131535   \nALT     1    163.04 2772.6 222.68  2.9988 0.089749 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlm_3 &lt;- update(lm_2, ~ . - DIST)\n\ndrop1(lm_3, test = \"F\") \n\nSingle term deletions\n\nModel:\nABUND ~ AGE + AREA + GRAZE + ALT\n       Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)   \n&lt;none&gt;              2640.7 220.05                    \nAGE     1    154.79 2795.5 221.13  2.8722 0.096468 . \nAREA    1    599.14 3239.8 229.09 11.1174 0.001635 **\nGRAZE   1    158.71 2799.4 221.20  2.9449 0.092467 . \nALT     1    138.17 2778.9 220.80  2.5639 0.115759   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlm_4 &lt;- update(lm_3, ~ . - ALT)\n\ndrop1(lm_4, test = \"F\") \n\nSingle term deletions\n\nModel:\nABUND ~ AGE + AREA + GRAZE\n       Df Sum of Sq    RSS    AIC F value  Pr(&gt;F)   \n&lt;none&gt;              2778.9 220.80                   \nAGE     1    163.46 2942.3 221.89  2.9412 0.09254 . \nAREA    1    541.87 3320.8 228.42  9.7497 0.00298 **\nGRAZE   1    264.51 3043.4 223.71  4.7593 0.03387 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlm_5 &lt;- update(lm_4, ~ . - AGE)\n\ndrop1(lm_5, test = \"F\")\n\nSingle term deletions\n\nModel:\nABUND ~ AREA + GRAZE\n       Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;              2942.3 221.89                      \nAREA    1    440.79 3383.1 227.43  7.6403  0.007923 ** \nGRAZE   1   1089.71 4032.1 236.91 18.8881 6.622e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm_5) \n\n\nCall:\nlm(formula = ABUND ~ AREA + GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5390  -6.3337   0.1902   4.4737  15.5567 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.2303     3.2660   8.644 1.45e-11 ***\nAREA          0.1045     0.0378   2.764  0.00792 ** \nGRAZE        -3.7009     0.8516  -4.346 6.62e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.596 on 51 degrees of freedom\nMultiple R-squared:  0.5177,    Adjusted R-squared:  0.4987 \nF-statistic: 27.37 on 2 and 51 DF,  p-value: 8.425e-09\n\npar(mfrow = c(2, 2))\nplot(lm_5)\n\n\n\n\n\n\n\n\n-&gt; das minimal adäquate Modell enthält noch zwei Prädiktoren (AREA; GRAZE) und dessen Residualplots sehen ok aus.\n\n\nHierarchical partitioning\nWir können auch schauen wie bedeutsam die einzelnen Variablen sind:\n\np_load(\"relaimpo\")\n\n# Berechnen\nmetrics &lt;- calc.relimp(lm_1, type = c(\"lmg\", \"first\"))\ncbind(I = metrics$lmg, J = metrics$first - metrics$lmg, Total = metrics$first)\n\n               I             J       Total\nAGE   0.11351597  0.1539730784 0.267489048\nAREA  0.17941694  0.1596031200 0.339020063\nDIST  0.01986977  0.0307746481 0.050644413\nLDIST 0.00827103 -0.0007561283 0.007514902\nGRAZE 0.19052943  0.2548693094 0.445398737\nALT   0.06061495  0.0610176713 0.121632624\n\n\n-&gt; auch hier haben AREA und GRAZE die höchsten Werte (und an dritter Stelle AGE, der mit GRAZE am stärksten korreliert ist)\n\n\nPlot partielle regressionen\n\n# Beispiel GRAZE\nlm_abund &lt;- lm(ABUND ~ AREA, data = loyn)\nlm_graze &lt;- lm(GRAZE ~ AREA, data = loyn)\n\nabundance_resid &lt;- resid(lm_abund)\ngraze_resid &lt;- resid(lm_graze)\n\np_load(ggplot2)\n\nggplot(data = NULL, aes(x = graze_resid, y = abundance_resid)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Graze | others\", y = \"Abund | others\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Einfacher geht es mit der function avPlots (package \"car\"). Nachteil ist, dass mit der funktion anders als mit der Methode oben, keine quadratische prädiktoren dargestellt werden können \n \npar(mfrow = c(1, 1))\navPlots(lm_5, ~GRAZE, ask = F)\n\n\n\n\n\n\n\n#Für alle prädktoren im Modell\navPlots(lm_5, layout = c(1, 2) )",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistik 4: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik4_Demo.html#multimodel-inference",
    "href": "statistik/Statistik4_Demo.html#multimodel-inference",
    "title": "Statistik 4: Demo",
    "section": "Multimodel inference",
    "text": "Multimodel inference\n\np_load(\"MuMIn\")\n\nglobal_model &lt;- lm(ABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\n\noptions(na.action = \"na.fail\")\nallmodels &lt;- dredge(global_model)\nallmodels\n\nGlobal model call: lm(formula = ABUND ~ AGE + AREA + DIST + LDIST + GRAZE + ALT, \n    data = loyn)\n---\nModel selection table \n   (Intrc)      AGE     ALT   AREA     DIST  GRAZE      LDIST df   logLik  AICc\n24  19.460 -0.09049 0.04249 0.1257          -1.954             6 -181.648 377.1\n22  27.250 -0.09295         0.1187          -2.434             5 -183.025 377.3\n23  20.180          0.04379 0.1120          -3.172             5 -183.186 377.6\n8   13.020 -0.14830 0.05448 0.1607                             5 -183.224 377.7\n21  28.230                  0.1045          -3.701             4 -184.568 378.0\n16  10.990 -0.14310 0.06018 0.1522 0.005130                    6 -182.621 379.0\n32  17.440 -0.09167 0.04764 0.1226 0.003705 -1.787             7 -181.328 379.1\n31  18.300          0.04861 0.1090 0.003463 -3.031             6 -182.922 379.6\n54  27.240 -0.09146         0.1257          -2.375 -4.979e-04  6 -182.936 379.7\n56  19.240 -0.09093 0.04375 0.1235          -1.960  1.699e-04  7 -181.638 379.7\n30  26.780 -0.09359         0.1170 0.001608 -2.386             6 -182.964 379.7\n6   21.570 -0.17130         0.1629                             4 -185.480 379.8\n53  28.190                  0.1137          -3.600 -6.372e-04  5 -184.429 380.1\n55  20.130          0.04406 0.1116          -3.174  3.620e-05  6 -183.185 380.2\n40  12.900 -0.14860 0.05513 0.1596                  8.597e-05  6 -183.221 380.2\n29  27.850                  0.1030 0.001316 -3.669             5 -184.530 380.3\n48  11.230 -0.14210 0.05871 0.1546 0.005306        -2.191e-04  7 -182.606 381.6\n38  21.780 -0.16570         0.1728                 -8.285e-04  5 -185.250 381.7\n14  20.930 -0.16970         0.1583 0.002886                    5 -185.298 381.8\n64  17.490 -0.09155 0.04731 0.1232 0.003751 -1.783 -5.331e-05  8 -181.327 381.9\n62  26.540 -0.09188         0.1256 0.002372 -2.285 -6.734e-04  7 -182.814 382.1\n63  18.450          0.04749 0.1112 0.003619 -3.012 -1.800e-04  7 -182.912 382.3\n61  27.540                  0.1136 0.002233 -3.521 -8.030e-04  6 -184.327 382.4\n17  34.250                                  -4.951             3 -188.337 383.2\n46  20.950 -0.16160         0.1696 0.004052        -1.107e-03  6 -184.914 383.6\n19  28.380          0.03376                 -4.613             4 -187.620 384.1\n18  34.150 -0.05604                         -4.290             4 -187.835 384.5\n51  24.960          0.04669                 -4.461  1.504e-03  5 -186.871 385.0\n49  33.610                                  -4.939  8.194e-04  4 -188.092 385.0\n25  33.120                         0.003213 -4.831             4 -188.133 385.1\n27  25.260          0.04131        0.005130 -4.345             5 -187.120 385.5\n20  28.540 -0.05253 0.03230                 -4.008             5 -187.169 385.6\n50  33.340 -0.06356                         -4.185  1.023e-03  5 -187.453 386.2\n52  24.690 -0.06351 0.04667                 -3.708  1.707e-03  6 -186.204 386.2\n26  32.900 -0.05863                0.003557 -4.126             5 -187.581 386.4\n28  25.270 -0.05561 0.04017        0.005404 -3.690             6 -186.605 387.0\n59  23.190          0.05027        0.003783 -4.287  1.273e-03  6 -186.611 387.0\n7    4.042          0.07835 0.1830                             4 -189.182 387.2\n57  32.930                         0.002318 -4.855  6.458e-04  5 -187.996 387.2\n15   1.681          0.08504 0.1703 0.007048                    5 -188.251 387.8\n60  22.870 -0.06421 0.05035        0.003888 -3.521  1.472e-03  7 -185.922 388.3\n58  32.630 -0.06400                0.002421 -4.092  8.429e-04  6 -187.346 388.5\n39   4.728          0.07497 0.1878                 -4.146e-04  5 -189.137 389.5\n47   2.837          0.07890 0.1788 0.007662        -8.235e-04  6 -188.077 389.9\n5   15.090                  0.1918                             3 -193.075 392.6\n37  15.990                  0.2110                 -1.793e-03  4 -192.219 393.3\n45  14.910                  0.2046 0.006347        -2.192e-03  5 -191.569 394.4\n13  14.250                  0.1847 0.004158                    4 -192.789 394.4\n36  12.550 -0.19070 0.07799                         2.643e-03  5 -192.165 395.6\n12  12.580 -0.17450 0.07003        0.010510                    5 -192.232 395.7\n44  10.040 -0.17890 0.08236        0.008064         2.057e-03  6 -191.118 396.0\n4   17.190 -0.18960 0.05881                                    4 -194.072 397.0\n2   26.500 -0.21500                                            3 -195.849 398.2\n10  24.300 -0.20710                0.008133                    4 -194.795 398.4\n34  25.520 -0.22070                                 1.612e-03  4 -195.130 399.1\n42  24.050 -0.21240                0.006624         1.084e-03  5 -194.496 400.2\n11   1.164          0.10260        0.013710                    4 -198.209 405.2\n43  -1.215          0.11370        0.011720         1.732e-03  5 -197.580 406.4\n35   1.436          0.11030                         2.577e-03  4 -199.349 407.5\n3    6.024          0.09136                                    3 -200.752 408.0\n9   16.550                         0.010850                    3 -202.851 412.2\n1   19.120                                                     2 -204.254 412.7\n41  16.470                         0.010600         1.941e-04  4 -202.843 414.5\n33  18.390                                          1.002e-03  3 -204.050 414.6\n   delta weight\n24  0.00  0.127\n22  0.22  0.114\n23  0.54  0.097\n8   0.61  0.093\n21  0.87  0.082\n16  1.95  0.048\n32  2.01  0.046\n31  2.55  0.035\n54  2.58  0.035\n56  2.63  0.034\n30  2.63  0.034\n6   2.69  0.033\n53  3.03  0.028\n55  3.08  0.027\n40  3.15  0.026\n29  3.23  0.025\n48  4.56  0.013\n38  4.67  0.012\n14  4.76  0.012\n64  4.77  0.012\n62  4.98  0.011\n63  5.18  0.010\n61  5.36  0.009\n17  6.07  0.006\n46  6.53  0.005\n19  6.97  0.004\n18  7.40  0.003\n51  7.91  0.002\n49  7.92  0.002\n25  8.00  0.002\n27  8.41  0.002\n20  8.50  0.002\n50  9.07  0.001\n52  9.11  0.001\n26  9.33  0.001\n28  9.91  0.001\n59  9.93  0.001\n7  10.10  0.001\n57 10.16  0.001\n15 10.67  0.001\n60 11.20  0.000\n58 11.40  0.000\n39 12.44  0.000\n47 12.86  0.000\n5  15.55  0.000\n37 16.17  0.000\n45 17.30  0.000\n13 17.31  0.000\n36 18.50  0.000\n12 18.63  0.000\n44 18.94  0.000\n4  19.88  0.000\n2  21.10  0.000\n10 21.32  0.000\n34 21.99  0.000\n42 23.16  0.000\n11 28.15  0.000\n43 29.33  0.000\n35 30.43  0.000\n3  30.90  0.000\n9  35.10  0.000\n1  35.66  0.000\n41 37.42  0.000\n33 37.50  0.000\nModels ranked by AICc(x) \n\n# Wir haben mehre Modelle mit einem delta AICc &lt;2, das heisst wir haben nicht ein eindeutig bestes Modell (welches wir mit der funktion \"get.models\" selektieren könnten)\n\n# Variable importance\nsw(allmodels)\n\n                     AREA GRAZE AGE  ALT  DIST LDIST\nSum of weights:      0.97 0.76  0.66 0.58 0.27 0.23 \nN containing models:   32   32    32   32   32   32 \n\n\n-&gt; Auch mit dieser Sichtweise sind AREA und GRAZE die wichtigste Prädiktoren\n\n# Model averaging\navgmodel &lt;- model.avg(allmodels)\nsummary(avgmodel)\n\n\nCall:\nmodel.avg(object = allmodels)\n\nComponent model call: \nlm(formula = ABUND ~ &lt;64 unique rhs&gt;, data = loyn)\n\nComponent models: \n       df  logLik   AICc delta weight\n1235    6 -181.65 377.08  0.00   0.13\n135     5 -183.02 377.30  0.22   0.11\n235     5 -183.19 377.62  0.54   0.10\n123     5 -183.22 377.70  0.61   0.09\n35      4 -184.57 377.95  0.87   0.08\n1234    6 -182.62 379.03  1.95   0.05\n12345   7 -181.33 379.09  2.01   0.05\n2345    6 -182.92 379.63  2.55   0.04\n1356    6 -182.94 379.66  2.58   0.03\n12356   7 -181.64 379.71  2.63   0.03\n1345    6 -182.96 379.72  2.63   0.03\n13      4 -185.48 379.78  2.69   0.03\n356     5 -184.43 380.11  3.03   0.03\n2356    6 -183.19 380.16  3.08   0.03\n1236    6 -183.22 380.23  3.15   0.03\n345     5 -184.53 380.31  3.23   0.03\n12346   7 -182.61 381.65  4.56   0.01\n136     5 -185.25 381.75  4.67   0.01\n134     5 -185.30 381.85  4.76   0.01\n123456  8 -181.33 381.85  4.77   0.01\n13456   7 -182.81 382.06  4.98   0.01\n23456   7 -182.91 382.26  5.18   0.01\n3456    6 -184.33 382.44  5.36   0.01\n5       3 -188.34 383.15  6.07   0.01\n1346    6 -184.91 383.61  6.53   0.00\n25      4 -187.62 384.06  6.97   0.00\n15      4 -187.83 384.49  7.40   0.00\n256     5 -186.87 384.99  7.91   0.00\n56      4 -188.09 385.00  7.92   0.00\n45      4 -188.13 385.08  8.00   0.00\n245     5 -187.12 385.49  8.41   0.00\n125     5 -187.17 385.59  8.50   0.00\n156     5 -187.45 386.16  9.07   0.00\n1256    6 -186.20 386.19  9.11   0.00\n145     5 -187.58 386.41  9.33   0.00\n1245    6 -186.61 387.00  9.91   0.00\n2456    6 -186.61 387.01  9.93   0.00\n23      4 -189.18 387.18 10.10   0.00\n456     5 -188.00 387.24 10.16   0.00\n234     5 -188.25 387.75 10.67   0.00\n12456   7 -185.92 388.28 11.20   0.00\n1456    6 -187.35 388.48 11.40   0.00\n236     5 -189.14 389.52 12.44   0.00\n2346    6 -188.08 389.94 12.86   0.00\n3       3 -193.07 392.63 15.55   0.00\n36      4 -192.22 393.25 16.17   0.00\n346     5 -191.57 394.39 17.30   0.00\n34      4 -192.79 394.39 17.31   0.00\n126     5 -192.16 395.58 18.50   0.00\n124     5 -192.23 395.71 18.63   0.00\n1246    6 -191.12 396.02 18.94   0.00\n12      4 -194.07 396.96 19.88   0.00\n1       3 -195.85 398.18 21.10   0.00\n14      4 -194.79 398.41 21.32   0.00\n16      4 -195.13 399.08 21.99   0.00\n146     5 -194.50 400.24 23.16   0.00\n24      4 -198.21 405.24 28.15   0.00\n246     5 -197.58 406.41 29.33   0.00\n26      4 -199.35 407.51 30.43   0.00\n2       3 -200.75 407.98 30.90   0.00\n4       3 -202.85 412.18 35.10   0.00\n(Null)  2 -204.25 412.74 35.66   0.00\n46      4 -202.84 414.50 37.42   0.00\n6       3 -204.05 414.58 37.50   0.00\n\nTerm codes: \n  AGE   ALT  AREA  DIST GRAZE LDIST \n    1     2     3     4     5     6 \n\nModel-averaged coefficients:  \n(full average) \n              Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)   \n(Intercept)  2.125e+01  7.540e+00   7.618e+00   2.790  0.00528 **\nAGE         -7.527e-02  7.177e-02   7.236e-02   1.040  0.29823   \nALT          2.811e-02  3.196e-02   3.231e-02   0.870  0.38427   \nAREA         1.235e-01  4.745e-02   4.818e-02   2.564  0.01035 * \nGRAZE       -2.080e+00  1.621e+00   1.633e+00   1.273  0.20292   \nDIST         9.132e-04  3.061e-03   3.117e-03   0.293  0.76950   \nLDIST       -4.925e-05  6.691e-04   6.839e-04   0.072  0.94259   \n \n(conditional average) \n              Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)   \n(Intercept) 21.2504039  7.5395494   7.6177190   2.790  0.00528 **\nAGE         -0.1132699  0.0587077   0.0597924   1.894  0.05817 . \nALT          0.0481783  0.0280034   0.0286744   1.680  0.09292 . \nAREA         0.1275241  0.0425967   0.0434334   2.936  0.00332 **\nGRAZE       -2.7514534  1.2757684   1.2968574   2.122  0.03387 * \nDIST         0.0034010  0.0051418   0.0052641   0.646  0.51823   \nLDIST       -0.0002128  0.0013783   0.0014092   0.151  0.87997   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Nur Esimates\nsummary(avgmodel)$coefficients\n\n       (Intercept)         AGE        ALT      AREA     GRAZE         DIST\nfull       21.2504 -0.07527122 0.02811021 0.1235275 -2.079840 0.0009132418\nsubset     21.2504 -0.11326991 0.04817829 0.1275241 -2.751453 0.0034009585\n               LDIST\nfull   -4.925166e-05\nsubset -2.128051e-04\n\n# Confindence intervals\nconfint(avgmodel)\n\n                   2.5 %       97.5 %\n(Intercept)  6.319949053 36.180858723\nAGE         -0.230460847  0.003921031\nALT         -0.008022574  0.104379146\nAREA         0.042396229  0.212651977\nGRAZE       -5.293247097 -0.209659662\nDIST        -0.006916400  0.013718317\nLDIST       -0.002974830  0.002549220",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistik 4: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik4_Uebung.html",
    "href": "statistik/Statistik4_Uebung.html",
    "title": "Statistik 4: Übung",
    "section": "",
    "text": "Lesen Sie den Datensatz steprasen_ukraine.csv in R ein. Dieser enthält Pflanzenartenzahlen (Species_richness) von 199 10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind.\nErmitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots durch die Umweltvariablen erklärt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein mit Quarto generiertes html-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n) und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse geeignet?\nExplorative Datenanalyse\nDefinition eines globalen Modelles und dessen Reduktion zu einem minimal adäquaten Modell\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\n\nZu erstellen sind (1) Ein quarto generiertes html-Dokument mit begründetem Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (2) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\n\n\n\n\n\n\n\n\nLösung Übung 4\n\n\n\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n\nLösungstext als Download\n\n\nlibrary(pacman)\np_load(\"tidyverse\")\n\nukr &lt;- read_delim(\"datasets/stat/steprasen_ukraine.csv\", delim = \";\")\nstr(ukr)\n\nspc_tbl_ [199 × 20] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PlotID           : chr [1:199] \"UA01NW\" \"UA01SE\" \"UA02NW\" \"UA02SE\" ...\n $ Species_richness : num [1:199] 44 53 48 50 53 40 46 56 30 35 ...\n $ Altitude         : num [1:199] 179 178 188 183 162 165 153 158 192 197 ...\n $ Inclination      : num [1:199] 24 17 27 33 7 33 30 32 25 18 ...\n $ Heat_index       : num [1:199] -0.42 -0.3 -0.51 -0.65 -0.09 -0.42 0 -0.59 0.46 0.32 ...\n $ Microrelief      : num [1:199] 5 2.5 2 2 3 4 16 15 5 3 ...\n $ Grazing_intensity: num [1:199] 0 0 0 0 0 0 1 1 0 0 ...\n $ Litter           : num [1:199] 12 10 0 4 15 30 5 6 10 20 ...\n $ Stones_and_rocks : num [1:199] 0 0 0 0 0 0 40 10 0 0 ...\n $ Gravel           : num [1:199] 0 0 0 0 0 0 0 0 0 0 ...\n $ Fine_soil        : num [1:199] 2 5 0 7 0 0 2 5 5 2 ...\n $ pH               : num [1:199] 7.32 6.91 6.72 6.44 6.1 6.23 6.79 6.43 7.19 7 ...\n $ Conductivity     : num [1:199] 90 115 126 90 73 76 163 119 151 69 ...\n $ CaCO3            : num [1:199] 0.0754 0.1271 0.0723 0.0771 0.0829 ...\n $ N_total          : num [1:199] 0.14 0.17 0.24 0.26 0.29 0.2 0.34 0.29 0.18 0.2 ...\n $ C_org            : num [1:199] 1.54 1.97 2.99 3.22 3.77 2.5 4.59 3.67 2.16 2.38 ...\n $ CN_ratio         : num [1:199] 10.8 11.5 12.5 12.6 12.8 ...\n $ Temperature      : num [1:199] 79 79 80 80 82 82 82 82 79 83 ...\n $ Temperature_range: num [1:199] 330 330 329 329 328 328 328 328 326 327 ...\n $ Precipitation    : num [1:199] 608 608 603 603 594 594 594 594 600 586 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PlotID = col_character(),\n  ..   Species_richness = col_double(),\n  ..   Altitude = col_double(),\n  ..   Inclination = col_double(),\n  ..   Heat_index = col_double(),\n  ..   Microrelief = col_double(),\n  ..   Grazing_intensity = col_double(),\n  ..   Litter = col_double(),\n  ..   Stones_and_rocks = col_double(),\n  ..   Gravel = col_double(),\n  ..   Fine_soil = col_double(),\n  ..   pH = col_double(),\n  ..   Conductivity = col_double(),\n  ..   CaCO3 = col_double(),\n  ..   N_total = col_double(),\n  ..   C_org = col_double(),\n  ..   CN_ratio = col_double(),\n  ..   Temperature = col_double(),\n  ..   Temperature_range = col_double(),\n  ..   Precipitation = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nMan erkennt, dass alle Spalten bis auf die erste mit der Plot ID numerisch (num) sind. Wir können nun die Variable “PlotID” als rownames setzten\n\n# Setze erste Spalte (colum) als rownames\nukr &lt;- ukr |&gt;\n  column_to_rownames(var = \"PlotID\")\n\nNun steht die abhängige Variable in Spalte 1 und die Prediktorvariablen in den Spalten 2 bis 19 stehen.\n\nsummary(ukr)\n\n Species_richness    Altitude      Inclination      Heat_index      \n Min.   :14.00    Min.   : 73.0   Min.   : 1.00   Min.   :-0.94000  \n 1st Qu.:34.00    1st Qu.:140.0   1st Qu.:12.00   1st Qu.:-0.15500  \n Median :40.00    Median :166.0   Median :19.00   Median : 0.01000  \n Mean   :40.23    Mean   :161.7   Mean   :19.28   Mean   : 0.01603  \n 3rd Qu.:47.50    3rd Qu.:188.0   3rd Qu.:25.00   3rd Qu.: 0.21500  \n Max.   :67.00    Max.   :251.0   Max.   :48.00   Max.   : 0.85000  \n  Microrelief      Grazing_intensity     Litter      Stones_and_rocks\n Min.   :  0.000   Min.   :0.0000    Min.   : 0.00   Min.   : 0.000  \n 1st Qu.:  2.500   1st Qu.:0.0000    1st Qu.: 3.50   1st Qu.: 0.000  \n Median :  5.000   Median :1.0000    Median : 7.00   Median : 0.500  \n Mean   :  7.126   Mean   :0.9296    Mean   :12.16   Mean   : 3.994  \n 3rd Qu.:  7.000   3rd Qu.:2.0000    3rd Qu.:13.50   3rd Qu.: 4.000  \n Max.   :100.000   Max.   :3.0000    Max.   :90.00   Max.   :68.000  \n     Gravel         Fine_soil           pH         Conductivity  \n Min.   : 0.000   Min.   : 0.00   Min.   :4.890   Min.   : 40.0  \n 1st Qu.: 0.000   1st Qu.: 2.00   1st Qu.:7.240   1st Qu.:148.5  \n Median : 0.000   Median : 5.00   Median :7.420   Median :171.0  \n Mean   : 2.984   Mean   : 7.02   Mean   :7.286   Mean   :162.3  \n 3rd Qu.: 3.000   3rd Qu.:10.00   3rd Qu.:7.545   3rd Qu.:189.5  \n Max.   :40.000   Max.   :38.00   Max.   :7.790   Max.   :232.0  \n     CaCO3            N_total           C_org           CN_ratio    \n Min.   : 0.0042   Min.   :0.0700   Min.   : 1.040   Min.   : 6.04  \n 1st Qu.: 0.4306   1st Qu.:0.2000   1st Qu.: 2.850   1st Qu.:12.24  \n Median : 4.6578   Median :0.2700   Median : 3.560   Median :12.95  \n Mean   : 7.4757   Mean   :0.2788   Mean   : 3.689   Mean   :13.48  \n 3rd Qu.:13.0002   3rd Qu.:0.3300   3rd Qu.: 4.400   3rd Qu.:14.02  \n Max.   :35.2992   Max.   :0.9500   Max.   :11.300   Max.   :27.42  \n  Temperature    Temperature_range Precipitation  \n Min.   :78.00   Min.   :326.0     Min.   :577.0  \n 1st Qu.:82.00   1st Qu.:328.0     1st Qu.:583.0  \n Median :84.00   Median :329.0     Median :592.0  \n Mean   :84.82   Mean   :328.6     Mean   :596.4  \n 3rd Qu.:88.00   3rd Qu.:330.0     3rd Qu.:602.5  \n Max.   :92.00   Max.   :331.0     Max.   :630.0  \n\n\nDa es sich bei Artenzahlen um Zähldaten handelt, müsste man theoretisch ein glm mit Poisson-Verteilung rechnen; bei einem Mittelwert, der hinreichend von Null verschieden ist (hier: ca. 40), ist eine Poisson-Verteilung aber praktisch nicht von einer Normalverteilung zu unterscheiden und wir können uns den Aufwand auch sparen).\n\ncor &lt;- cor(ukr[, 2:19])\ncor[abs(cor)&lt;0.7] &lt;- 0\ncor\n\nDie Korrelationsanalyse dient dazu, zu entscheiden, ob die Prädiktorvariablen hinreichend voneinander unabhängig sind, um alle in das globale Modell hinein zu nehmen. Bei Pearson’s Korrelationskoeffizienten r, die betragsmässig grösser als 0.7 sind, würde es problematisch. Alternativ hätten wir auch den VIF (Variance Inflation Factor) als Kriterium für den möglichen Ausschluss von Variablen aus dem globalen Modell nehmen können.\nWenn man auf cor nun doppel-klickt und es in einem separaten Fenster öffnet, sieht man, wo es problematische Korrelationen zwischen Variablenpaaren gibt. Es sind dies Altitude vs. Temperature und N.total vs. C.org. Wir müssen aus jedem dieser Paare jetzt eine Variable rauswerfen, am besten jene, die weniger gut interpretierbar ist. Ich entscheide mich dafür Temperature statt Altitude (weil das der direktere ökologische Wirkfaktor ist) und C.org statt N.total zu behalten (weil es in der Literatur mehr Daten zum Humusgehalt als zum N-Gehalt gibt, damit eine bessere Vergleichbarkeit erzielt wird). Die Aussagen, die wir für die beibehaltene Variable erzielen, stehen aber +/- auch für die entfernte.\nDas Problem ist aber, dass wir immer noch 16 Variablen haben, was einen sehr leistungsfähigen Rechner oder sehr lange Rechenzeit erfordern würde. Wir sollten also unter 15 Variablen kommen. Wir könnten uns jetzt überlegen, welche uns ökologisch am wichtigsten sind, oder ein noch strengeres Kriterium bei r verwenden, etwa 0.6\n\ncor &lt;- cor(ukr[, c(2:19)])\ncor[abs(cor) &lt; 0.6] &lt;- 0\ncor\n\n                    Altitude Inclination Heat_index Microrelief\nAltitude           1.0000000           0          0           0\nInclination        0.0000000           1          0           0\nHeat_index         0.0000000           0          1           0\nMicrorelief        0.0000000           0          0           1\nGrazing_intensity  0.0000000           0          0           0\nLitter             0.0000000           0          0           0\nStones_and_rocks   0.0000000           0          0           0\nGravel             0.0000000           0          0           0\nFine_soil          0.0000000           0          0           0\npH                 0.0000000           0          0           0\nConductivity       0.0000000           0          0           0\nCaCO3              0.0000000           0          0           0\nN_total            0.0000000           0          0           0\nC_org              0.0000000           0          0           0\nCN_ratio           0.0000000           0          0           0\nTemperature       -0.8309559           0          0           0\nTemperature_range -0.6794514           0          0           0\nPrecipitation      0.0000000           0          0           0\n                  Grazing_intensity Litter Stones_and_rocks Gravel Fine_soil\nAltitude                          0      0                0      0         0\nInclination                       0      0                0      0         0\nHeat_index                        0      0                0      0         0\nMicrorelief                       0      0                0      0         0\nGrazing_intensity                 1      0                0      0         0\nLitter                            0      1                0      0         0\nStones_and_rocks                  0      0                1      0         0\nGravel                            0      0                0      1         0\nFine_soil                         0      0                0      0         1\npH                                0      0                0      0         0\nConductivity                      0      0                0      0         0\nCaCO3                             0      0                0      0         0\nN_total                           0      0                0      0         0\nC_org                             0      0                0      0         0\nCN_ratio                          0      0                0      0         0\nTemperature                       0      0                0      0         0\nTemperature_range                 0      0                0      0         0\nPrecipitation                     0      0                0      0         0\n                        pH Conductivity CaCO3   N_total     C_org CN_ratio\nAltitude          0.000000     0.000000     0 0.0000000 0.0000000        0\nInclination       0.000000     0.000000     0 0.0000000 0.0000000        0\nHeat_index        0.000000     0.000000     0 0.0000000 0.0000000        0\nMicrorelief       0.000000     0.000000     0 0.0000000 0.0000000        0\nGrazing_intensity 0.000000     0.000000     0 0.0000000 0.0000000        0\nLitter            0.000000     0.000000     0 0.0000000 0.0000000        0\nStones_and_rocks  0.000000     0.000000     0 0.0000000 0.0000000        0\nGravel            0.000000     0.000000     0 0.0000000 0.0000000        0\nFine_soil         0.000000     0.000000     0 0.0000000 0.0000000        0\npH                1.000000     0.674678     0 0.0000000 0.0000000        0\nConductivity      0.674678     1.000000     0 0.0000000 0.0000000        0\nCaCO3             0.000000     0.000000     1 0.0000000 0.0000000        0\nN_total           0.000000     0.000000     0 1.0000000 0.9551133        0\nC_org             0.000000     0.000000     0 0.9551133 1.0000000        0\nCN_ratio          0.000000     0.000000     0 0.0000000 0.0000000        1\nTemperature       0.000000     0.000000     0 0.0000000 0.0000000        0\nTemperature_range 0.000000     0.000000     0 0.0000000 0.0000000        0\nPrecipitation     0.000000     0.000000     0 0.0000000 0.0000000        0\n                  Temperature Temperature_range Precipitation\nAltitude           -0.8309559        -0.6794514     0.0000000\nInclination         0.0000000         0.0000000     0.0000000\nHeat_index          0.0000000         0.0000000     0.0000000\nMicrorelief         0.0000000         0.0000000     0.0000000\nGrazing_intensity   0.0000000         0.0000000     0.0000000\nLitter              0.0000000         0.0000000     0.0000000\nStones_and_rocks    0.0000000         0.0000000     0.0000000\nGravel              0.0000000         0.0000000     0.0000000\nFine_soil           0.0000000         0.0000000     0.0000000\npH                  0.0000000         0.0000000     0.0000000\nConductivity        0.0000000         0.0000000     0.0000000\nCaCO3               0.0000000         0.0000000     0.0000000\nN_total             0.0000000         0.0000000     0.0000000\nC_org               0.0000000         0.0000000     0.0000000\nCN_ratio            0.0000000         0.0000000     0.0000000\nTemperature         1.0000000         0.6900784    -0.6237053\nTemperature_range   0.6900784         1.0000000     0.0000000\nPrecipitation      -0.6237053         0.0000000     1.0000000\n\n\nEntsprechend „werfen“ wir auch noch die folgenden Variablen „raus“: Temperature.range (positiv mit Temperature), Precipitation (negativ mit Temperature) sowie Conductivity (positiv mit pH).\nNun können wir das globale Modell definieren, indem wir alle verbleibenden Variablen aufnehmen, das sind 13. (Wenn das nicht eh schon so viele wären, dass es uns an die Grenze der Rechenleistung bringt, hätten wir auch noch darüber nachdenken können, einzelne quadratische Terme oder Interaktionsterme zu berücksichtigen).\n\nglobal_model &lt;- lm(Species_richness ~ Inclination + Heat_index + Microrelief + Grazing_intensity +\n    Litter + Stones_and_rocks + Gravel + Fine_soil + pH + CaCO3 + C_org + CN_ratio + Temperature, data = ukr)\n\nNun gibt es im Prinzip zwei Möglichkeiten, vom globalen (vollen) Modell zu einem minimal adäquaten Modell zu kommen. (1) Der Ansatz der „frequentist statistic“, in dem man aus dem vollen Modell so lange schrittweise Variablen entfernt, bis nur noch signifikante Variablen verbleiben. (2) Den informationstheoretischen Ansatz, bei dem alle denkbaren Modelle berechnet und verglichen werden (also alle möglichen Kombinationen von 13,12,…, 1, 0 Parametern). Diese Lösung stelle ich im Folgenden vor:\n\n# Multimodel inference\np_load(\"MuMIn\")\n\noptions(na.action = \"na.fail\")\nallmodels &lt;- dredge(global_model)\n\n\nallmodels\n\nJetzt bekommen wir die besten der insgesamt 8192 möglichen Modelle gelistet mit ihren Parameterschätzungen und ihrem AICc.\nDas beste Modell umfasst 5 Parameter (CaCO3, CN.ratio, Grazing.intensity. Heat.index, Litter). Allerdings ist das nächstbeste Modell (mit 6 Parametern) nur wenig schlechter (delta AICc = 0.71), was sich in fast gleichen (und zudem sehr niedrigen) Akaike weights bemerkbar macht. Nach dem Verständnis des Information theoretician approach, sollte man in einer solchen Situation nicht das eine „beste“ Modell benennen, sondern eine Aussage über die Gruppe der insgesamt brauchbaren Modelle treffen. Hierzu kann man (a) Importance der Parameter über alle Modelle hinweg berechnen (= Summe der Akaike weights aller Modelle, die den betreffenden Parameter enthalten) und/oder (b) ein nach Akaike weights gemitteltes Modell berechnen.\n\n# Importance values der Variablen\nsw(allmodels)\n\n                     Heat_index Litter CaCO3 CN_ratio Grazing_intensity\nSum of weights:      1.00       0.92   0.82  0.73     0.68             \nN containing models: 4096       4096   4096  4096     4096             \n                     Stones_and_rocks Temperature Microrelief Gravel Fine_soil\nSum of weights:      0.43             0.39        0.33        0.31   0.31     \nN containing models: 4096             4096        4096        4096   4096     \n                     C_org pH   Inclination\nSum of weights:      0.30  0.26 0.26       \nN containing models: 4096  4096 4096       \n\n\nDemnach ist Heat.index die wichtigste Variable (in 100% aller relevanten Modelle), während ferner Litter, CaCO3, CN_ratio und Grazing_intensity in mehr als 50% der relevanten Modelle enthalten sind.\n\n# Modelaveraging (Achtung: dauert mit 13 Variablen einige Minuten)\navgmodel &lt;- model.avg(allmodels)\nsummary(avgmodel)\n\nAus dem gemittelten Modell können wir die Richtung der Beziehung (positiv oder negativ) und ggf. die Effektgrössen (wie verändert sich die Artenzahl, wenn die Prädiktorvariable um eine Einheit zunimmt?) ermitteln.\n\n# Modelldiagnostik nicht vergessen\npar(mfrow = c(2, 2))\nplot(global_model)\n\n\n\n\n\n\n\nplot(lm(Species_richness ~ Heat_index + Litter + CaCO3 + CN_ratio + Grazing_intensity, data = ukr))\n\n\n\n\n\n\n\n\nWie immer kommt am Ende die Modelldiagnostik. Wir können uns entweder das globale Modell oder das Modell mit den 5 Variablen mit importance &gt; 50% anschauen. Das Bild sieht fast identisch aus und zeigt keinerlei problematische Abweichungen, d. h. links oben weder ein Keil, noch eine Banane, rechts oben eine nahezu perfekte Gerade.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistik 4: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik5_Demo.html",
    "href": "statistik/Statistik5_Demo.html",
    "title": "Statistik 5: Demo",
    "section": "",
    "text": "von LMs zu GLMs\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\nlibrary(\"pacman\")\np_load(\"tidyverse\")\n\n# Daten erstellen und anschauen\nstrand &lt;- tibble(\n  Temperatur = c(10, 12, 16, 20, 24, 25, 30, 33, 37),\n  Besucher = c(40, 12, 50, 500, 400, 900, 1500, 900, 2000)\n)\n\nggplot(strand, aes(x = Temperatur, y = Besucher)) +\n  geom_point() +\n  xlim(0, 40) +\n  theme_classic()\n# Modell definieren und anschauen\nlm_strand &lt;- lm(Besucher ~ Temperatur, data = strand)\nsummary(lm_strand)\n\n\nCall:\nlm(formula = Besucher ~ Temperatur, data = strand)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-476.41 -176.89   55.59  218.82  353.11 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.7 on 7 degrees of freedom\nMultiple R-squared:  0.8244,    Adjusted R-squared:  0.7993 \nF-statistic: 32.86 on 1 and 7 DF,  p-value: 0.0007115\n\n# Modelvalidierung\npar(mfrow = c(2, 2))\nplot(lm_strand)\nggplot(strand, aes(x = Temperatur, y = Besucher)) +\n  geom_point() +\n  xlim(0, 40) +    \n  stat_smooth(method = \"lm\") +\n  theme_classic()\n# GLMs definieren und anschauen\n\n# ist dasselbe wie ein LM\nglm_gaussian &lt;- glm(Besucher ~ Temperatur, family = \"gaussian\", data = strand) \nsummary(glm_gaussian)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = \"gaussian\", data = strand)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 97138.03)\n\n    Null deviance: 3871444  on 8  degrees of freedom\nResidual deviance:  679966  on 7  degrees of freedom\nAIC: 132.63\n\nNumber of Fisher Scoring iterations: 2",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistik 5: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik5_Demo.html#poisson-regression",
    "href": "statistik/Statistik5_Demo.html#poisson-regression",
    "title": "Statistik 5: Demo",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n# Poisson passt besser zu den Daten \nglm_poisson &lt;- glm(Besucher ~ Temperatur, family = \"poisson\", data = strand) \n\nsummary(glm_poisson)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = \"poisson\", data = strand)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 3.500301   0.056920   61.49   &lt;2e-16 ***\nTemperatur  0.112817   0.001821   61.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: 1185.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nRücktranformation der Werte auf die orginale Skale (Hier Exponentialfunktion da family=possion als Link-Funktion den natürlichen Logarithmus (log) verwendet) Besucher = exp(3.50 + 0.11 Temperatur/°C)\n\n# So kann man auf die Coefficients des Modells \"extrahieren\" und dann mit[] auswählen\nglm_poisson$coefficients \n\n(Intercept)  Temperatur \n  3.5003009   0.1128168 \n\nexp(glm_poisson$coefficients[1])# Anzahl besucher bei 0°C\n\n(Intercept) \n   33.12542 \n\nexp(glm_poisson$coefficients[1] + 30 * glm_poisson$coefficients[2]) # Anzahl besucher bei 30°C\n\n(Intercept) \n   977.3102 \n\n# Test Overdispersion\np_load(\"performance\")\ncheck_overdispersion(glm_poisson)\n\n# Overdispersion test\n\n       dispersion ratio =  149.683\n  Pearson's Chi-Squared = 1047.778\n                p-value =  &lt; 0.001\n\n\n-&gt; Es liegt Overdispersion vor. Darum quasipoisson wählen.\n\nglm_quasipoisson &lt;- glm(Besucher ~ Temperatur, family = \"quasipoisson\", data = strand)\nsummary(glm_quasipoisson)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = \"quasipoisson\", \n    data = strand)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  3.50030    0.69639   5.026  0.00152 **\nTemperatur   0.11282    0.02227   5.065  0.00146 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 149.6826)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\n\npar(mfrow = c(2, 2))\nplot(glm_gaussian, main = \"glm_gaussian\")\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(glm_poisson, main = \"glm_poisson\")\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(glm_quasipoisson, main = \"glm_quasipoisson\")\n\n\n\n\n\n\n\n\n-&gt; Die Outputs von glm_poisson und glm_quasipoisson sind bis auf die p-Werte identisch.\n\nggplot(data = strand, aes(x = Temperatur, y = Besucher)) +\n  geom_point() +\n  xlim(0, 40) +    \n  stat_smooth(method = \"lm\", color = \"blue\", se = FALSE) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"poisson\"), \n                color = \"red\", se = FALSE) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"quasipoisson\"), \n                  color = \"green\", linetype = \"dashed\", se = FALSE) +\n    annotate(geom=\"text\", x = 4, y = 2000, label = \"gaussian\", color = \"blue\") +\n    annotate(geom=\"text\", x = 4, y = 1800, label = \"poisson\", color = \"red\") +\n    annotate(geom=\"text\", x = 4, y = 1600, label = \"quasipoisson\", color = \"green\") +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistik 5: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik5_Demo.html#logistische-regression",
    "href": "statistik/Statistik5_Demo.html#logistische-regression",
    "title": "Statistik 5: Demo",
    "section": "Logistische Regression",
    "text": "Logistische Regression\n\nbathing &lt;- tibble(\n  temperatur = c(1, 2, 5, 9, 14, 14, 15, 19, 22, 24, 25, 26, 27, 28, 29),\n  badend = c(0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1)\n)\n\nggplot(bathing, aes(x = temperatur, y = badend)) +\n  geom_point() +\n  xlim(0, 30) +\n  theme_classic()\n\n\n\n\n\n\n\n# Logistisches Modell definieren\nglm_logistic &lt;- glm(badend ~ temperatur, family = \"binomial\", data = bathing)\nsummary(glm_logistic)\n\n\nCall:\nglm(formula = badend ~ temperatur, family = \"binomial\", data = bathing)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -5.4652     2.8501  -1.918   0.0552 .\ntemperatur    0.2805     0.1350   2.077   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20.728  on 14  degrees of freedom\nResidual deviance: 10.829  on 13  degrees of freedom\nAIC: 14.829\n\nNumber of Fisher Scoring iterations: 6\n\n# Test Overdispersion\ncheck_overdispersion(glm_logistic)\n\n# Overdispersion test\n\n dispersion ratio = 1.120\n          p-value = 0.808\n\n# Modeldiagnostik (godness of fit test, wenn nicht signifikant, dann OK)\n1 - pchisq(glm_logistic$deviance, glm_logistic$df.resid)\n\n[1] 0.6251679\n\n# Modeldiagnostik mit funktion \"check_model\"\ncheck_model(glm_logistic)\n\n\n\n\n\n\n\n\n\n# Modelresultate\n\n# pseudo-R²\nr2(glm_logistic)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.538\n\n# Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x)\nexp(glm_logistic$coefficients[2])\n\ntemperatur \n  1.323807 \n\n# LD50 (also hier: Temperatur, bei der 50% der Touristen baden)\n-glm_logistic$coefficients[1] / glm_logistic$coefficients[2]\n\n(Intercept) \n   19.48311 \n\n# oder\np_load(\"MASS\")\ndose.p(glm_logistic, p = 0.5)\n\n             Dose       SE\np = 0.5: 19.48311 2.779485\n\n# Vorhersagen\npredicted &lt;- predict(glm_logistic, type = \"response\")\n\n# Konfusionsmatrix\nkm &lt;- table(bathing$badend, predicted &gt; 0.5)\nkm\n\n   \n    FALSE TRUE\n  0     7    1\n  1     1    6\n\n# Missklassifizierungsrate\n1 - sum(diag(km) / sum(km))\n\n[1] 0.1333333\n\n# Plotting\nggplot(data = bathing, aes(x = temperatur, y = badend)) +\n  geom_point() +\n  xlim(0, 30) +\n  labs(x = \"Temperature (°C)\", y = \"% Bathing\") +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistik 5: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik5_Demo.html#binominale-regression",
    "href": "statistik/Statistik5_Demo.html#binominale-regression",
    "title": "Statistik 5: Demo",
    "section": "Binominale Regression",
    "text": "Binominale Regression\n\np_load(\"doBy\")\n?budworm\n\ndata(budworm)\nstr(budworm)\n\n'data.frame':   12 obs. of  4 variables:\n $ sex   : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 1 1 1 1 ...\n $ dose  : int  1 2 4 8 16 32 1 2 4 8 ...\n $ ndead : int  1 4 9 13 18 20 0 2 6 10 ...\n $ ntotal: int  20 20 20 20 20 20 20 20 20 20 ...\n\nsummary(budworm)\n\n     sex         dose          ndead           ntotal  \n female:6   Min.   : 1.0   Min.   : 0.00   Min.   :20  \n male  :6   1st Qu.: 2.0   1st Qu.: 3.50   1st Qu.:20  \n            Median : 6.0   Median : 9.50   Median :20  \n            Mean   :10.5   Mean   : 9.25   Mean   :20  \n            3rd Qu.:16.0   3rd Qu.:13.75   3rd Qu.:20  \n            Max.   :32.0   Max.   :20.00   Max.   :20  \n\n\nDie Insektiziddosen wurden als Zweierpotenzen gewählt (d.h. jede Dosis ist doppelt so hoch wie die vorhergehende Dosis). Da wir von einer multiplikativen Wirkung der Dosis ausgehen, ist es vorteilhaft, die Werte mit einem Logarithmus mit Basis 2 zu logarithmieren.\n\nbudworm$ldose &lt;- log2(budworm$dose)\n\n# Das Modell kann auf zwei verschiedene Varianten spezifiziert werden \nglm_binomial &lt;- glm( cbind( ndead, ntotal-ndead) ~ ldose*sex, family = binomial, data = budworm)\n\nglm_binom &lt;- glm(ndead/ntotal ~ ldose*sex, family = binomial, weights = ntotal, data = budworm)\n\ncoef(glm_binomial)\n\n  (Intercept)         ldose       sexmale ldose:sexmale \n   -2.9935418     0.9060364     0.1749868     0.3529130 \n\ncoef(glm_binom)\n\n  (Intercept)         ldose       sexmale ldose:sexmale \n   -2.9935418     0.9060364     0.1749868     0.3529130 \n\n# Das Resultat ist identisch\n\n# Model optimierung\ndrop1(glm_binomial, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\ncbind(ndead, ntotal - ndead) ~ ldose * sex\n          Df Deviance    AIC    LRT Pr(&gt;Chi)\n&lt;none&gt;         4.9937 43.104                \nldose:sex  1   6.7571 42.867 1.7633   0.1842\n\nglm_binomial_2 &lt;- update( glm_binomial, .~.-sex:ldose)\ndrop1(glm_binomial_2, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\ncbind(ndead, ntotal - ndead) ~ ldose + sex\n       Df Deviance     AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;       6.757  42.867                      \nldose   1  118.799 152.909 112.042 &lt; 2.2e-16 ***\nsex     1   16.984  51.094  10.227  0.001384 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Validate Model\ncheck_overdispersion(glm_binomial_2)\n\n# Overdispersion test\n\n dispersion ratio = 0.929\n          p-value = 0.592\n\ncheck_model(glm_binomial_2)\n\n\n\n\n\n\n\n# Resultat und Visualisierung\nsummary(glm_binomial_2)\n\n\nCall:\nglm(formula = cbind(ndead, ntotal - ndead) ~ ldose + sex, family = binomial, \n    data = budworm)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -3.4732     0.4685  -7.413 1.23e-13 ***\nldose         1.0642     0.1311   8.119 4.70e-16 ***\nsexmale       1.1007     0.3558   3.093  0.00198 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 124.8756  on 11  degrees of freedom\nResidual deviance:   6.7571  on  9  degrees of freedom\nAIC: 42.867\n\nNumber of Fisher Scoring iterations: 4\n\n# Modellgüte (pseudo-R²)\n1 - (glm_binomial_2$dev / glm_binomial_2$null)\n\n[1] 0.9458896\n\n# ld 50 Female (cf = c(1, 2) = Intercept und dosis)\n( ld50_feamle &lt;- dose.p(glm_binomial_2, cf = c(1, 2)) ) \n\n             Dose        SE\np = 0.5: 3.263587 0.2297539\n\n# Zurücktransformieren\n2^ld50_feamle\n\n            Dose        SE\np = 0.5: 9.60368 0.2297539\n\n# ld 50 male\n# dose.p(glm_binomial_2, cf = c(1, 2, 3)) \n# funktioniert nicht wir müssen es manuell ausrechnen\nld50_male &lt;- -(glm_binomial_2$coefficients[1] + glm_binomial_2$coefficients[2] ) / glm_binomial_2$coefficients[3]\n# Zurücktransformieren\n2^ld50_male\n\n(Intercept) \n   4.558211 \n\n# Männliche Tiere reagieren wesentlich empfindlicher auf das Insektizid\n\n# Visualisierung\nggplot(budworm, aes(x = ldose, y = ndead / 20, color = sex)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  labs(x = \"log2(dose)\", y = \"probability dead\") +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistik 5: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik5_Uebung.html",
    "href": "statistik/Statistik5_Uebung.html",
    "title": "Statistik 5: Übung",
    "section": "",
    "text": "Datensatz polis.csv\nDer Datensatz polis.csv beschreibt für 19 Inseln im Golf von Kalifornien, ob Eidechsen der Gattung Uta vorkommen (presence/absence: PA) in Abhängigkeit von der Form der Inseln (Verhältnis Umfang zu Fläche: RATIO\nBitte prüft mit einer logistischen Regression, ob und ggf. wie die Inselform die Präsenz der Eidechsen beinflusst",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Statistik 5: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik5_Uebung.html#musterlösung-übung-5",
    "href": "statistik/Statistik5_Uebung.html#musterlösung-übung-5",
    "title": "Statistik 5: Übung",
    "section": "Musterlösung Übung 5",
    "text": "Musterlösung Übung 5\n\nlibrary(\"pacman\")\np_load(\"tidyverse\")\n\npolis &lt;- read_delim(\"datasets/stat/polis.csv\", delim = \";\") |&gt;\n  mutate(across(where(is.character), as.factor))\n                                     \nstr(polis)\n\ntibble [19 × 3] (S3: tbl_df/tbl/data.frame)\n $ ISLAND: Factor w/ 19 levels \"Angeldlg\",\"Bahiaan\",..: 5 6 7 8 9 10 11 12 14 15 ...\n $ RATIO : num [1:19] 15.41 5.63 25.92 15.17 13.04 ...\n $ PA    : num [1:19] 1 1 1 0 1 0 0 0 0 1 ...\n\nsummary(polis)\n\n      ISLAND       RATIO             PA        \n Angeldlg: 1   Min.   : 0.21   Min.   :0.0000  \n Bahiaan : 1   1st Qu.: 5.86   1st Qu.:0.0000  \n Bahiaas : 1   Median :15.17   Median :1.0000  \n Blanca  : 1   Mean   :18.74   Mean   :0.5263  \n Bota    : 1   3rd Qu.:23.20   3rd Qu.:1.0000  \n Cabeza  : 1   Max.   :63.16   Max.   :1.0000  \n (Other) :13                                   \n\n\nMan erkennt, dass polis 19 Beobachtungen von drei Parametern enthält, wobei ISLAND ein Faktor mit den Inselnamen ist, während RATIO metrisch ist und PA nur 0 oder 1 enthält. Prädiktorvariable ist RATIO, abhängige Variable PA, mithin ist das korrekte statistische Verfahren eine logistische Regression (GLM).\n\n# Definition des logistischen Modells\nglm_1 &lt;- glm(PA ~ RATIO, family = \"binomial\", data = polis)\nsummary(glm_1)\n\n\nCall:\nglm(formula = PA ~ RATIO, family = \"binomial\", data = polis)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   3.6061     1.6953   2.127   0.0334 *\nRATIO        -0.2196     0.1005  -2.184   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 26.287  on 18  degrees of freedom\nResidual deviance: 14.221  on 17  degrees of freedom\nAIC: 18.221\n\nNumber of Fisher Scoring iterations: 6\n\n\nModell ist signifikant (p-Wert in Zeile RATIO ist &lt; 0.05). Jetzt müssen wir noch prüfen, ob es auch valide ist:\n\n# Test Overdispersion\np_load(\"performance\")\ncheck_overdispersion(glm_1)\n\n# Overdispersion test\n\n dispersion ratio = 1.081\n          p-value = 0.768\n\n# Modelldiagnostik für das gewählte Modell (wenn nicht signifikant, dann OK)\n1 - pchisq(glm_1$deviance, glm_1$df.resid)\n\n[1] 0.6514215\n\ncheck_model(glm_1)\n\n\n\n\n\n\n\n\nIst ok Jetzt brauchen wir noch die Modellgüte (Pseudo-R2):\n\n# Modellgüte (pseudo-R²)\nr2(glm_1)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.521\n\n\nUm zu unser Modell zu interpretieren müssen wir noch in Betracht ziehen, dass wir nicht die Vorkommenswahrscheinlichkeit selbst, sondern logit (Vorkommenswahrscheinlichkeit) modelliert haben. Unser Ergebnis (die beiden Parameterschätzungen von oben, also 3.6061 und -0.2196) muss also zunächst in etwas Interpretierbares übersetzt werden:\n\n# Steilheit der Beziehung in Modellen mit nur einem Parameter\nexp(glm_1$coef[2])\n\n    RATIO \n0.8028734 \n\n\n&lt; 1, d. h. Vorkommenswahrscheinlichkeit sinkt mit zunehmender Isolation.\n\n# LD50 für 1-Parameter-Modelle (hier also x-Werte, bei der 50% der Inseln besiedelt sind)\n-glm_1$coef[1] / glm_1$coef[2]\n\n(Intercept) \n    16.4242 \n\np_load(MASS)\ndose.p(glm_1, p = 0.5)\n\n            Dose       SE\np = 0.5: 16.4242 3.055921\n\n\nAm besten stellen wir auch unsere Funktionsgleichung dar. Dazu müssen wir das „Rohergebnis“ (mit P = Vorkommenswahrscheinlichkeit)\nln (P/ (1- P)) = 3.606 – 0.220 RATIO\nso umformen, dass wir links nur P stehen haben:\nP = exp (3.606 – 0.220 RATIO) / (1 + exp (3.606 – 0.220 RATIO))\nDas ist also unsere vorhergesagte Regressionsfunktion, die wir in einem letzten Schritt auch noch visualisieren können (und sollten):\n\n# Ergebnisplots\nggplot(data = polis, aes(x = RATIO, y = PA)) +\n  geom_point() +\n  labs(x = \"Umfang-Flächen-Verhältnis\", y = \"Vorkommenswahrscheinlichkeit\") +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Statistik 5: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik6_Demo.html",
    "href": "statistik/Statistik6_Demo.html",
    "title": "Statistik 6: Demo",
    "section": "",
    "text": "Repeated measurement\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\nEs handelt sich um einen Düngeversuch (Daten aus Lepš & Šmilauer 2020). 18 Pflanzenindividuen wurden zufällig einer von drei Düngevarianten zugewiesen und dabei zu vier Zeitpunkten ihre Wuchshöhe gemessen. Abhängigkeiten sind hier in zwei Aspekten vorhanden: einerseits wurde jedes Pflanzenindividuum mehrfach gemessen, andererseits hat es nur jeweils eine Düngervariante erhalten\n# Daten laden\nplantf &lt;- read_delim(\"datasets/stat/rm_plants.csv\", delim = \";\") |&gt;\n  mutate(across(where(is.character), as.factor))\n\nstr(plantf)\n\ntibble [72 × 4] (S3: tbl_df/tbl/data.frame)\n $ PlantHeight: num [1:72] 5 7 9 11 6 9 12 15 7 8 ...\n $ Treatment  : Factor w/ 3 levels \"Nitrogen\",\"Phosphorus\",..: 3 3 3 3 1 1 1 1 2 2 ...\n $ Time       : Factor w/ 4 levels \"T0\",\"T1\",\"T2\",..: 1 2 3 4 1 2 3 4 1 2 ...\n $ PlantID    : Factor w/ 18 levels \"P01\",\"P02\",\"P03\",..: 1 1 1 1 2 2 2 2 3 3 ...\n\nsummary(plantf)\n\n  PlantHeight          Treatment  Time       PlantID  \n Min.   : 4.000   Nitrogen  :24   T0:18   P01    : 4  \n 1st Qu.: 6.000   Phosphorus:24   T1:18   P02    : 4  \n Median : 9.000   Water     :24   T2:18   P03    : 4  \n Mean   : 9.306                   T3:18   P04    : 4  \n 3rd Qu.:11.000                           P05    : 4  \n Max.   :18.000                           P06    : 4  \n                                          (Other):48  \n\ntable(plantf$Treatment, plantf$PlantID)\n\n            \n             P01 P02 P03 P04 P05 P06 P07 P08 P09 P10 P11 P12 P13 P14 P15 P16\n  Nitrogen     0   4   0   0   4   0   0   4   0   0   4   0   0   4   0   0\n  Phosphorus   0   0   4   0   0   4   0   0   4   0   0   4   0   0   4   0\n  Water        4   0   0   4   0   0   4   0   0   4   0   0   4   0   0   4\n            \n             P17 P18\n  Nitrogen     4   0\n  Phosphorus   0   4\n  Water        0   0\n\n# Mit aov\npf_eaov &lt;- aov(PlantHeight ~ Treatment * Time  + Error(PlantID), data = plantf)\nsummary(pf_eaov)\n\n\nError: PlantID\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment  2 115.36   57.68   21.68 3.76e-05 ***\nResiduals 15  39.92    2.66                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTime            3  588.1  196.02  382.13  &lt; 2e-16 ***\nTreatment:Time  6   64.9   10.81   21.07 1.37e-11 ***\nResiduals      45   23.1    0.51                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Als lmm \npf_lmm &lt;- glmmTMB(PlantHeight ~ Treatment * Time + (1 | PlantID), \n                 family = gaussian, \n                 REML = TRUE,\n                 data = plantf)\n\nAnova(pf_lmm)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: PlantHeight\n                  Chisq Df Pr(&gt;Chisq)    \nTreatment        43.351  2  3.859e-10 ***\nTime           1146.390  3  &lt; 2.2e-16 ***\nTreatment:Time  126.444  6  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(pf_lmm)\n\n Family: gaussian  ( identity )\nFormula:          PlantHeight ~ Treatment * Time + (1 | PlantID)\nData: plantf\n\n     AIC      BIC   logLik deviance df.resid \n   204.4    236.3    -88.2    176.4       70 \n\nRandom effects:\n\nConditional model:\n Groups   Name        Variance Std.Dev.\n PlantID  (Intercept) 0.537    0.7328  \n Residual             0.513    0.7162  \nNumber of obs: 72, groups:  PlantID, 18\n\nDispersion estimate for gaussian family (sigma^2): 0.513 \n\nConditional model:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  5.1667     0.4183  12.351  &lt; 2e-16 ***\nTreatmentPhosphorus          0.6667     0.5916   1.127 0.259796    \nTreatmentWater               0.1667     0.5916   0.282 0.778160    \nTimeT1                       4.3333     0.4135  10.479  &lt; 2e-16 ***\nTimeT2                       7.6667     0.4135  18.541  &lt; 2e-16 ***\nTimeT3                      11.3333     0.4135  27.408  &lt; 2e-16 ***\nTreatmentPhosphorus:TimeT1  -2.1667     0.5848  -3.705 0.000211 ***\nTreatmentWater:TimeT1       -2.8333     0.5848  -4.845 1.27e-06 ***\nTreatmentPhosphorus:TimeT2  -3.6667     0.5848  -6.270 3.61e-10 ***\nTreatmentWater:TimeT2       -4.1667     0.5848  -7.125 1.04e-12 ***\nTreatmentPhosphorus:TimeT3  -5.0000     0.5848  -8.550  &lt; 2e-16 ***\nTreatmentWater:TimeT3       -5.8333     0.5848  -9.975  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nr2(pf_lmm)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.957\n     Marginal R2: 0.912\n\n# Modellvalidierung mit DHARMa\nset.seed(123)\nsimulationOutput &lt;- simulateResiduals(fittedModel = pf_lmm, plot = TRUE, n = 1000)\n\n\n\n\n\n\n\n# Plot residuals vs. covariates des models\nplotResiduals(simulationOutput, form = plantf$Time)\n\n\n\n\n\n\n\nplotResiduals(simulationOutput, form = plantf$Treatment) \n\n\n\n\n\n\n\n# Darstellung (Interaktions Plot)\nplot_model(pf_lmm, \n           type = \"pred\", pred.type = \"re\",\n           terms = c(\"Time\", \"Treatment\") ) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Statistik 6: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik6_Demo.html#split-plot-design",
    "href": "statistik/Statistik6_Demo.html#split-plot-design",
    "title": "Statistik 6: Demo",
    "section": "Split-plot Design",
    "text": "Split-plot Design\nEs wurden zufällig fünf Untersuchungsgebiete (sites) ausgewählt, in denen jeweils der gleiche Versuchsblock implementiert wurde, bestehend aus dem Management (treatment) und der Frage, ob wilde Grossherbivoren Zugang zur Fläche hatten (plot type) (Abb. 6.1). Je ein Drittel jeder Untersuchungsfläche (zufällig zugewiesen) blieb gänzlich ungenutzt(U), wurde zu Beginn kontrolliert abgebrannt und danach sich selbst überlassen (B) oder wurde jährlich gemäht (M). Innerhalb jedes Drittels wiederum wurde (zufällig zugewiesen) die Hälfte eingezäunt (F = fenced vs. O = open), um die Beweidung durch Grossherbivoren zu verhindern.\n\n# Import Data\nglex &lt;- read_delim(\"datasets/stat/Riesch_et_al_2020_grassland.csv\", delim = \";\") |&gt;\n  mutate(across(where(is.character), as.factor))\n\nstr(glex)\n\ntibble [60 × 9] (S3: tbl_df/tbl/data.frame)\n $ ID_plot       : Factor w/ 60 levels \"Eul_A1_MP_14\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year          : num [1:60] 2014 2018 2014 2018 2014 ...\n $ site_code     : Factor w/ 5 levels \"Eul\",\"Kug\",\"Nun\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ site_no       : num [1:60] 4 4 4 4 4 4 4 4 4 4 ...\n $ treatment     : Factor w/ 3 levels \"B\",\"M\",\"U\": 2 2 2 2 3 3 3 3 1 1 ...\n $ plot_type     : Factor w/ 2 levels \"F\",\"O\": 2 2 1 1 2 2 1 1 1 1 ...\n $ SR            : num [1:60] 44 48 44 47 43 29 44 24 45 37 ...\n $ inverseSimpson: num [1:60] 11.41 9.9 10.25 6.51 12.35 ...\n $ BergerParker  : num [1:60] 0.193 0.21 0.187 0.35 0.155 ...\n\nglex$year &lt;- as.factor(glex$year)\nsummary(glex)\n\n         ID_plot     year    site_code    site_no  treatment plot_type\n Eul_A1_MP_14: 1   2014:30   Eul :12   Min.   :1   B:20      F:30     \n Eul_A1_MP_18: 1   2018:30   Kug :12   1st Qu.:2   M:20      O:30     \n Eul_A2_MK_14: 1             Nun :12   Median :3   U:20               \n Eul_A2_MK_18: 1             SomO:12   Mean   :3                      \n Eul_B1_WP_14: 1             SomW:12   3rd Qu.:4                      \n Eul_B1_WP_18: 1                       Max.   :5                      \n (Other)     :54                                                      \n       SR        inverseSimpson    BergerParker   \n Min.   :24.00   Min.   : 2.538   Min.   :0.1200  \n 1st Qu.:39.75   1st Qu.: 7.169   1st Qu.:0.1732  \n Median :44.00   Median : 9.945   Median :0.2133  \n Mean   :43.52   Mean   : 9.347   Mean   :0.2551  \n 3rd Qu.:48.00   3rd Qu.:11.555   3rd Qu.:0.2891  \n Max.   :61.00   Max.   :16.102   Max.   :0.6100  \n                                                  \n\ntable(glex$site_code, glex$treatment, glex$plot_type, glex$year)\n\n, ,  = F,  = 2014\n\n      \n       B M U\n  Eul  1 1 1\n  Kug  1 1 1\n  Nun  1 1 1\n  SomO 1 1 1\n  SomW 1 1 1\n\n, ,  = O,  = 2014\n\n      \n       B M U\n  Eul  1 1 1\n  Kug  1 1 1\n  Nun  1 1 1\n  SomO 1 1 1\n  SomW 1 1 1\n\n, ,  = F,  = 2018\n\n      \n       B M U\n  Eul  1 1 1\n  Kug  1 1 1\n  Nun  1 1 1\n  SomO 1 1 1\n  SomW 1 1 1\n\n, ,  = O,  = 2018\n\n      \n       B M U\n  Eul  1 1 1\n  Kug  1 1 1\n  Nun  1 1 1\n  SomO 1 1 1\n  SomW 1 1 1\n\n# balanced Design",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Statistik 6: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik6_Demo.html#lmm-with-random-interecept",
    "href": "statistik/Statistik6_Demo.html#lmm-with-random-interecept",
    "title": "Statistik 6: Demo",
    "section": "LMM with random interecept",
    "text": "LMM with random interecept\n\n# REML = TRUE : (Restricted maximum likelihood) v.s \n# REML = FALSE: Maximum likelihood (ML) \n# Bei REML sind die estimates genauer, aber REML sollte nicht für likelihood \n# ratio test (drop1) benutzt werden\n# Dies ist nur relevant für Gaussian mixed models (LMM) nicht für GLMMs\n\n# 2.1 Model fitten\nlmm_1 &lt;- glmmTMB(SR ~ year * treatment * plot_type + \n               (1| site_code/treatment/plot_type), \n               family = gaussian, \n               REML = FALSE,\n               data = glex)\n\nAnova(lmm_1)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SR\n                           Chisq Df Pr(&gt;Chisq)    \nyear                     58.6751  1  1.860e-14 ***\ntreatment                 5.3910  2   0.067509 .  \nplot_type                 3.1397  1   0.076406 .  \nyear:treatment           27.5733  2  1.029e-06 ***\nyear:plot_type            8.0419  1   0.004571 ** \ntreatment:plot_type       3.6181  2   0.163812    \nyear:treatment:plot_type  0.9435  2   0.623917    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Model optimierung\ndrop1(lmm_1, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nSR ~ year * treatment * plot_type + (1 | site_code/treatment/plot_type)\n                         Df    AIC     LRT Pr(&gt;Chi)\n&lt;none&gt;                      385.27                 \nyear:treatment:plot_type  2 382.20 0.92894   0.6285\n\nlmm_2 &lt;- update(lmm_1,~. -year:treatment:plot_type)\ndrop1(lmm_2, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nSR ~ year + treatment + plot_type + (1 | site_code/treatment/plot_type) + \n    year:treatment + year:plot_type + treatment:plot_type\n                    Df    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;                 382.20                      \nyear:treatment       2 398.84 20.6412 3.295e-05 ***\nyear:plot_type       1 387.26  7.0678  0.007848 ** \ntreatment:plot_type  2 381.44  3.2413  0.197775    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlmm_3 &lt;- update(lmm_2,~. -treatment:plot_type)\ndrop1(lmm_3, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nSR ~ year + treatment + plot_type + (1 | site_code/treatment/plot_type) + \n    year:treatment + year:plot_type\n               Df    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;            381.44                      \nyear:treatment  2 397.16 19.7253 5.208e-05 ***\nyear:plot_type  1 386.37  6.9307  0.008473 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Refit with REML\nlmm_4 &lt;- update(lmm_3, REML = TRUE)\n\n# Resultat\nAnova(lmm_4)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SR\n                 Chisq Df Pr(&gt;Chisq)    \nyear           49.3013  1  2.195e-12 ***\ntreatment       4.3128  2   0.115741    \nplot_type       2.3609  1   0.124407    \nyear:treatment 23.1682  2  9.313e-06 ***\nyear:plot_type  6.7571  1   0.009338 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lmm_4)\n\n Family: gaussian  ( identity )\nFormula:          \nSR ~ year + treatment + plot_type + (1 | site_code/treatment/plot_type) +  \n    year:treatment + year:plot_type\nData: glex\n\n     AIC      BIC   logLik deviance df.resid \n   358.2    383.3   -167.1    334.2       56 \n\nRandom effects:\n\nConditional model:\n Groups                        Name        Variance Std.Dev.\n plot_type:treatment:site_code (Intercept)  2.1332  1.4606  \n treatment:site_code           (Intercept)  8.0931  2.8448  \n site_code                     (Intercept)  0.8542  0.9242  \n Residual                                  18.6692  4.3208  \nNumber of obs: 60, groups:  \nplot_type:treatment:site_code, 30; treatment:site_code, 15; site_code, 5\n\nDispersion estimate for gaussian family (sigma^2): 18.7 \n\nConditional model:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           45.900      2.136  21.487  &lt; 2e-16 ***\nyear2018              -8.200      2.231  -3.675 0.000238 ***\ntreatmentM             2.300      2.720   0.846 0.397761    \ntreatmentU             3.800      2.720   1.397 0.162377    \nplot_typeO            -1.000      1.665  -0.600 0.548210    \nyear2018:treatmentM    2.400      2.733   0.878 0.379808    \nyear2018:treatmentU  -10.000      2.733  -3.659 0.000253 ***\nyear2018:plot_typeO    5.800      2.231   2.599 0.009338 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nr2(lmm_4)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.688\n     Marginal R2: 0.502\n\n# Modellvalidierung \nset.seed(123)\nsimulationOutput &lt;- simulateResiduals(fittedModel = lmm_4, plot = TRUE, n = 1000)\n\n\n\n\n\n\n\n# Plot residuals vs. covariates des models\nplotResiduals(simulationOutput, form = glex$year)\n\n\n\n\n\n\n\nplotResiduals(simulationOutput, form = glex$treatment)\n\n\n\n\n\n\n\nplotResiduals(simulationOutput, form = glex$plot_type)\n\n\n\n\n\n\n\n# Darstellung\nplot_model(lmm_4, \n           type = \"pred\", pred.type = \"re\", \n           terms = c(\"year\", \"treatment\", \"plot_type\") ) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Statistik 6: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik6_Demo.html#random-slope-random-intercept",
    "href": "statistik/Statistik6_Demo.html#random-slope-random-intercept",
    "title": "Statistik 6: Demo",
    "section": "Random slope & random intercept",
    "text": "Random slope & random intercept\nIm folgenden Beispiel wurde bei 18 Versuchspersonen die Wirkung von zunehmenden Schlafentzug auf die Reaktionszeit (in ms) gemessen. Bei jeder Versuchsperson wurde der Effekt an den Tagen 2–9 des Versuchs je einmal gemessen (Tage 0 und 1 sind die Trainingsphase und bleiben daher unberücksichtigt).\n\n# Daten laden\ndata(sleepstudy)\n?sleepstudy\n\n# Daten ohne Trainingsphase\nsleepstudy_2 &lt;- subset(sleepstudy, Days&gt;=2)\nstr(sleepstudy_2)\n\n'data.frame':   144 obs. of  3 variables:\n $ Reaction: num  251 321 357 415 382 ...\n $ Days    : num  2 3 4 5 6 7 8 9 2 3 ...\n $ Subject : Factor w/ 18 levels \"308\",\"309\",\"310\",..: 1 1 1 1 1 1 1 1 2 2 ...\n\nsummary(sleepstudy_2)\n\n    Reaction          Days         Subject  \n Min.   :203.0   Min.   :2.00   308    : 8  \n 1st Qu.:265.2   1st Qu.:3.75   309    : 8  \n Median :303.2   Median :5.50   310    : 8  \n Mean   :308.0   Mean   :5.50   330    : 8  \n 3rd Qu.:347.7   3rd Qu.:7.25   331    : 8  \n Max.   :466.4   Max.   :9.00   332    : 8  \n                                (Other):96  \n\ntable(sleepstudy_2$Subject)\n\n\n308 309 310 330 331 332 333 334 335 337 349 350 351 352 369 370 371 372 \n  8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8 \n\n# Visualisierung\nggplot(sleepstudy_2, aes(y = Reaction, x = Days)) +\n  geom_point() +\n  xlab(\"Number of days of sleep deprivation\") +\n  ylab(\"Average reaction time (ms)\") +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = F, fullrange = T) +\n  theme_classic() +\n  facet_wrap(~Subject)\n\n\n\n\n\n\n\n\nWie man in der Visualisierung sehen kann, unterscheiden sich nicht nur die Intercepts (Reaktionszeit ohne Schlafmangel), sondern der Schlafmangel scheint sich auch unterschiedlich stark auf die Reaktionszeit der Probanden auszuwirken. In diesem Fall ist es daher sinnvoll, nicht nur einen Random Intercept, sondern auch einen Random Slope zu fitten.\n\n# Fit model\nlmm_1 &lt;- glmmTMB(Reaction ~ Days + (Days | Subject),\n                 family = gaussian,\n                 REML = TRUE,\n                 data = sleepstudy_2)\n\nsummary(lmm_1)\n\n Family: gaussian  ( identity )\nFormula:          Reaction ~ Days + (Days | Subject)\nData: sleepstudy_2\n\n     AIC      BIC   logLik deviance df.resid \n  1416.1   1433.9   -702.0   1404.1      140 \n\nRandom effects:\n\nConditional model:\n Groups   Name        Variance Std.Dev. Corr  \n Subject  (Intercept) 992.63   31.506         \n          Days         45.78    6.766   -0.25 \n Residual             651.60   25.526         \nNumber of obs: 144, groups:  Subject, 18\n\nDispersion estimate for gaussian family (sigma^2):  652 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  245.097      9.260  26.469  &lt; 2e-16 ***\nDays          11.435      1.845   6.197 5.75e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nr2(lmm_1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.806\n     Marginal R2: 0.206\n\n# Modellvalidierung\nset.seed(123)\nsimulationOutput &lt;- simulateResiduals(fittedModel = lmm_1, plot = TRUE, n = 1000)\n\n\n\n\n\n\n\nplotResiduals(simulationOutput, form = sleepstudy_2$Days)\n\n\n\n\n\n\n\n# Visualisierung\nplot_model(lmm_1, \n           type = \"pred\", pred.type = \"re\",\n           show.data = TRUE) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Statistik 6: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik6_Demo.html#glmm",
    "href": "statistik/Statistik6_Demo.html#glmm",
    "title": "Statistik 6: Demo",
    "section": "GLMM",
    "text": "GLMM\nBefall von Rothirschen (Cervus elaphus) in spanischen Farmen mit dem Parasiten Elaphostrongylus cervi. Modelliert wird Vorkommen/Nichtvorkommen von L1-Larven 130 dieser Nematode in Abhängigkeit von Körperlänge und Geschlecht der Hirsche. Erhoben wurden die Daten auf 24 Farmen.\n\n# Daten laden und für GLMM aufbereiten\nDeerEcervi &lt;- read_delim(\"datasets/stat/DeerEcervi.csv\", delim = \";\") |&gt;\n  mutate(across(where(is.character), as.factor))\n\n# Daten anschauen\nstr(DeerEcervi)\n\ntibble [826 × 4] (S3: tbl_df/tbl/data.frame)\n $ Farm  : Factor w/ 24 levels \"AL\",\"AU\",\"BA\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Sex   : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 1 1 1 1 1 1 ...\n $ Length: num [1:826] 164 216 208 206 204 200 199 197 196 196 ...\n $ Ecervi: num [1:826] 0 0 0 1 0 0 1 1 1 0 ...\n\nsummary(DeerEcervi)\n\n      Farm         Sex          Length          Ecervi      \n MO     :209   female:448   Min.   : 75.0   Min.   :0.0000  \n CB     : 85   male  :378   1st Qu.:151.0   1st Qu.:0.0000  \n QM     : 60                Median :163.0   Median :1.0000  \n BA     : 50                Mean   :161.8   Mean   :0.6465  \n PN     : 37                3rd Qu.:174.9   3rd Qu.:1.0000  \n MB     : 34                Max.   :216.0   Max.   :1.0000  \n (Other):351                                                \n\ntable(DeerEcervi$Farm)\n\n\n  AL   AU   BA   BE   CB  CRC   HB   LN  MAN   MB   MO   NC   NV   PN   QM   RF \n  15   32   50   13   85    1   17   33   27   34  209   27   20   37   60   20 \n  RN   RO  SAU   SE   TI   TN VISO   VY \n  23   30    3   26   19   25   13    7 \n\n\n\n# Kontinuierliche variable Hischlänge standardisieren\nStd &lt;- function(x) { (x - mean(x)) / sd(x)}\nDeerEcervi$Lengt_std &lt;-  Std(DeerEcervi$Length)\n# oder\nDeerEcervi$Length_std &lt;- as.vector( scale(DeerEcervi$Length, center = TRUE))\n\n# Model fitten\nglmm_1 &lt;- glmmTMB(Ecervi ~ scale(Length) * Sex + (1 | Farm), \n                  family = binomial, \n                  data = DeerEcervi)\n\ncar::Anova(glmm_1)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: Ecervi\n                    Chisq Df Pr(&gt;Chisq)    \nscale(Length)     70.7552  1  &lt; 2.2e-16 ***\nSex                4.4962  1   0.033969 *  \nscale(Length):Sex  9.8309  1   0.001716 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndrop1(glmm_1, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nEcervi ~ scale(Length) * Sex + (1 | Farm)\n                  Df    AIC    LRT Pr(&gt;Chi)   \n&lt;none&gt;               832.56                   \nscale(Length):Sex  1 840.78 10.225 0.001385 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(glmm_1)\n\n Family: binomial  ( logit )\nFormula:          Ecervi ~ scale(Length) * Sex + (1 | Farm)\nData: DeerEcervi\n\n     AIC      BIC   logLik deviance df.resid \n   832.6    856.1   -411.3    822.6      821 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n Farm   (Intercept) 2.393    1.547   \nNumber of obs: 826, groups:  Farm, 24\n\nConditional model:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.9397     0.3579   2.625  0.00865 ** \nscale(Length)           0.7638     0.1363   5.604 2.09e-08 ***\nSexmale                 0.6245     0.2242   2.785  0.00535 ** \nscale(Length):Sexmale   0.7027     0.2241   3.135  0.00172 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nr2(glmm_1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.504\n     Marginal R2: 0.143\n\n\n\n#Modellvalidierung\n\n# Test overdisepersion\ncheck_overdispersion(glmm_1)\n\n# Overdispersion test\n\n dispersion ratio = 1.064\n          p-value =   0.6\n\nset.seed(123)\nsimulationOutput &lt;- simulateResiduals(fittedModel = glmm_1, plot = TRUE, n = 1000)\n\n\n\n\n\n\n\n# Plot the scaled quantile residuals versus fitted values.\nplotResiduals(simulationOutput, form = DeerEcervi$Sex)\n\n\n\n\n\n\n\nplotResiduals(simulationOutput, form = scale(DeerEcervi$Length) )\n\n\n\n\n\n\n\n\n\n# Visualisierung\nplot_model(glmm_1, \n           type = \"pred\", pred.type = \"re\", \n           terms = c(\"Length[all]\", \"Sex\"),  \n           show.data = TRUE) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Statistik 6: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik6_Uebung.html",
    "href": "statistik/Statistik6_Uebung.html",
    "title": "Statistik 6: Übung",
    "section": "",
    "text": "Datensatz novanimal.csv\n\nHintergrund Daten\nIm Forschungsprojekt NOVANIMAL wird u.a. der Frage nachgegangen, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren? Ein interessanter Ansatzpunkt ist die Ausser-Haus-Verpflegung. Gemäss der ersten in den Jahren 2014/2015 durchgeführten nationalen Ernährungserhebung menuCH essen 70 % der Bevölkerung zwischen 18 und 75 Jahren am Mittag auswärts (Bochud u. a. 2017). Daher rückt die Gastronomie als zentraler Akteur einer innovativen und nachhaltigen Ernährungswirtschaft ins Blickfeld. Welche Innovationen in der Gastronomie könnten dazu beitragen, den Pro-Kopf-Verbrauch an tierischen Nahrungsmitteln zu senken?\nDazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien (F, K, W) randomisiert angeboten.\n\n\n\nDie Abbildung zeigt das Versuchsdesign der ersten 6 Experimentwochen (Kalenderwoche 40 bis 45).\n\n\nMehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf der Webpage.\n\n\nAufgaben\nFührt mit dem novanimal Datensatz (inviduelle Daten) eine logistische Regression durch, wobei ihr die einzelnen Käufer (single campus_card holder “ccrs”) als randomisierte Variable mitberücksichtigt. Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Berücksichtigt auch mögliche Interaktionen zwischen dem Geschlecht und dem Alter sowie dem Geshchlecht und der Hochschulzugehörigkeit\n\nBestimmt das minimal adäquate Modell\nStellt die Ergebnisse dar\n\n\n\n\n\n\n\nMusterlösung Übung 6: GLMM\n\n\n\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n\nLösungstext als Download\n\n\n# lade daten\nnova &lt;- read_delim(\"datasets/stat/novanimal.csv\", delim = \";\") |&gt;\n  mutate(across(where(is.character), as.factor))\nstr(nova)\n\ntibble [17,900 × 6] (S3: tbl_df/tbl/data.frame)\n $ ccrs       : num [1:17900] 1e+09 1e+09 1e+09 1e+09 1e+09 ...\n $ meat       : num [1:17900] 1 0 1 0 0 0 0 0 0 0 ...\n $ gender     : Factor w/ 2 levels \"F\",\"M\": 2 2 2 2 2 2 2 2 2 2 ...\n $ member     : Factor w/ 2 levels \"Mitarbeitende\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ age_group  : Factor w/ 4 levels \"AK1\",\"AK2\",\"AK3\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ age_group_2: Factor w/ 4 levels \"15 - 25-jaehrig\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Die Variable“ccrs” (single campus_card holder) sollte ein facor sein\nnova$ccrs &lt;- as.factor(nova$ccrs)\nsummary(nova)\n\n         ccrs            meat        gender              member      age_group \n 1000564796:   57   Min.   :0.0000   F: 5863   Mitarbeitende: 6868   AK1:7651  \n 1000621354:   57   1st Qu.:0.0000   M:12037   Studierende  :11032   AK2:5720  \n 1000594415:   54   Median :1.0000                                   AK3:3104  \n 1000597794:   54   Mean   :0.6041                                   AK4:1425  \n 1000608246:   54   3rd Qu.:1.0000                                             \n 1000621447:   54   Max.   :1.0000                                             \n (Other)   :17570                                                              \n          age_group_2  \n 15 - 25-jaehrig:7651  \n 26 - 34-jaehrig:5720  \n 35 - 49-jaehrig:3104  \n 50 - 64-jaehrig:1425  \n                       \n                       \n                       \n\n# sieht euch die Verteilung zwischen Fleisch und  kein Fleisch an,\n# beide Kategorien kommen nicht gleich häufig vor, ist aber nicht tragisch\ntable(nova$meat) \n\n\n    0     1 \n 7087 10813 \n\nprop.table(table(nova$meat)) # Werte in Prozent\n\n\n        0         1 \n0.3959218 0.6040782 \n\n# Definiert das logistische Modell mit ccrs als random intercept und\n# wendet es auf den Datensatz an\nglmm_1 &lt;- glmmTMB(meat ~ gender + age_group + member + gender:age_group + gender:member + \n                    (1 | ccrs), \n                 family = binomial, \n                 data = nova)\n\nAnova(glmm_1)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: meat\n                    Chisq Df Pr(&gt;Chisq)    \ngender           125.9903  1     &lt;2e-16 ***\nage_group          4.3512  3     0.2260    \nmember             1.7696  1     0.1834    \ngender:age_group   0.8800  3     0.8302    \ngender:member      1.1107  1     0.2919    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(glmm_1)\n\n Family: binomial  ( logit )\nFormula:          \nmeat ~ gender + age_group + member + gender:age_group + gender:member +  \n    (1 | ccrs)\nData: nova\n\n     AIC      BIC   logLik deviance df.resid \n 21128.1  21213.8 -10553.0  21106.1    17889 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n ccrs   (Intercept) 1.478    1.216   \nNumber of obs: 17900, groups:  ccrs, 1427\n\nConditional model:\n                           Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)               -0.060517   0.221841  -0.273  0.78501   \ngenderM                    0.825594   0.281970   2.928  0.00341 **\nage_groupAK2              -0.076795   0.171426  -0.448  0.65417   \nage_groupAK3              -0.023239   0.241615  -0.096  0.92338   \nage_groupAK4               0.209746   0.313142   0.670  0.50298   \nmemberStudierende         -0.336658   0.203570  -1.654  0.09817 . \ngenderM:age_groupAK2      -0.152603   0.213397  -0.715  0.47454   \ngenderM:age_groupAK3       0.002457   0.319039   0.008  0.99385   \ngenderM:age_groupAK4      -0.211615   0.410679  -0.515  0.60636   \ngenderM:memberStudierende  0.274622   0.260581   1.054  0.29194   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## erste Interpretation: Geschlecht (Referenzkategorie: Mann) scheint den Fleischkonsum positiv zu beeinflussen, schauen wir ob wir das Model vereinfachen können:\n\n# Model optimierung\ndrop1(glmm_1, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nmeat ~ gender + age_group + member + gender:age_group + gender:member + \n    (1 | ccrs)\n                 Df   AIC     LRT Pr(&gt;Chi)\n&lt;none&gt;              21128                 \ngender:age_group  3 21123 0.87972   0.8303\ngender:member     1 21127 1.11067   0.2919\n\nglmm_2 &lt;- update(glmm_1,~. -gender:age_group)\n\ndrop1(glmm_2, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nmeat ~ gender + age_group + member + (1 | ccrs) + gender:member\n              Df   AIC    LRT Pr(&gt;Chi)  \n&lt;none&gt;           21123                  \nage_group      3 21121 4.3315  0.22782  \ngender:member  1 21124 3.2749  0.07035 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nglmm_3 &lt;- update(glmm_2,~. -gender:member)\n\ndrop1(glmm_3, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nmeat ~ gender + age_group + member + (1 | ccrs)\n          Df   AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;       21124                     \ngender     1 21245 123.112   &lt;2e-16 ***\nage_group  3 21123   4.256   0.2351    \nmember     1 21124   1.968   0.1607    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nglmm_4 &lt;- update(glmm_3,~. -age_group)\n\ndrop1(glmm_4, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nmeat ~ gender + member + (1 | ccrs)\n       Df   AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;    21123                     \ngender  1 21242 121.295   &lt;2e-16 ***\nmember  1 21126   4.963   0.0259 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Model validierung\n# Test overdisepersion\ncheck_overdispersion(glmm_4)\n\n# Overdispersion test\n\n dispersion ratio = 0.976\n          p-value = 0.024\n\n# der Wert ist nahe 1 daher i.o\n\nset.seed(123)\nsimulationOutput &lt;- simulateResiduals(fittedModel = glmm_4, plot = TRUE, n = 500)\n\n\n\n\n\n\n\n# Plot residuals vs covariates des models\nplotResiduals(simulationOutput, form = nova$gender)\n\n\n\n\n\n\n\nplotResiduals(simulationOutput, form = nova$member)\n\n\n\n\n\n\n\n# Die formalen Tests zeigen, dass es Probleme gibt. Die visuelle Inspektion zeigt jedoch, dass es überhaupt keine Probleme gibt und die Residiuen beinahe perfekt normalverteilt sind (dies zeigt, dass die Testergebnisse stark von der Anzahl der Beobachtungen (in diesem Fall viele) abhängen).\n\n\n# Modelresultat\nAnova(glmm_4)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: meat\n          Chisq Df Pr(&gt;Chisq)    \ngender 123.6924  1    &lt; 2e-16 ***\nmember   4.9512  1    0.02607 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(glmm_4)\n\n Family: binomial  ( logit )\nFormula:          meat ~ gender + member + (1 | ccrs)\nData: nova\n\n     AIC      BIC   logLik deviance df.resid \n 21122.5  21153.7 -10557.3  21114.5    17896 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n ccrs   (Intercept) 1.499    1.224   \nNumber of obs: 17900, groups:  ccrs, 1427\n\nConditional model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -0.15918    0.08504  -1.872   0.0612 .  \ngenderM            0.93860    0.08439  11.122   &lt;2e-16 ***\nmemberStudierende -0.19512    0.08769  -2.225   0.0261 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Pseudo R^2\nr2(glmm_1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.338\n     Marginal R2: 0.040\n\n# das marginale R^2 (r2m) gibt uns die erklärte Varianz der fixen Effekte: hier 4% (das ist sehr wenig)\n# das conditionale R^2 (r2c) gibt uns die erklärte Varianz für das ganze Modell\n# (mit fixen und variablen Effekten): hier 34% \n\n\n# Odds Ratios\n\n# Extrahiere Estimates\nestimates &lt;- fixef(glmm_4)\n\n# Berechne Odds Ratios\nodsr &lt;- rbind( \n  \"Mitarbeiterinnen\" = exp( estimates$cond[\"(Intercept)\"] ), \n  \"Mitarbeiter\" = exp( estimates$cond[\"(Intercept)\"] + estimates$cond[\"genderM\"]),\n  \"Studentinnen\" =  exp( estimates$cond[\"(Intercept)\"] + estimates$cond[\"memberStudierende\"]),\n  \"Studenten\" = exp( estimates$cond[\"(Intercept)\"] + estimates$cond[\"genderM\"] + estimates$cond[\"memberStudierende\"])\n                 )\ncolnames(odsr) &lt;- c(\"Odds_ratios\")\nodsr &lt;- as.data.frame(odsr)\n\n\n# Wahrscheinlichkeiten in %\nodsr$Wahrscheinlichkeit &lt;- ( odsr$Odds_ratios / (odsr$Odds_ratios + 1 ) )\nodsr\n\n                 Odds_ratios Wahrscheinlichkeit\nMitarbeiterinnen    0.852846          0.4602897\nMitarbeiter         2.180207          0.6855550\nStudentinnen        0.701665          0.4123403\nStudenten           1.793729          0.6420555\n\n# oder\np_load(\"ggeffects\")\npredict_response(glmm_4, c(\"member\", \"gender\") )\n\n# Predicted probabilities of meat\n\ngender: F\n\nmember        | Predicted |     95% CI\n--------------------------------------\nMitarbeitende |      0.46 | 0.42, 0.50\nStudierende   |      0.41 | 0.38, 0.45\n\ngender: M\n\nmember        | Predicted |     95% CI\n--------------------------------------\nMitarbeitende |      0.69 | 0.65, 0.72\nStudierende   |      0.64 | 0.61, 0.67\n\nAdjusted for:\n* ccrs = NA (population-level)\n\n\n\n# Visualisierung\np_load(\"sjPlot\")\nplot_model(glmm_4, \n           type = \"pred\", pred.type = \"re\", \n           terms = c(\"member\", \"gender\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBochud, Murielle, Angéline Chatelan, Juan-Manuel Blanco, und Sigrid Maria Beer-Borst. 2017. „Anthropometric characteristics and indicators of eating and physical activity behaviors in the Swiss adult population: results from menuCH 2014-2015“.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Statistik 6: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik7_Demo.html",
    "href": "statistik/Statistik7_Demo.html",
    "title": "Statistik 7: Demo",
    "section": "",
    "text": "Ordination Hauptkomponentenanalyse (PCA)\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Statistik 7: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik7_Demo.html#ordination-hauptkomponentenanalyse-pca",
    "href": "statistik/Statistik7_Demo.html#ordination-hauptkomponentenanalyse-pca",
    "title": "Statistik 7: Demo",
    "section": "",
    "text": "Datenbeschreibung\nDer Datensatz enthält Daten zum Vorkommen von Fischarten und den zugehörigen Umweltvariablen im Fluss Doubs (Jura). Es gibt 29 Probestellen (sites), an denen jeweils die Abundanzen von 27 Fischarten (auf einer Skalen von 0 bis 5) sowie 10 Umweltvariablen erhoben wurden. In dieser Demo verwenden wir die Umweltdaten:\n\nele = Elevation (m a.s.l.)\nslo = Slope (‰)\ndis = Mean annual discharge (m3 s-1)\npH = pH of water\nhar = Hardness (Ca concentration) (mg L-1)\npho = Phosphate concentration (mg L-1)\nnit = Nitrate concentration (mg L-1)\namm = Ammonium concentration (mg L-1)\noxy = Dissolved oxygen (mg L-1)\nbod = Biological oxygen demand (mg L-1)\n\n\nlibrary(\"pacman\")\np_load(\"tidyverse\")\n\n# Daten importieren\nenv &lt;- read_delim(\"./datasets/stat/Doubs_env.csv\", delim = \";\") |&gt;\n  column_to_rownames(var = \"Site\")\n\nstr(env)\n\n'data.frame':   29 obs. of  10 variables:\n $ ele: num  934 932 914 854 849 846 841 752 617 483 ...\n $ slo: num  48 3 3.7 3.2 2.3 3.2 6.6 1.2 9.9 4.1 ...\n $ dis: num  0.84 1 1.8 2.53 2.64 2.86 4 4.8 10 19.9 ...\n $ pH : num  7.9 8 8.3 8 8.1 7.9 8.1 8 7.7 8.1 ...\n $ har: num  45 40 52 72 84 60 88 90 82 96 ...\n $ pho: num  0.01 0.02 0.05 0.1 0.38 0.2 0.07 0.3 0.06 0.3 ...\n $ nit: num  0.2 0.2 0.22 0.21 0.52 0.15 0.15 0.82 0.75 1.6 ...\n $ amm: num  0 0.1 0.05 0 0.2 0 0 0.12 0.01 0 ...\n $ oxy: num  12.2 10.3 10.5 11 8 10.2 11.1 7.2 10 11.5 ...\n $ bod: num  2.7 1.9 3.5 1.3 6.2 5.3 2.2 5.2 4.3 2.7 ...\n\nsummary(env)\n\n      ele             slo              dis              pH       \n Min.   :172.0   Min.   : 0.200   Min.   : 0.84   Min.   :7.700  \n 1st Qu.:246.0   1st Qu.: 0.500   1st Qu.: 4.80   1st Qu.:7.900  \n Median :375.0   Median : 1.200   Median :23.00   Median :8.000  \n Mean   :470.9   Mean   : 3.531   Mean   :22.92   Mean   :8.048  \n 3rd Qu.:752.0   3rd Qu.: 3.000   3rd Qu.:28.80   3rd Qu.:8.100  \n Max.   :934.0   Max.   :48.000   Max.   :69.00   Max.   :8.600  \n      har              pho            nit             amm        \n Min.   : 40.00   Min.   :0.01   Min.   :0.150   Min.   :0.0000  \n 1st Qu.: 84.00   1st Qu.:0.10   1st Qu.:0.520   1st Qu.:0.0000  \n Median : 88.00   Median :0.30   Median :1.600   Median :0.1000  \n Mean   : 85.83   Mean   :0.57   Mean   :1.697   Mean   :0.2124  \n 3rd Qu.: 97.00   3rd Qu.:0.58   3rd Qu.:2.500   3rd Qu.:0.2000  \n Max.   :110.00   Max.   :4.22   Max.   :6.200   Max.   :1.8000  \n      oxy              bod        \n Min.   : 4.100   Min.   : 1.300  \n 1st Qu.: 8.100   1st Qu.: 2.700  \n Median :10.200   Median : 4.100  \n Mean   : 9.472   Mean   : 5.014  \n 3rd Qu.:11.000   3rd Qu.: 5.200  \n Max.   :12.400   Max.   :16.700  \n\n\n\n\nPCA durchführen\n\n# Berechnen der PCA\npca_1 &lt;- prcomp(env, scale = TRUE)\n\nWir verwenden hier «scale = TRUE» weil wir Umweltvariablen mit unterschiedliche Masseinheiten verwenden die variierende Skalen aufweisen. Eine unskalierte PCA (default: scale = FALSE ) ist sinnvoll, wenn alle Variablen ähnliche Skalen haben oder die Skalenunterschiede von Interesse sind.\n\n# Erklärte Varianzen der einzelnen Achsen (Principle Components)\nsummary(pca_1)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.3261 1.4081 0.99377 0.82554 0.57389 0.54229 0.40525\nProportion of Variance 0.5411 0.1983 0.09876 0.06815 0.03293 0.02941 0.01642\nCumulative Proportion  0.5411 0.7393 0.83809 0.90624 0.93917 0.96858 0.98501\n                           PC8     PC9    PC10\nStandard deviation     0.33106 0.15186 0.13146\nProportion of Variance 0.01096 0.00231 0.00173\nCumulative Proportion  0.99597 0.99827 1.00000\n\n# Korrelationen der Variablen mit den Ordinationsachsen (Loadings)\npca_1$rotation\n\n            PC1         PC2          PC3         PC4        PC5         PC6\nele -0.32983498  0.37326508  0.206379458  0.19101532 -0.2108916 -0.13925126\nslo -0.18939659  0.36225406 -0.317430615 -0.78012687 -0.2534671 -0.10409281\ndis  0.29534136 -0.38102320 -0.289744571 -0.25273405 -0.2684698  0.30100160\npH  -0.02798027 -0.30500230  0.833784024 -0.40105127 -0.1951586  0.07666589\nhar  0.29140671 -0.40748250 -0.128533282  0.05461084 -0.2491737 -0.71799621\npho  0.37752293  0.27210613  0.155636940 -0.10896147  0.1007383 -0.20179074\nnit  0.39070251  0.01310715 -0.007977941 -0.20607662  0.5141959  0.23374094\namm  0.36361130  0.32397140  0.159353915 -0.06949906  0.2419581 -0.15450710\noxy -0.35471920 -0.20647070 -0.007373084 -0.25539560  0.5526433 -0.45522763\nbod  0.35944351  0.32155396  0.106029560  0.05309465 -0.2837688 -0.17696504\n            PC7          PC8         PC9          PC10\nele  0.18472542  0.588745894  0.23188511  0.4176369609\nslo -0.20463767  0.034575041 -0.01624550 -0.0715819377\ndis  0.59712652  0.176111385  0.19635151  0.1759578515\npH  -0.07146037 -0.007914091  0.02364683 -0.0214444926\nhar -0.28531107  0.261864429  0.04422096  0.0001391902\npho  0.34531100  0.146608797 -0.74268910  0.0644251748\nnit -0.42339586  0.270495568  0.13496263  0.4636479419\namm  0.21872209  0.152261020  0.50422013 -0.5700251793\noxy  0.36532131 -0.222406219  0.12492546  0.2347031060\nbod  0.03358753 -0.620662606  0.25544171  0.4357752560\n\n# # Koordinaten Sites im Ordinationsraum (Scores)\npca_1$x\n\n            PC1        PC2           PC3         PC4          PC5          PC6\nS1  -4.06058121  3.3572582 -1.5353663864 -3.11664281 -0.440940815  0.023962858\nS2  -2.85884496  1.6453967  0.6199429169  0.81205916  0.462056595  1.198866058\nS3  -2.59500367  0.9087955  1.9223023314  0.08390276 -0.149900842  0.742123168\nS4  -2.42274201  0.5759475  0.2416999136  0.73737973  0.197031209 -0.175001441\nS5  -0.87891903  1.0132044  0.9242023016  0.92176239 -0.899225211 -0.317177629\nS6  -2.07214852  1.4521772 -0.0223558512  1.05958043 -0.018694903  0.245670558\nS7  -2.16112216  0.1677976  0.4568713333  0.24335994 -0.322106920 -0.877019196\nS8  -0.57361742  0.7209215  0.2639757108  1.22605607 -0.878760741 -0.203164295\nS9  -1.35708234  0.9878134 -1.7146160465  0.66588881 -0.007739829 -0.393125521\nS10 -0.79292243 -0.8705883  0.0009546047 -0.26509619  0.291231194 -0.650848271\nS11 -1.36709366 -0.5303252 -0.8181608908  0.48791374  0.458423383 -0.563740808\nS12 -1.22749339 -1.2640408 -0.0337087077 -0.04373209  0.156070410 -0.969587498\nS13 -0.78848467 -1.4750236  1.0008336819 -0.52937272  0.125890673 -0.835849370\nS14 -1.04150973 -1.8473507  2.4695198950 -1.16235808 -0.117675534 -0.019680069\nS15 -0.49079952 -0.5350769 -0.3520651388  0.18345496  0.584620728 -0.025245565\nS16  0.24701457 -0.6047699 -0.3687986865  0.13321152  0.579189656 -0.068332189\nS17  0.09988521 -0.6508563 -0.3833031874  0.04976758  0.683326378 -0.024162612\nS18  0.02438927 -0.7420695  0.1223176607 -0.23010483  0.691984141  0.214472693\nS19  0.34858245 -0.6284685 -0.4163658010 -0.13136986  1.081653869  0.335357672\nS20  0.25655489 -0.4423705 -0.9691575293  0.38503444  0.366563949  0.530706156\nS21  0.14920132 -0.8509911 -0.0692153090 -0.02307061 -0.169105752  0.366684836\nS22  4.40893902  1.7764650  1.0312263310 -0.27455931 -0.217479493 -0.607578015\nS23  2.99431167  0.7625691  0.0170945415  0.48455983 -0.940755575  0.035682696\nS24  7.11385299  3.0888496  0.4707839188 -0.34766485  0.841598956 -0.381766823\nS25  2.49964796  0.2144704 -0.8136841889  0.34331209 -0.386714310  0.514435592\nS26  1.60318946 -0.6308112 -0.1178042084 -0.23842040 -0.252238973  0.894423496\nS27  1.90404098 -1.4844247  0.7045838208 -0.94039312  0.124061006  0.646597704\nS28  1.42466912 -1.7701617 -2.2784706379  0.15724184 -0.607940975  0.004076669\nS29  1.61408580 -2.3443369 -0.3532363923 -0.67170041 -1.234422275  0.359219148\n            PC7         PC8           PC9         PC10\nS1  -0.16756651 -0.01661397 -0.0005434083 -0.027276628\nS2   0.66409559  0.08312574  0.0453575992 -0.114126005\nS3   0.36457987 -0.06164832  0.1363572490  0.106940451\nS4   0.20481751  0.41948659 -0.1190803292 -0.057120717\nS5  -0.33938509  0.29033210  0.1331611406 -0.012913680\nS6   0.40988744 -0.32654553 -0.0386629851  0.296369679\nS7  -0.11117406  0.48938544  0.0184021086 -0.012862367\nS8  -0.67526691  0.44432628 -0.0427742137 -0.107371788\nS9  -0.22123560 -0.10940926 -0.0297606988  0.019485234\nS10 -0.17677815  0.17051596 -0.0891684003  0.197809147\nS11  0.47423174 -0.36666506  0.0305048878 -0.036333811\nS12  0.22914572 -0.15537004  0.0333334255 -0.133603754\nS13  0.02658378 -0.24507682  0.0275604057  0.219092722\nS14  0.17355943 -0.15968840 -0.2347153211 -0.143569558\nS15 -0.48268849 -0.02834067 -0.1346994236 -0.055576670\nS16 -0.32433001 -0.08783303  0.3085297270  0.140390082\nS17 -0.07858369  0.12396263 -0.0949232855 -0.147315115\nS18  0.06659440 -0.12827386 -0.2055645165 -0.014781123\nS19 -0.24395665  0.13680466  0.2479536264 -0.103423285\nS20 -0.31209067 -0.24618792 -0.0271824151 -0.091732053\nS21 -0.25594533 -0.46246075 -0.0354205351 -0.180973464\nS22  0.24323679 -0.72159949  0.1819244804  0.009669496\nS23 -0.39736429 -0.53241824  0.0374850854 -0.145733335\nS24  0.43932542  0.60742531 -0.0909990349 -0.048500568\nS25 -0.15806719 -0.14778266 -0.4470511938  0.249196626\nS26 -0.38708608 -0.06975558  0.1101065003  0.024382817\nS27 -0.58223511  0.51151147  0.1409060157  0.199078478\nS28  0.88253998  0.27851440  0.1417856127  0.037595473\nS29  0.73515615  0.31027906 -0.0028221033 -0.066796286\n\n\n\np_load(\"vegan\")\n\n# Visualisierung der Anteile erklärter Varianz, im Vergleich zu einem Broken-Stick-Modell\nscreeplot(pca_1, bstick = TRUE)\n\n\n\n\n\n\n\n\n\n\nPCA visualisieren\n\n# Mit biplot von base R\nbiplot(pca_1)\n\n\n\n\n\n\n\n# Mit package factoextra \np_load(\"factoextra\")\n\n# Biplot\nfviz_pca_biplot(pca_1)\n\n\n\n\n\n\n\n# Biplot der 1 und 3 Achse\nfviz_pca_biplot(pca_1,  axes = c(1, 3) )\n\n\n\n\n\n\n\n# Biplot mit angepasssten Grafikparametern\n# repl = TRUE verhindert die Überlagerung der Textlabels\nfviz_pca_biplot(pca_1, repel = TRUE, \n                col.var = \"red\",  col.ind = \"black\") +\n    ggtitle(NULL) +\n    theme_classic()\n\n\n\n\n\n\n\n# Nur Inidviudals (hier Sites)\nfviz_pca_ind(pca_1, repel = TRUE) +\n  theme_classic()\n\n\n\n\n\n\n\n# Nur Variables (hier Umweltparameter)\nfviz_pca_var(pca_1, repel = TRUE) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Statistik 7: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik7_Uebung.html",
    "href": "statistik/Statistik7_Uebung.html",
    "title": "Statistik 7: Übung",
    "section": "",
    "text": "Datensatz USstates vom package “HSAUR3”\nBeschreibung des Datensets mit command:\n?USstates\n\n-Führt mit den Datensatz “USstates” vom package “HSAUR3” eine PCA aus und visualisiert das Resultat mit einem Biplot.\n\n\n\n\n\n\nLösung Übung 7\n\n\n\n\n[Lösungstext folgt]\n\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n\n# Packete laden\nlibrary(\"pacman\")\np_load(\"tidyverse\", \"vegan\", \"factoextra\")\n\n\n# Daten laden\np_load(\"HSAUR3\")\ndata(USstates)\n?USstates\nstr(USstates)\n\n'data.frame':   10 obs. of  7 variables:\n $ Population     : num  3615 21198 2861 2341 812 ...\n $ Income         : num  3624 5114 4628 3098 4281 ...\n $ Illiteracy     : num  2.1 1.1 0.5 2.4 0.7 0.8 0.6 1 0.5 0.6\n $ Life.Expectancy: num  69 71.7 72.6 68.1 71.2 ...\n $ Homicide       : num  15.1 10.3 2.3 12.5 3.3 7.4 4.2 6.1 1.7 5.5\n $ Graduates      : num  41.3 62.6 59 41 57.6 53.2 60 50.2 52.3 57.1\n $ Freezing       : num  20 20 140 50 174 124 44 126 172 168\n\nsummary(USstates)\n\n   Population        Income       Illiteracy    Life.Expectancy\n Min.   :  472   Min.   :3098   Min.   :0.500   Min.   :68.09  \n 1st Qu.: 1180   1st Qu.:3972   1st Qu.:0.600   1st Qu.:70.53  \n Median : 2601   Median :4365   Median :0.750   Median :71.44  \n Mean   : 5686   Mean   :4249   Mean   :1.030   Mean   :70.97  \n 3rd Qu.: 8955   3rd Qu.:4611   3rd Qu.:1.075   3rd Qu.:71.99  \n Max.   :21198   Max.   :5114   Max.   :2.400   Max.   :72.56  \n    Homicide        Graduates        Freezing    \n Min.   : 1.700   Min.   :41.00   Min.   : 20.0  \n 1st Qu.: 3.525   1st Qu.:50.73   1st Qu.: 45.5  \n Median : 5.800   Median :55.15   Median :125.0  \n Mean   : 6.840   Mean   :53.43   Mean   :103.8  \n 3rd Qu.: 9.575   3rd Qu.:58.65   3rd Qu.:161.0  \n Max.   :15.100   Max.   :62.60   Max.   :174.0  \n\n\n\n# PCA durchführen\npca_1 &lt;- prcomp(USstates, scale = TRUE)\nsummary(pca_1)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.1024 1.3894 0.64719 0.34264 0.25132 0.18292 0.12781\nProportion of Variance 0.6315 0.2758 0.05984 0.01677 0.00902 0.00478 0.00233\nCumulative Proportion  0.6315 0.9073 0.96709 0.98386 0.99289 0.99767 1.00000\n\nscreeplot(pca_1, bstick = TRUE)\n\n\n\n\n\n\n\n\nWie wir in der Summary und im Screeplot sehen, wird ein sehr grosser Anteil der Varianz (63.2 %) durch die erste Achse erklärt. Die zweite Achse erklärt noch einen Anteil von 27.6 % während der dritten Achse nur noch 6.0 % Anteil der Varianz erklärt. Wir beschränken uns deshalb darauf, die ersten beiden Achsen zu visualisieren.\n\n# Visualisierung\nfviz_pca_biplot(pca_1, repel = TRUE, \n                col.var = \"blue\",  col.ind = \"black\") +\n    ggtitle(NULL) +\n  theme_classic()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Statistik 7: Übung</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik8_Demo.html",
    "href": "statistik/Statistik8_Demo.html",
    "title": "Statistik 8: Demo",
    "section": "",
    "text": "Cluster-Analyse k-means\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Statistik 8: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik8_Demo.html#cluster-analyse-k-means",
    "href": "statistik/Statistik8_Demo.html#cluster-analyse-k-means",
    "title": "Statistik 8: Demo",
    "section": "",
    "text": "Datenbeschreibung\nDer Datensatz enthält Daten zum Vorkommen von Fischarten und den zugehörigen Umweltvariablen im Fluss Doubs (Jura). Es gibt 29 Probestellen (sites), an denen jeweils die Abundanzen von 27 Fischarten (auf einer Skalen von 0 bis 5) erhoben wurden. In dieser Demo verwenden wir die Artdaten.\n\nlibrary(\"pacman\")\np_load(\"tidyverse\")\n\nspe &lt;- read_delim(\"./datasets/stat/Doubs_species.csv\", delim = \";\") |&gt;\n  column_to_rownames(var = \"Site\")\n\nstr(spe)\n\n'data.frame':   29 obs. of  27 variables:\n $ Cogo: num  0 0 0 0 0 0 0 0 0 1 ...\n $ Satr: num  3 5 5 4 2 3 5 0 1 3 ...\n $ Phph: num  0 4 5 5 3 4 4 1 4 4 ...\n $ Babl: num  0 3 5 5 2 5 5 3 4 1 ...\n $ Thth: num  0 0 0 0 0 0 0 0 0 1 ...\n $ Teso: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Chna: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Pato: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Lele: num  0 0 0 0 5 1 1 0 2 0 ...\n $ Sqce: num  0 0 0 1 2 2 1 5 2 1 ...\n $ Baba: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Albi: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Gogo: num  0 0 0 1 2 1 0 0 1 0 ...\n $ Eslu: num  0 0 1 2 4 1 0 0 0 0 ...\n $ Pefl: num  0 0 0 2 4 1 0 0 0 0 ...\n $ Rham: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Legi: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Scer: num  0 0 0 0 2 0 0 0 0 0 ...\n $ Cyca: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Titi: num  0 0 0 1 3 2 0 1 0 0 ...\n $ Abbr: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Icme: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Gyce: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Ruru: num  0 0 0 0 5 1 0 4 0 0 ...\n $ Blbj: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Alal: num  0 0 0 0 0 0 0 0 0 0 ...\n $ Anan: num  0 0 0 0 0 0 0 0 0 0 ...\n\nsummary(spe)\n\n      Cogo             Satr            Phph            Babl      \n Min.   :0.0000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:1.000  \n Median :0.0000   Median :1.000   Median :3.000   Median :2.000  \n Mean   :0.5172   Mean   :1.966   Mean   :2.345   Mean   :2.517  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :3.0000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      Thth             Teso             Chna             Pato       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.5172   Mean   :0.6552   Mean   :0.6207   Mean   :0.8966  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.0000  \n Max.   :4.0000   Max.   :5.0000   Max.   :3.0000   Max.   :4.0000  \n      Lele            Sqce            Baba            Albi      \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.000   1st Qu.:0.000  \n Median :1.000   Median :2.000   Median :0.000   Median :0.000  \n Mean   :1.483   Mean   :1.931   Mean   :1.483   Mean   :0.931  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:1.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n      Gogo            Eslu            Pefl            Rham            Legi  \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0  \n Median :1.000   Median :1.000   Median :1.000   Median :0.000   Median :0  \n Mean   :1.897   Mean   :1.379   Mean   :1.241   Mean   :1.138   Mean   :1  \n 3rd Qu.:4.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5  \n      Scer             Cyca             Titi            Abbr       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.7241   Mean   :0.8621   Mean   :1.552   Mean   :0.8966  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:3.000   3rd Qu.:1.0000  \n Max.   :5.0000   Max.   :5.0000   Max.   :5.000   Max.   :5.0000  \n      Icme             Gyce           Ruru            Blbj      \n Min.   :0.0000   Min.   :0.00   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.00   1st Qu.:0.000   1st Qu.:0.000  \n Median :0.0000   Median :0.00   Median :1.000   Median :0.000  \n Mean   :0.6207   Mean   :1.31   Mean   :2.172   Mean   :1.069  \n 3rd Qu.:0.0000   3rd Qu.:2.00   3rd Qu.:5.000   3rd Qu.:2.000  \n Max.   :5.0000   Max.   :5.00   Max.   :5.000   Max.   :5.000  \n      Alal            Anan      \n Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:0.000  \n Median :0.000   Median :0.000  \n Mean   :1.966   Mean   :0.931  \n 3rd Qu.:5.000   3rd Qu.:2.000  \n Max.   :5.000   Max.   :5.000  \n\n\nk-means ist eine lineare Methode und daher nicht für Artdaten geeignet), darum müssen wir unsere Daten transformieren (für die meisten anderen Daten ist die Funktion „scale“, welche jede Variable so skaliert, dass sie einen Mittelwert von 0 und einen Standardabweichungswert von 1 hat, besser geeignet) die Randsumme der Quadrate gleich eins machen\n\np_load(vegan)\nspe_norm &lt;- decostand(spe, \"normalize\")\n\n\n\nk-means clustering mit Artdaten\n\n# k-means-Clustering mit 4 Gruppen durchführen\nset.seed(123)\nkmeans_1 &lt;- kmeans(spe_norm, centers = 4, nstart = 100)\nkmeans_1$cluster\n\n S1  S2  S3  S4  S5  S6  S7  S8  S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 \n  3   3   3   3   1   3   3   1   3   3   3   3   3   3   1   1   1   1   2   2 \nS21 S22 S23 S24 S25 S26 S27 S28 S29 \n  2   4   4   4   2   2   2   2   2 \n\n#  Visualisierung\np_load(factoextra)\n\nfviz_cluster(kmeans_1, main = \"\", data = spe) +\n  theme_classic()\n\n\n\n\n\n\n\n\nWie viele Cluster (Gruppen) sollen definiert werden? Oft haben wir eine Vorstelung über den Range der Anzahl Cluster. Value criterions wie der Simple Structure Index (ssi) können eine zusätzliche Hilfe sein, um die geeignete Anzahl von Clustern zu finden.\n\n# k-means partionierung, 2 bis 10 Gruppen\nset.seed(123)\nkm_cascade &lt;- cascadeKM(spe_norm,  inf.gr = 2, sup.gr = 10, iter = 100, criterion = \"ssi\")\nkm_cascade$results\n\n     2 groups  3 groups  4 groups   5 groups  6 groups  7 groups  8 groups\nSSE 8.2149405 6.4768108 5.0719796 4.30155732 3.5856120 2.9523667 2.4840549\nssi 0.1312111 0.1675852 0.1240975 0.05927008 0.1178577 0.1444813 0.1369294\n     9 groups  10 groups\nSSE 2.0521888 1.75992916\nssi 0.1462769 0.09995081\n\nkm_cascade$partition\n\n    2 groups 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups\nS1         1        3        4        1        5        6        4        7\nS2         1        3        4        5        3        3        7        2\nS3         1        3        4        5        3        3        7        2\nS4         1        3        4        5        3        3        7        2\nS5         2        2        1        2        4        4        5        8\nS6         1        3        4        5        3        3        7        2\nS7         1        3        4        5        3        3        7        2\nS8         2        2        1        2        4        4        8        9\nS9         1        3        4        5        3        3        7        2\nS10        1        3        4        5        3        1        1        3\nS11        1        3        4        5        3        1        1        3\nS12        1        3        4        5        3        1        1        3\nS13        1        3        4        5        3        1        1        3\nS14        1        3        4        5        3        1        1        6\nS15        1        2        1        2        1        5        2        6\nS16        2        2        1        2        1        5        2        1\nS17        2        2        1        2        1        5        2        1\nS18        2        2        1        2        1        5        2        1\nS19        2        1        3        3        2        7        6        4\nS20        2        1        3        3        2        7        6        4\nS21        2        1        3        3        2        7        6        4\nS22        2        1        2        4        6        2        3        5\nS23        2        1        2        4        6        2        3        5\nS24        2        1        2        4        6        2        3        5\nS25        2        1        3        3        2        7        6        4\nS26        2        1        3        3        2        7        6        4\nS27        2        1        3        3        2        7        6        4\nS28        2        1        3        3        2        7        6        4\nS29        2        1        3        3        2        7        6        4\n    10 groups\nS1          3\nS2          4\nS3          4\nS4          4\nS5         10\nS6          8\nS7          4\nS8          9\nS9          8\nS10         5\nS11         5\nS12         5\nS13         5\nS14         1\nS15         1\nS16         6\nS17         6\nS18         6\nS19         7\nS20         7\nS21         7\nS22         2\nS23         2\nS24         2\nS25         7\nS26         7\nS27         7\nS28         7\nS29         7\n\n# Visualisierung citerion Simple Structure Index\nplot(km_cascade, sortg = TRUE)\n\n\n\n\n\n\n\n# k-means-Clustering mit 3 Gruppen durchführen\nset.seed(123)\nkmeans_2 &lt;- kmeans(spe_norm, centers = 3, nstart = 100)\n\n\n#  Clustering-Resultat in Ordinationsplots darstellen\nfviz_cluster(kmeans_2, main = \"\", data = spe) +\n  theme_classic()\n\n\n\n\n\n\n\n# Resultat intepretieren\nkmeans_2\n\nK-means clustering with 3 clusters of sizes 6, 11, 12\n\nCluster means:\n        Cogo        Satr       Phph       Babl        Thth        Teso\n1 0.06167791 0.122088022 0.26993915 0.35942538 0.032664966 0.135403325\n2 0.00000000 0.004866252 0.01822625 0.05081739 0.004866252 0.004866252\n3 0.10380209 0.542300691 0.50086515 0.43325916 0.114024105 0.075651573\n        Chna       Pato       Lele      Sqce       Baba       Albi       Gogo\n1 0.06212775 0.21568957 0.25887226 0.2722562 0.15647062 0.15743876 0.16822286\n2 0.09192201 0.06820012 0.12408793 0.2326491 0.17693085 0.09644087 0.26235343\n3 0.00000000 0.00000000 0.06983991 0.1237394 0.02385019 0.00000000 0.05670453\n        Eslu       Pefl      Rham       Legi       Scer       Cyca       Titi\n1 0.12276089 0.17261621 0.0793181 0.06190283 0.04516042 0.06190283 0.14539027\n2 0.17089496 0.12305815 0.1610382 0.15286338 0.11664707 0.11650273 0.19076381\n3 0.04722294 0.02949244 0.0000000 0.00000000 0.00000000 0.00000000 0.03833408\n        Abbr       Icme       Gyce       Ruru       Blbj      Alal       Anan\n1 0.01473139 0.00000000 0.03192175 0.32201597 0.01473139 0.1095241 0.04739636\n2 0.14226648 0.09686076 0.24352816 0.31984654 0.18061983 0.4497421 0.13725875\n3 0.00000000 0.00000000 0.00000000 0.01049901 0.00000000 0.0000000 0.00000000\n\nClustering vector:\n S1  S2  S3  S4  S5  S6  S7  S8  S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 \n  3   3   3   3   1   3   3   1   3   3   3   3   3   3   1   1   1   1   2   2 \nS21 S22 S23 S24 S25 S26 S27 S28 S29 \n  2   2   2   2   2   2   2   2   2 \n\nWithin cluster sum of squares by cluster:\n[1] 1.736145 2.230527 2.510139\n (between_SS / total_SS =  57.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n# Zuordnung Sites zu den Clustern (separat)\nkmeans_2$cluster\n\n S1  S2  S3  S4  S5  S6  S7  S8  S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 \n  3   3   3   3   1   3   3   1   3   3   3   3   3   3   1   1   1   1   2   2 \nS21 S22 S23 S24 S25 S26 S27 S28 S29 \n  2   2   2   2   2   2   2   2   2 \n\n# Anzahl Sites pro Cluster\nkmeans_2$size\n\n[1]  6 11 12\n\n# Mittlere Abundance für jede Variable (Fischart) in jedem Cluster (mit untransformierten Daten)\naggregate(spe, by = list(cluster = kmeans_2$cluster), mean)\n\n  cluster      Cogo       Satr      Phph      Babl       Thth       Teso\n1       1 0.6666667 1.33333333 2.8333333 3.6666667 0.33333333 1.50000000\n2       2 0.0000000 0.09090909 0.2727273 0.7272727 0.09090909 0.09090909\n3       3 0.9166667 4.00000000 4.0000000 3.5833333 1.00000000 0.75000000\n       Chna     Pato      Lele     Sqce     Baba     Albi     Gogo      Eslu\n1 0.6666667 2.333333 2.8333333 2.500000 1.666667 1.666667 1.833333 1.3333333\n2 1.2727273 1.090909 1.7272727 2.636364 2.727273 1.545455 3.454545 2.4545455\n3 0.0000000 0.000000 0.5833333 1.000000 0.250000 0.000000 0.500000 0.4166667\n      Pefl      Rham      Legi     Scer      Cyca      Titi      Abbr     Icme\n1 1.833333 0.8333333 0.6666667 0.500000 0.6666667 1.5000000 0.1666667 0.000000\n2 2.000000 2.5454545 2.2727273 1.636364 1.9090909 2.9090909 2.2727273 1.636364\n3 0.250000 0.0000000 0.0000000 0.000000 0.0000000 0.3333333 0.0000000 0.000000\n       Gyce       Ruru      Blbj     Alal     Anan\n1 0.3333333 3.16666667 0.1666667 1.166667 0.500000\n2 3.2727273 3.90909091 2.7272727 4.545455 2.181818\n3 0.0000000 0.08333333 0.0000000 0.000000 0.000000\n\n# Mittlere Fisch-Artenzahl in jedem Cluster\naggregate( specnumber(spe), by = list(cluster = kmeans_2$cluster), mean)\n\n  cluster         x\n1       1 16.833333\n2       2 18.000000\n3       3  6.333333\n\n# Unterschiede Mittlere Fisch-Artenzahl pro Cluster testen\n\n# File für Anova erstellen\nspe_2 &lt;- data.frame(spe, \n                    \"cluster\" = as.factor(kmeans_2$cluster), \n                    \"species_richness\" = specnumber(spe))\nstr(spe_2)\n\n'data.frame':   29 obs. of  29 variables:\n $ Cogo            : num  0 0 0 0 0 0 0 0 0 1 ...\n $ Satr            : num  3 5 5 4 2 3 5 0 1 3 ...\n $ Phph            : num  0 4 5 5 3 4 4 1 4 4 ...\n $ Babl            : num  0 3 5 5 2 5 5 3 4 1 ...\n $ Thth            : num  0 0 0 0 0 0 0 0 0 1 ...\n $ Teso            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Chna            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Pato            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Lele            : num  0 0 0 0 5 1 1 0 2 0 ...\n $ Sqce            : num  0 0 0 1 2 2 1 5 2 1 ...\n $ Baba            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Albi            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Gogo            : num  0 0 0 1 2 1 0 0 1 0 ...\n $ Eslu            : num  0 0 1 2 4 1 0 0 0 0 ...\n $ Pefl            : num  0 0 0 2 4 1 0 0 0 0 ...\n $ Rham            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Legi            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Scer            : num  0 0 0 0 2 0 0 0 0 0 ...\n $ Cyca            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Titi            : num  0 0 0 1 3 2 0 1 0 0 ...\n $ Abbr            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Icme            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Gyce            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Ruru            : num  0 0 0 0 5 1 0 4 0 0 ...\n $ Blbj            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Alal            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Anan            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ cluster         : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 3 3 1 3 3 1 3 3 ...\n $ species_richness: int  1 3 4 8 11 10 5 5 6 6 ...\n\naov_1 &lt;- aov(species_richness~cluster, data = spe_2)\nsummary(aov_1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncluster      2  896.4   448.2   11.99 0.000204 ***\nResiduals   26  971.5    37.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\np_load(\"agricolae\")\n( Tukey &lt;- HSD.test(aov_1, \"cluster\") )\n\n$statistics\n   MSerror Df     Mean       CV\n  37.36538 26 12.93103 47.27173\n\n$parameters\n   test  name.t ntr StudentizedRange alpha\n  Tukey cluster   3         3.514171  0.05\n\n$means\n  species_richness      std  r       se Min Max   Q25  Q50   Q75\n1        16.833333 7.440878  6 2.495509   5  23 12.50 19.5 22.75\n2        18.000000 7.720104 11 1.843055   3  26 14.50 22.0 22.00\n3         6.333333 2.994945 12 1.764591   1  11  4.75  6.0  8.50\n\n$comparison\nNULL\n\n$groups\n  species_richness groups\n2        18.000000      a\n1        16.833333      a\n3         6.333333      b\n\nattr(,\"class\")\n[1] \"group\"\n\nsig_letters &lt;- Tukey$groups[order(row.names(Tukey$groups)), ]\n\nggplot(spe_2, aes(x = cluster,  y = species_richness)) + \n  geom_boxplot() +\n  geom_text(data = sig_letters, \n            aes(label = groups, x = c(1:3), y = max(spe_2$species_richness) * 1.2)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nBorcard, Daniel, François Gillet, Pierre Legendre, u. a. 2011. Numerical ecology with R. Bd. 2. Springer.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Statistik 8: Demo</span>"
    ]
  },
  {
    "objectID": "statistik/Statistik8_Uebung.html",
    "href": "statistik/Statistik8_Uebung.html",
    "title": "Statistik 8: Übung",
    "section": "",
    "text": "Datensatz crime.csv\n\nRaten von 7 Kriminalitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA\n\n(a) Führt eine k-means- Clusteranalyse durch.\n(b) Überlegt bzw testet viele Cluster sinnvoll sind\n(c) Abschliessend entscheidet euch für eine Clusterung und vergleicht die erhaltenen Cluster bezüglich der Kriminalitätsformen mittels ANOVA und interpretiert die Cluster entsprechend.\n\nHinweis: Wegen der sehr ungleichen Varianzen muss auf jeden Fall eine Standardisierung stattfinden, damit Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können\n\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein mit Quarto generiertes html-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nZu erstellen sind (a) Ein quarto generiertes html-Dokument mit begründetem Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (b) ausformulierter Methoden- und Ergebnisteil\n\n\n\n\n\n\n\nLösung Übung 8\n\n\n\n\n[Lösungstext folgt]\n\nDemoscript herunterladen (.R)\nDemoscript herunterladen (.qmd)\n\nlibrary(\"pacman\")\np_load(\"tidyverse\")\n\ncrime &lt;- read_delim(\"./datasets/stat/crime.csv\", delim = \";\", col_names = TRUE) |&gt;\n  column_to_rownames(var = \"State\")\n\nstr(crime)\n\n'data.frame':   50 obs. of  7 variables:\n $ Murder  : num  2 2.2 2 3.6 3.5 4.6 10.7 5.2 5.5 5.5 ...\n $ Rape    : num  14.8 21.5 21.8 29.7 21.4 23.8 30.5 33.2 25.1 38.6 ...\n $ Robbery : num  28 24 22 193 119 192 514 269 152 142 ...\n $ Assault : num  102 92 103 331 192 205 431 265 176 235 ...\n $ Burglary: num  803 755 949 1071 1294 ...\n $ Theft   : num  2347 2208 2697 2189 2568 ...\n $ Vehicle : num  164 228 181 906 705 447 637 776 354 376 ...\n\nsummary(crime)\n\n     Murder            Rape          Robbery         Assault     \n Min.   : 1.000   Min.   :11.60   Min.   :  7.0   Min.   : 32.0  \n 1st Qu.: 3.700   1st Qu.:23.27   1st Qu.: 67.0   1st Qu.:176.5  \n Median : 6.300   Median :30.10   Median :109.5   Median :248.0  \n Mean   : 6.776   Mean   :33.85   Mean   :142.1   Mean   :275.7  \n 3rd Qu.: 9.275   3rd Qu.:43.45   3rd Qu.:202.8   3rd Qu.:366.0  \n Max.   :13.500   Max.   :72.70   Max.   :514.0   Max.   :605.0  \n    Burglary        Theft         Vehicle     \n Min.   : 385   Min.   :1358   Min.   : 99.0  \n 1st Qu.: 894   1st Qu.:2366   1st Qu.:209.8  \n Median :1148   Median :2812   Median :328.0  \n Mean   :1197   Mean   :2918   Mean   :382.2  \n 3rd Qu.:1425   3rd Qu.:3382   3rd Qu.:529.5  \n Max.   :2221   Max.   :4373   Max.   :906.0  \n\n\n\ncrimez &lt;- scale(crime)\n\n„scale“ führt eine Standardisierung (z-Transformation) durch, so dass alle Variablen anschiessen einen Mittelwert von 0 und eine SD von 1 haben. Anschliessend wird das SSI-Kriterium getestet und zwar für Partitionierungen von 2 bis 6 Gruppen (wie viele Gruppen man maximal haben will, muss man pragmatisch nach der jeweiligen Fragestelltung entscheiden).\n\np_load(\"vegan\")\nset.seed(123)\nkm_cascade &lt;- cascadeKM(crimez, inf.gr = 2, sup.gr = 6, iter = 100, criterion = \"ssi\")\n\nkm_cascade$results\n\n      2 groups   3 groups   4 groups   5 groups  6 groups\nSSE 174.959159 144.699605 124.437221 108.119280 95.446705\nssi   1.226057   1.304674   1.555594   1.539051  1.507395\n\nkm_cascade$partition\n\n   2 groups 3 groups 4 groups 5 groups 6 groups\nME        2        3        3        2        1\nNH        2        3        3        2        1\nVT        2        3        3        2        1\nMA        1        1        2        1        6\nRI        2        1        4        1        6\nCT        2        1        4        1        6\nNY        1        1        2        4        4\nNJ        1        1        2        1        6\nPA        2        3        4        5        3\nOH        2        1        4        5        2\nIN        2        3        4        5        3\nIL        1        1        2        4        4\nMI        1        2        1        4        4\nWI        2        3        3        2        1\nMN        2        3        3        2        1\nIA        2        3        3        2        1\nMO        1        1        4        5        2\nND        2        3        3        2        1\nSD        2        3        3        2        3\nNE        2        3        3        2        1\nKS        2        3        4        5        1\nDE        2        1        4        5        2\nMD        1        1        2        4        4\nVA        2        3        4        5        3\nWV        2        3        3        2        3\nNC        2        1        4        5        2\nSC        1        1        4        5        2\nGA        1        1        1        4        2\nFL        1        2        1        4        4\nKY        2        3        4        5        3\nTN        1        1        2        5        2\nAL        2        1        4        5        2\nMS        2        3        4        5        3\nAR        2        3        4        5        3\nLA        1        2        1        4        4\nOK        1        1        1        3        5\nTX        1        2        1        4        4\nMT        2        3        3        2        1\nID        2        3        3        2        1\nWY        2        3        3        2        1\nCO        1        2        1        3        5\nNM        1        2        1        3        5\nAZ        1        2        1        3        5\nUT        2        3        3        2        1\nNV        1        2        1        4        4\nWA        1        2        1        3        5\nOR        1        2        1        3        5\nCA        1        2        1        4        4\nAK        1        2        1        3        5\nHI        2        3        4        2        1\n\n# k-means visualisation\n#p_load(\"cclust\")\nplot(km_cascade, sortg = TRUE)\n\n\n\n\n\n\n\n\nNach SSI ist die 4-Gruppenlösung die beste, mit dieser wird also weitergerechnet.\n\n# 4 Kategorien sind nach SSI offensichtlich besonders gut\nset.seed(123)\nkmeans_1 &lt;- kmeans(crimez, 4, nstart = 100)\n\nkmeans_1\n\nK-means clustering with 4 clusters of sizes 14, 16, 14, 6\n\nCluster means:\n        Murder       Rape    Robbery    Assault     Burglary      Theft\n1 -1.088869933 -0.9575423 -0.9573223 -1.0018455 -0.966220768 -0.3729397\n2 -0.002098593 -0.2436615 -0.2668763 -0.1457373 -0.279054485 -0.5371659\n3  0.943560464  1.1705377  0.6331926  0.8825420  1.287858358  1.1323972\n4  0.344651676  0.1527746  1.4679727  0.6670075 -0.006342418 -0.3396250\n     Vehicle\n1 -0.9310433\n2 -0.3276196\n3  0.7120264\n4  1.3846917\n\nClustering vector:\nME NH VT MA RI CT NY NJ PA OH IN IL MI WI MN IA MO ND SD NE KS DE MD VA WV NC \n 1  1  1  4  2  2  4  4  2  2  2  4  3  1  1  1  2  1  1  1  2  2  4  2  1  2 \nSC GA FL KY TN AL MS AR LA OK TX MT ID WY CO NM AZ UT NV WA OR CA AK HI \n 2  3  3  2  4  2  2  2  3  3  3  1  1  1  3  3  3  1  3  3  3  3  3  2 \n\nWithin cluster sum of squares by cluster:\n[1] 19.37307 39.51883 48.95481 16.59050\n (between_SS / total_SS =  63.7 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nkmeans_1$size\n\n[1] 14 16 14  6\n\n#  Clustering-Resultat in Ordinationsplots darstellen\np_load(factoextra)\nfviz_cluster(kmeans_1, data = crimez, \n             ggtheme = theme_classic(), main = \"\" )\n\n\n\n\n\n\n\n# File für ANOVA (Originaldaten der Vorfälle, nicht die ztransformierten)\ncrime_KM4 &lt;- data.frame(crime, kmeans_1[1])\ncrime_KM4$cluster &lt;- as.factor(crime_KM4$cluster)\ncrime_KM4\n\n   Murder Rape Robbery Assault Burglary Theft Vehicle cluster\nME    2.0 14.8      28     102      803  2347     164       1\nNH    2.2 21.5      24      92      755  2208     228       1\nVT    2.0 21.8      22     103      949  2697     181       1\nMA    3.6 29.7     193     331     1071  2189     906       4\nRI    3.5 21.4     119     192     1294  2568     705       2\nCT    4.6 23.8     192     205     1198  2758     447       2\nNY   10.7 30.5     514     431     1221  2924     637       4\nNJ    5.2 33.2     269     265     1071  2822     776       4\nPA    5.5 25.1     152     176      735  1654     354       2\nOH    5.5 38.6     142     235      988  2574     376       2\nIN    6.0 25.9      90     186      887  2333     328       2\nIL    8.9 32.4     325     434     1180  2938     628       4\nMI   11.3 67.4     301     424     1509  3378     800       3\nWI    3.1 20.1      73     162      783  2802     254       1\nMN    2.5 31.8     102     148     1004  2785     288       1\nIA    1.8 12.5      42     179      956  2801     158       1\nMO    9.2 29.2     170     370     1136  2500     439       2\nND    1.0 11.6       7      32      385  2049     120       1\nSD    4.0 17.7      16      87      554  1939      99       1\nNE    3.1 24.6      51     184      748  2677     168       1\nKS    4.4 32.9      80     252     1188  3008     258       2\nDE    4.9 56.9     124     241     1042  3090     272       2\nMD    9.0 43.6     304     476     1296  2978     545       4\nVA    7.1 26.5     106     167      813  2522     219       2\nWV    5.9 18.9      41      99      625  1358     169       1\nNC    8.1 26.4      88     354     1225  2423     208       2\nSC    8.6 41.3      99     525     1340  2846     277       2\nGA   11.2 43.9     214     319     1453  2984     430       3\nFL   11.7 52.7     367     605     2221  4373     598       3\nKY    6.7 23.1      83     222      824  1740     193       2\nTN   10.4 47.0     208     274     1325  2126     544       4\nAL   10.1 28.4     112     408     1159  2304     267       2\nMS   11.2 25.8      65     172     1076  1845     150       2\nAR    8.1 28.9      80     278     1030  2305     195       2\nLA   12.8 40.1     224     482     1461  3417     442       3\nOK    8.1 36.4     107     285     1787  3142     649       3\nTX   13.5 51.6     240     354     2049  3987     714       3\nMT    2.9 17.3      20     118      783  3314     215       1\nID    3.2 20.0      21     178     1003  2800     181       1\nWY    5.3 21.9      22     243      817  3078     169       1\nCO    7.0 42.3     145     329     1792  4231     486       3\nNM   11.5 46.9     130     538     1845  3712     343       3\nAZ    9.3 43.0     169     437     1908  4337     419       3\nUT    3.2 25.3      59     180      915  4074     223       1\nNV   12.6 64.9     287     354     1604  3489     478       3\nWA    5.0 53.4     135     244     1861  4267     315       3\nOR    6.6 51.1     206     286     1967  4163     402       3\nCA   11.3 44.9     343     521     1696  3384     762       3\nAK    8.6 72.7      88     401     1162  3910     604       3\nHI    4.8 31.0     106     103     1339  3759     328       2\n\nstr(crime_KM4)\n\n'data.frame':   50 obs. of  8 variables:\n $ Murder  : num  2 2.2 2 3.6 3.5 4.6 10.7 5.2 5.5 5.5 ...\n $ Rape    : num  14.8 21.5 21.8 29.7 21.4 23.8 30.5 33.2 25.1 38.6 ...\n $ Robbery : num  28 24 22 193 119 192 514 269 152 142 ...\n $ Assault : num  102 92 103 331 192 205 431 265 176 235 ...\n $ Burglary: num  803 755 949 1071 1294 ...\n $ Theft   : num  2347 2208 2697 2189 2568 ...\n $ Vehicle : num  164 228 181 906 705 447 637 776 354 376 ...\n $ cluster : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 4 2 2 4 4 2 2 ...\n\n\nDamit die Boxplots und die ANOVA direkt interpretierbar sind, werden für diese, anders als für die Clusterung, die untransformierten Incidenz-Werte verwendet (also crime statt crimez). Die Spalte mit der Clusterzugehörigkeit im Fall von k-means mit 4 Clustern hängt man als Spalte an (Achtung: muss als Faktor definiert werden!).\nAnschliessend kann man die 7 ANOVAs rechnen, die Posthoc-Vergleiche durchführen und die zugehörigen Boxplots mit Buchstaben für die homogenen Gruppen erzeugen. Sinnvollerweise gruppiert man die Abbildungen gleich, z. B. je 2 x 2. Das Skript ist hier simple für jede Verbrechensart wiederholt.\n\np_load(\"multcomp\")\n\naov_Murder &lt;- aov(Murder ~ cluster, data = crime_KM4)\nsummary(aov_Murder)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncluster      3  355.4  118.46   23.75 1.96e-09 ***\nResiduals   46  229.4    4.99                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nletters &lt;- cld(glht(aov_Murder, linfct = mcp(cluster = \"Tukey\")))\ncld_letters &lt;- data.frame(cluster = names(letters$mcletters$Letters), \n                          letters = letters$mcletters$Letters)\nf_Murder &lt;-\n  ggplot(crime_KM4, aes(x = cluster,  y = Murder)) + \n  geom_boxplot() +\n  geom_text(data = cld_letters, aes(label = letters, x = c(1:4), y = max(crime_KM4$Murder) * 1.2)) +\n  ylim(0, max(crime_KM4$Murder) * 1.2) +\n  theme_classic()\n#\naov_Rape &lt;- aov(Rape ~ cluster, data = crime_KM4)\nsummary(aov_Rape)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncluster      3   6945  2315.0   31.95 2.58e-11 ***\nResiduals   46   3333    72.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nletters &lt;- cld(glht(aov_Rape, linfct = mcp(cluster = \"Tukey\")))\ncld_letters &lt;- data.frame(cluster = names(letters$mcletters$Letters), \n                          letters = letters$mcletters$Letters)\nf_Rape &lt;-\n  ggplot(crime_KM4, aes(x = cluster,  y = Rape)) + \n  geom_boxplot() +\n  geom_text(data = cld_letters, aes(label = letters, x = c(1:4), y = max(crime_KM4$Rape)* 1.2)) +\n  ylim(0, max(crime_KM4$Rape) * 1.2) +\n  theme_classic()\n#\naov_Robbery &lt;- aov(Robbery ~ cluster, data = crime_KM4)\nsummary(aov_Robbery)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncluster      3 386563  128854   30.24 5.96e-11 ***\nResiduals   46 196025    4261                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nletters &lt;- cld(glht(aov_Robbery, linfct = mcp(cluster = \"Tukey\")))\ncld_letters &lt;- data.frame(cluster = names(letters$mcletters$Letters), \n                          letters = letters$mcletters$Letters)\nf_Robbery &lt;-\n  ggplot(crime_KM4, aes(x = cluster,  y = Robbery)) + \n  geom_boxplot() +\n  geom_text(data = cld_letters, aes(label = letters, x = c(1:4), y = max(crime_KM4$Robbery) * 1.2)) +\n  ylim(0, max(crime_KM4$Robbery) * 1.2) +\n  theme_classic()\n#\naov_Assault &lt;- aov(Assault ~ cluster, data = crime_KM4)\nsummary(aov_Assault)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncluster      3 541786  180595   20.39 1.51e-08 ***\nResiduals   46 407517    8859                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nletters &lt;- cld(glht(aov_Rape, linfct = mcp(cluster = \"Tukey\")))\ncld_letters &lt;- data.frame(cluster = names(letters$mcletters$Letters), \n                          letters = letters$mcletters$Letters)\nf_Ausault &lt;-\n  ggplot(crime_KM4, aes(x = cluster,  y = Assault)) + \n  geom_boxplot() +\n  geom_text(data = cld_letters, aes(label = letters, x = c(1:4), y = max(crime_KM4$Assault) * 1.2)) +\n  ylim(0, max(crime_KM4$Assault) * 1.2) +\n  theme_classic()\n#\naov_Burglary &lt;- aov(Burglary ~ cluster, data = crime_KM4)\nsummary(aov_Burglary)\n\n            Df  Sum Sq Mean Sq F value  Pr(&gt;F)    \ncluster      3 6602474 2200825   50.21 1.5e-14 ***\nResiduals   46 2016382   43834                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nletters &lt;- cld(glht(aov_Burglary, linfct = mcp(cluster = \"Tukey\")))\ncld_letters &lt;- data.frame(cluster = names(letters$mcletters$Letters), \n                          letters = letters$mcletters$Letters)\nf_Burglary &lt;-\n  ggplot(crime_KM4, aes(x = cluster,  y = Burglary)) + \n  geom_boxplot() +\n  geom_text(data = cld_letters, aes(label = letters, x = c(1:4), y = max(crime_KM4$Burglary)* 1.2)) +\n  ylim(0, max(crime_KM4$Burglary) * 1.2) +\n  theme_classic()\n#\naov_Theft &lt;- aov(Theft ~ cluster, data = crime_KM4)\nsummary(aov_Theft)\n\n            Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \ncluster      3 14249791 4749930   16.25 2.44e-07 ***\nResiduals   46 13448760  292364                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nletters &lt;- cld(glht(aov_Theft, linfct = mcp(cluster = \"Tukey\")))\ncld_letters &lt;- data.frame(cluster = names(letters$mcletters$Letters), \n                          letters = letters$mcletters$Letters)\nf_Theft &lt;-\n  ggplot(crime_KM4, aes(x = cluster,  y = Theft)) + \n  geom_boxplot() +\n  geom_text(data = cld_letters, aes(label = letters, x = c(1:4), y = max(crime_KM4$Theft)* 1.2)) +\n  ylim(0, max(crime_KM4$Theft) * 1.2) +\n  theme_classic()\n#\naov_Vehicle &lt;- aov(Vehicle ~ cluster, data = crime_KM4)\nsummary(aov_Vehicle)\n\n            Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \ncluster      3 1427939  475980   30.08 6.46e-11 ***\nResiduals   46  727932   15825                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nletters &lt;- cld(glht(aov_Vehicle, linfct = mcp(cluster = \"Tukey\")))\ncld_letters &lt;- data.frame(cluster = names(letters$mcletters$Letters), \n                          letters = letters$mcletters$Letters)\nf_Vehicle &lt;-\n  ggplot(crime_KM4, aes(x = cluster,  y = Vehicle)) + \n  geom_boxplot() +\n  geom_text(data = cld_letters, aes(label = letters, x = c(1:4), y = max(crime_KM4$Vehicle) * 1.2)) +\n  ylim(0, max(crime_KM4$Vehicle) * 1.2) +\n  theme_classic()\n\n\np_load(patchwork)\n\n  f_Murder + f_Rape + f_Robbery  + \n  f_Ausault + f_Burglary + f_Theft +\n  f_Vehicle +\n  plot_layout(ncol = 3, nrow = 3)\n\n\n\n\n\n\n\n\nDie Boxplots erlauben jetzt auch eine Beurteilung der Modelldiagnostik: Die Residuen sind hinreichen normalverteilt (symmetrisch)",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Statistik 8: Übung</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman0_Vorbereitung.html",
    "href": "rauman/Rauman0_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von Rauman 1 - 5 werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog der Vorbereitungsübung in Prepro1 könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\npackages &lt;- c(\"sf\", \"dplyr\", \"ggplot2\", \"spatstat.geom\", \"spatstat.explore\",\n  \"gstat\", \"tidyr\", \"terra\")\n\n\nsapply(packages,\\(x) pacman::p_install(x, force = FALSE, character.only = TRUE))\n\n\n\n\n\n\n\nHinweis\n\n\n\nBei tmap ist die CRAN Version veraltet. Hier brauchen wir die neuste Version von GitHub, welche man mit folgendem Befehl installieren kann:\n\npacman::p_install_gh(\"r-tmap/tmap\")\n\n\n\nZudem könnt ihr die Daten für die Übungen auf Moodle herunterladen.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html",
    "href": "rauman/Rauman1_Uebung_A.html",
    "title": "Rauman 1: Übung A",
    "section": "",
    "text": "Aufgabe 1: Vektor Daten importieren\nFür die kommenden Übungen könnt ihr folgende Packages installieren bzw. laden:\nImportiere die Datensätze kantone.gpkg und gemeinden.gpkg wie folgt. Es handelt sich um Geodatensätze im Format Geopackage (“*.gpkg”), ein alternatives Datenformat zum bekannteren Format “Shapefiles”.\nkantone &lt;- read_sf(\"datasets/rauman/kantone.gpkg\")\ngemeinden &lt;- read_sf(\"datasets/rauman/gemeinden.gpkg\")\nSchau Dir die importierten Datensätze an.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Rauman 1: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-1-vektor-daten-importieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-1-vektor-daten-importieren",
    "title": "Rauman 1: Übung A",
    "section": "",
    "text": "Hinweis\n\n\n\nAm meisten Informationen zu sf Objekten bekommst du, wenn du dir den Datensatz in der Konsole anschaust (in dem du den Variabel-Name in der Konsole eintippst). Mit dem RStudio Viewer werden sf Objekte nur sehr langsam geladen und die Metadaten werden nicht angezeigt.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Rauman 1: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-2-daten-visualisieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-2-daten-visualisieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 2: Daten visualisieren",
    "text": "Aufgabe 2: Daten visualisieren\nEine sehr einfache Möglichkeit, sf-Objekte zu visualiseren, ist die base-R Funktion plot(). Führe die angegebenen R-Befehle aus und studiere die entstehenden Plots. Welche Unterschiede findest Du? Wie erklärst Du diese Unterschiede?\n\n# ohne max.plot = 1 macht R einen Plot pro Spalte\nplot(gemeinden, max.plot = 1)\n\n\n\n\n\n\n\n\n# Alternativ kann man auch eine spezifische Spalte plotten\nplot(kantone[\"KANTONSFLA\"])",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Rauman 1: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#input-koodinatensysteme",
    "href": "rauman/Rauman1_Uebung_A.html#input-koodinatensysteme",
    "title": "Rauman 1: Übung A",
    "section": "Input: Koodinatensysteme",
    "text": "Input: Koodinatensysteme\nIn der obigen Visualierung fällt folgendes auf:\n\ndie X/Y Achsen weisen zwei ganz unterschiedliche Zahlenbereiche auf (vergleiche die Achsenbeschriftungen)\nder Umriss der Schweiz sieht in den beiden Datensätzen unterschiedlich aus (kantone ist gegenüber gemeinden gestaucht)\n\nDies hat natürlich damit zu tun, dass die beiden Datensätze in unterschiedlichen Koordinatensystemen erfasst wurden. Koordinatensysteme werden mit CRS (Coordinate Reference System) abgekürzt. Mit st_crs() können die zugewiesenen Koordinatensysteme abgefragt werden.\n\nst_crs(kantone)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"Unknown engineering datum\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\nst_crs(gemeinden)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"Unknown engineering datum\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\n\nLeider sind in unserem Fall keine Koordinatensysteme zugewiesen. Mit etwas Erfahrung kann man das Koordinatensystem aber erraten, so viele kommen nämlich gar nicht in Frage. Am häufigsten trifft man hierzulande eines der drei folgenden Koordinatensysteme an:\n\nCH1903 LV03: das alte Koordinatensystem der Schweiz\nCH1903+ LV95: das neue Koordinatensystem der Schweiz\nWGS84: ein häufig genutztes, weltumspannendes geodätisches Koordinatensystem, sprich die Koordinaten werden in Länge und Breite angegeben (Lat/Lon).\n\nNun gilt es, anhand der Koordinaten, die in der Spalte geometry ersichtlich sind, das korrekte Koordinatensystem festzustellen. Wenn man auf map.geo.admin.ch mit der rechten Maustaste einen Ort anwählt, erfährt man die Koordinaten dieses Ortes in verschiedenen Koordinatenbezugssystemen.\n\n\nWenn man diese Koordinaten mit den Koordinaten unserer Datensätze vergleicht, dann ist schnell klar, dass es sich beim Datensatz kantone um das Koordinatenbezugsystem (CRS) WGS84 handelt. Wir können diese Information nutzen, um das CRS unserers Datensatzes mit st_set_crs() zu setzen.\n\n# Zuweisen mit st_set_crs()...\nkantone &lt;- st_set_crs(kantone, \"WGS84\")\n\nWenn wir die CRS Information nun abrufen, sehen wir, dass diese Zuweisung funktioniert hat.\n\n# ... abfragen mit st_crs()\nst_crs(kantone)\n## Coordinate Reference System:\n##   User input: WGS84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     DATUM[\"World Geodetic System 1984\",\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     ID[\"EPSG\",4326]]\n\nEtwas komplizierter ist es, wenn wir das CRS vom Datensatz gemeinden setzen wollen. Im Vergleich mit map.geo.admin.ch sehen wir, dass es sich hier um das CRS CH1903+ LV95 handeln muss. Wenn wir diesen Namen für unsere CRS Zuweisung verwenden möchten, funktioniert das nicht:\n\n# Zuweisen mit st_set_crs()...\ngemeinden &lt;- st_set_crs(gemeinden, \"CH1903+ LV95\")\n\n# ... abfragen mit st_crs()\nst_crs(gemeinden)\n## Coordinate Reference System: NA\n\nDie ausgeschriebenen Namen dieser CRS sind fehleranfällig. Deshalb ist es besser, mit den jeweiligen EPSG Codes der Bezugssysteme zu arbeiten. Diese EPSG Codes kann man auf folgender Website erfahren: epsg.io/map. Es lohnt sich aber, die EPSG Codes der für uns relevanten CRS zu notieren:\n\nCH1903 LV03: EPSG:21781\nCH1903+ LV95: EPSG:2056\nWGS84: EPSG:4326\n\nDiesen Code können wir nutzen, um das CRS des Datensatz gemeinde zu setzen:\n\n# Zuweisen mit st_set_crs()...\ngemeinden &lt;- st_set_crs(gemeinden, 2056)\n\n# ... abfragen mit st_crs()\nst_crs(gemeinden)\n## Coordinate Reference System:\n##   User input: EPSG:2056 \n##   wkt:\n## PROJCRS[\"CH1903+ / LV95\",\n##     BASEGEOGCRS[\"CH1903+\",\n##         DATUM[\"CH1903+\",\n##             ELLIPSOID[\"Bessel 1841\",6377397.155,299.1528128,\n##                 LENGTHUNIT[\"metre\",1]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         ID[\"EPSG\",4150]],\n##     CONVERSION[\"Swiss Oblique Mercator 1995\",\n##         METHOD[\"Hotine Oblique Mercator (variant B)\",\n##             ID[\"EPSG\",9815]],\n##         PARAMETER[\"Latitude of projection centre\",46.9524055555556,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8811]],\n##         PARAMETER[\"Longitude of projection centre\",7.43958333333333,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8812]],\n##         PARAMETER[\"Azimuth of initial line\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8813]],\n##         PARAMETER[\"Angle from Rectified to Skew Grid\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8814]],\n##         PARAMETER[\"Scale factor on initial line\",1,\n##             SCALEUNIT[\"unity\",1],\n##             ID[\"EPSG\",8815]],\n##         PARAMETER[\"Easting at projection centre\",2600000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8816]],\n##         PARAMETER[\"Northing at projection centre\",1200000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8817]]],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1]],\n##     USAGE[\n##         SCOPE[\"Cadastre, engineering survey, topographic mapping (large and medium scale).\"],\n##         AREA[\"Liechtenstein; Switzerland.\"],\n##         BBOX[45.82,5.96,47.81,10.49]],\n##     ID[\"EPSG\",2056]]\n\nJetzt, wo das CRS der Datensätze bekannt ist, können wir ggplot2 nutzen, um usere Daten zu visualisieren. In InfoVis 1 & 2 haben wir intensiv mit ggplot2 gearbeitet und dort die Layers geom_point() und geom_line() kennengelernt. Zusätzlich beinhaltet ggplot die Möglichkeit, mit geom_sf() Vektordaten direkt und sehr einfach zu plotten.\n\n\nMusterlösung\nggplot() +\n  # bei geom_sf müssen weder x noch y Spalten angegeben werden\n  geom_sf(data = gemeinden)",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Rauman 1: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-3-koordinatensyteme-transformieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-3-koordinatensyteme-transformieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 3: Koordinatensyteme transformieren",
    "text": "Aufgabe 3: Koordinatensyteme transformieren\nIn der vorherigen Übung haben wir das bestehende Koordinatensystem zugewiesen. Dabei haben wir die bestehenden Koordinaten (in der Spalte geom) nicht manipuliert. Ganz anders ist eine Transformation der Daten von einem Koordinatensystem in das andere. Bei einer Transformation werden die Koordinaten in das neue Koordinatensystem umgerechnet und somit manipuliert. Aus praktischen Gründen wollen  wir all unsere Daten ins neue Schweizer Koordinatensystem CH1903+ LV95 transfomieren. Transformiere den Datensatz kantone mit st_transform()in CH1903+ LV95, nutze dafür den korrekten EPSG-Code.\nVor der Transformation (betrachte die Attribute Bounding box, Projected CRS sowie die Werte in der Spalte geom):\n\nkantone\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\n## Geodetic CRS:  WGS 84\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [°]&gt;\n\n\n\nMusterlösung\nkantone &lt;- st_transform(kantone, 2056)\n\n\nNach der Transformation (betrachte die Attribute Bounding box, Projected CRS sowie die Werte in der Spalte geom):\n\nkantone\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2485410 ymin: 1075268 xmax: 2833858 ymax: 1295934\n## Projected CRS: CH1903+ / LV95\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [m]&gt;",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Rauman 1: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-4-tidyverse-funktionen",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-4-tidyverse-funktionen",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 4: Tidyverse Funktionen",
    "text": "Aufgabe 4: Tidyverse Funktionen\nsf Objekte sind im wesentlichen data.frames mit ein paar Metadaten und einer speziellen geometry-Spalte. Wir können mit ihnen die gleichen Operationen durchführen wie mit data.frames. Beispielsweise können wir aus den Spalten EINWOHNERZ und KANTONSFLA die Einwohnerdichte berechnen:\n\nkantone &lt;- kantone |&gt;\n  mutate(\n    # hektaren in km2 konvertieren\n    flaeche_km2 = KANTONSFLA / 100,\n    # dichte pro km2 berechnen\n    bevoelkerungsdichte = EINWOHNERZ / flaeche_km2\n  )\n\nBerechne nun die Einwohnerdichte auf der Ebene der Gemeinden.\n\ngemeinden &lt;- gemeinden |&gt;\n  mutate(\n    flaeche_km2 = GEM_FLAECH / 100,\n    bevoelkerungsdichte = EINWOHNERZ / flaeche_km2\n  )",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Rauman 1: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-5-choroplethen-karte",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-5-choroplethen-karte",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 5: Choroplethen Karte",
    "text": "Aufgabe 5: Choroplethen Karte\nNun wollen wir die Gemeinden respektive die Kantone nach ihrer Bevölkerungsdichte einfärben. Dafür verwenden wir wie gewohnt die Methode aes(fill = ...) von ggplot.\n\n\nMusterlösung\nggplot(kantone) +\n  geom_sf(aes(fill = bevoelkerungsdichte))\n\n\n\n\n\n\n\n\n\nHier sind farblich kaum Unterschiede erkennbar, weil die extrem hohe Bevölkerungsdichte vom Halbkanton Basel-Stadt (&gt;5’000 Einwohner pro km2!) die ganze Farbskala dominiert. Der Statistischer Atlas der Schweiz löst das Problem, indem es Klassen mit irregulären Schwellenwerte verwendet und alle Zahlen &gt;2’000 gruppiert. Diese Vorgehensweise können wir mit cut() rekonstruieren.\n\n# Schwellwerte analog BFS \"Statistischer Atlas der Schweiz\"\nbreaks = c(0, 50, 100, 150, 200, 300, 500, 750, 1000, 2000, Inf)\n\n# Klassen auf der Basis dieser Schwellenwerte bilden\nkantone &lt;- kantone |&gt;\n    mutate(bevoelkerungsdichte_klassen = cut(bevoelkerungsdichte, breaks))\n\n# Farbpalette erstellen: Wir brauchen so viele Farben, wie wir \"breaks\" haben, minus 1\nncols &lt;- length(breaks) - 1\n\n# Farbpalette erstellen (siehe RColorBrewer::display.brewer.all())\nred_yellow_green &lt;- RColorBrewer::brewer.pal(ncols, \"RdYlGn\")\n\n# Farbpalette umdrehen (zu green-red-yellow)\ngreen_red_yellow &lt;- rev(red_yellow_green)\n\np_kantone &lt;- ggplot(kantone, aes(fill = bevoelkerungsdichte_klassen)) +\n  geom_sf(colour = NA) +\n  scale_fill_manual(values = green_red_yellow) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\nErstelle die gleichen Klassen für die Bevölkerungsdichte der Gemeinden und vergleiche die Plots.\n\n\nMusterlösung\ngemeinden &lt;- gemeinden |&gt;\n  mutate(bevoelkerungsdichte_klassen = cut(bevoelkerungsdichte, breaks))\n\np_gemeinden &lt;- ggplot(gemeinden, aes(fill = bevoelkerungsdichte_klassen)) +\n  geom_sf(colour = NA) +\n  scale_fill_manual(values = green_red_yellow) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Kantone\n\n\n\n\n\n\n\n\n\n\n\n(b) Gemeinde\n\n\n\n\n\n\n\nAbbildung 33.1: Der Vergleich dieser beiden Darstellungen veranschaulicht die MAUP Problematik sehr deutlich",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Rauman 1: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html",
    "href": "rauman/Rauman1_Uebung_B.html",
    "title": "Rauman 1: Übung B",
    "section": "",
    "text": "Aufgabe 1: Geopackage “Layers”\nFür die kommende Übung arbeiten wir mit dem Datensatz gruental.gpkg. Importiere ihn in R. Zudem brauchen wir die folgenden libraries:\nAllenfalls ist euch beim Importieren des Geopackage gruental.gpkg folgende Warnmeldung aufgefallen:\nDiese Warnmeldung weist darauf hin, dass das Geopackage gruental.gpkg mehrere Layers (rep. Datensätze) enthält und nur der erste Layer importiert wurde. Bringe mit dem Befehl st_layers die Layer-Namen in Erfahrung und nutze diese im Anschluss in read_sf (als Argument layer =), um die layers einzeln zu importieren und in Variablen zu speichern (z.B. als Variable wiesen und baeume).\nMusterlösung\nst_layers(\"datasets/rauman/gruental.gpkg\")\n## Driver: GPKG \n## Available layers:\n##   layer_name geometry_type features fields       crs_name\n## 1     wiesen       Polygon       37      1 CH1903+ / LV95\n## 2     baeume         Point      185      2 CH1903+ / LV95\n\nwiesen &lt;- read_sf(\"datasets/rauman/gruental.gpkg\", \"wiesen\")\nbaeume &lt;- read_sf(\"datasets/rauman/gruental.gpkg\", \"baeume\")",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Rauman 1: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html#aufgabe-1-geopackage-layers",
    "href": "rauman/Rauman1_Uebung_B.html#aufgabe-1-geopackage-layers",
    "title": "Rauman 1: Übung B",
    "section": "",
    "text": "Warning message:\nIn evalq((function (..., call. = TRUE, immediate. = FALSE, noBreaks. = FALSE,  :\n  automatically selected the first layer in a data source containing more than one.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Rauman 1: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html#aufgabe-2-datensätze-erkunden",
    "href": "rauman/Rauman1_Uebung_B.html#aufgabe-2-datensätze-erkunden",
    "title": "Rauman 1: Übung B",
    "section": "Aufgabe 2: Datensätze erkunden",
    "text": "Aufgabe 2: Datensätze erkunden\nNimm dir etwas Zeit und erkunde die beiden Datensätze. Nutze dafür auch die Visualisierungsmöglichkeiten von ggplot (insbesondere geom_sf). Du kannst mehrere geom_sf() übereinander lagern, um gleichzeitig mehrere Datensätze darzustellen.\n\nMusterlösung\nggplot(wiesen) +\n  geom_sf(aes(fill = flaechen_typ)) +\n  geom_sf(data = baeume) +\n  theme_void()\n\nggplot(wiesen) +\n  geom_sf() +\n  geom_sf(data = baeume, aes(colour = baum_typ)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nAbbildung 34.1: Wiesen-Flächen eingefärbt nach Typ\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 34.2: Bäume eingefärbt nach Baumtyp",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Rauman 1: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html#sec-spatial-join-points",
    "href": "rauman/Rauman1_Uebung_B.html#sec-spatial-join-points",
    "title": "Rauman 1: Übung B",
    "section": "Aufgabe 3: Spatial Join mit Punkten",
    "text": "Aufgabe 3: Spatial Join mit Punkten\nWir wollen nun für jeden Baum wissen, ob er sich in einer Wiese befindet oder nicht. Dazu nutzen wir die GIS-Technik Spatial Join, die in der Vorlesung beschrieben wurde. In sf können wir Spatial Joins mit der Funktion st_join durchführen, dabei gibt es nur left sowie inner-Joins (vgl. PrePro 1 & 2). So müssen die Punkte “Links” also an erster Stelle aufgeführt werden, da wir ja Attribute an die Punkte anheften wollen.\nBeachte, dass der Output eine neue Spalte flaechen_typ aufweist. Diese ist leer (NA), wenn sich der entsprechende Baum nicht in einer Wiese befindet. Wie viele Bäume befinden sich in einer Wiese, wie viele nicht?\n\n\nMusterlösung\nbaeume_join &lt;- st_join(baeume, wiesen)\n\nanzahl_in_wiese &lt;- sum(!is.na(baeume_join$flaechen_typ))\nanzahl_nicht_in_wiese &lt;- sum(is.na(baeume_join$flaechen_typ))",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Rauman 1: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html",
    "href": "rauman/Rauman2_Uebung_A.html",
    "title": "Rauman 2: Übung A",
    "section": "",
    "text": "Aufgabe 1\nIn der letzten Übung haben wir einen Spatial Join zwischen Bäumen und Wiesen durchgeführt, um herauszufinden, ob sich der Baum in einer Wiese befindet oder nicht. Basis waren dafür die Daten vom Campus Grüental (gruental.gpkg)\nHeute gehen wir einen Schritt weiter und wollen folgende Frage beantworten: Wie viel Wiese befinden sich in einem Umkreis von 20m um jeden Baum?\nLade dazu die benötigten Libraries und Datensätze in deine Session. Exploriere die Daten und visualisiere sie räumlich.\nUm die Übung etwas zu vereinfachen, arbeiten wir erstmals mit nur 10 Bäumen. Diese Bäume befinden sich im Shapefile baume_sample.shp (auf Moodle).\nAls erster Schritt müssen wir jeden Baum mit einem 20m Puffer versehen. Nutze dazu st_buffer und speichere den Output als baeume_20m. Schau dir baeume_20m nun genau an. Um welchen Geometrietyp handelt es sich dabei nun?\nMusterlösung\nbaeume_20m &lt;- st_buffer(baeume_sample, 20)\nMusterlösung\nggplot() +\n  geom_sf(data = wiesen) +\n  geom_sf(data = baeume_sample) +\n  geom_sf(data = baeume_20m, fill = NA)\n\n\n\n\n\n\n\n\nAbbildung 35.1: Dargestellt sind die Bäume als Punkte mit einem 20m Puffer, sowie die Wiesen im Hintergrund.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Rauman 2: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-2",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-2",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nBerechnen nun die Schnittmenge aus baeume_20m und wiesen mit der Funktion st_intersection und speichere den Output als baeume_wiesen. Exploriere nun baeume_wiesen. Was ist passiert? Überprüfe die Anzahl Zeilen pro Datensatz. Haben die sich verändert? Wenn ja, warum?\n\n\nMusterlösung\nbaeume_wiesen &lt;- st_intersection(baeume_20m, wiesen)\n\nggplot() +\n  geom_sf(data = wiesen, fill = \"blue\", alpha = .2) +\n  geom_sf(data = baeume_20m, fill = \"red\", alpha = .2) +\n  geom_sf(data = baeume_wiesen, fill = \"green\", alpha = 0.2)",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Rauman 2: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-3",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-3",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nBerechnen nun die Flächengrösse pro Geometrie mit der Funktion st_area(). Speichere den Output in einer neuen Spalte von baeume_wiesen (z.B. mit dem Namen wiesen_flaeche). Tipp: Konvertiere den Output aus st_area in einen nummerischen Vektor mit as.numeric().\n\n\nMusterlösung\nbaeume_wiesen$wiesen_flaeche &lt;- as.numeric(st_area(baeume_wiesen))",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Rauman 2: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-4-optional",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-4-optional",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 4 (Optional)",
    "text": "Aufgabe 4 (Optional)\nBerechne nun aus wiesen_flaeche den wiesen_anteil. Tipp: 100% ist die Kreisfläche aus \\(r^2\\times \\pi\\), wobei in unserem Fall \\(r = 20\\) entspricht.\n\n\nMusterlösung\nkreisflaeche &lt;- 20^2 * pi\nbaeume_wiesen$wiesen_anteil &lt;- baeume_wiesen$wiesen_flaeche / kreisflaeche\n\n\nÜberführe anschliessend die berechneten Anteilswerte in den Datensatz baeume mit einem left_join zwischen baeume und baeume_wiesen. Welche Spalte wäre für diesen Join geeignet? Hinweis: Nutze st_drop_geometry(), um die Geometriespalte in baeme_wiesen vor dem Join zu entfernen.\n\n\nMusterlösung\nbaeume_wiesen_df &lt;- st_drop_geometry(baeume_wiesen)\n\nbaeume_2 &lt;- left_join(baeume_sample, baeume_wiesen_df, by = \"baum_id\")\n\nggplot() +\n  geom_sf(data = wiesen) +\n  geom_sf(data = baeume_2, aes(colour = wiesen_anteil)) +\n  scale_color_binned(\"Wiesen Anteil\", low = \"blue\", high = \"red\", limits = c(0, 1), label = scales::label_percent()) +\n  coord_sf(datum = 2056)\n\n\n\n\n\n\n\n\nAbbildung 35.2: Nach dieser Übung kannst du das Resultat in dieser Weise visualisieren.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Rauman 2: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#sec-raster-intro1",
    "href": "rauman/Rauman2_Uebung_A.html#sec-raster-intro1",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nNun habt ihr ein paar Vektoroperationen wie st_buffer(), st_intersection() und st_area() durchgeführt. Gewisse Fragen lassen sich aber besser in der Raster-Welt beantworten. Wollen wir beispielsweise für jeden Punkt im Raum wissen, wie weit der nächstgelegene Baum ist, lässt sich das besser in einem Raster darstellen.\nBevor wir die Frage aber beantworten können, müssen wir den Vektordatensatz in ein Rasterdatensatz konvertieren. Dafür wiederum braucht es ein Raster “Template”, damit R in etwa weiss, wie der Raster-Output auszusehen hat.\n\n\n# Um mit Raster arbeiten zu können brauchen wir das Package \"terra\"\nlibrary(\"terra\")\n\n# Um ein Vektor Datensatz zu vektorieren, brauchen wir ein Template.\n# Für das Template nutzen wir \"wiesen\" und setzen eine Zellgrösse (resolution)\ntemplate &lt;- rast(wiesen, resolution = 20)\n\n# Mit rasterize können wir \"baeume\" in einen Raster konvertieren\n# Nutzt hier wieder alle bäume, nicht baeume_sample\nbaeume_rast &lt;- terra::rasterize(baeume, template)\n\nDer Unterschied zwischen Raster und Vektor kann sehr anschaulich dargestellt werden, wenn die beiden Datensätze übereinander gelagert werden.\n\nplot(baeume_rast, col = \"grey\")\nplot(baeume, add = TRUE, col = \"red\", pch = \"x\")\n\n\n\n\n\n\n\n\nMit baeume_rast können wir nun mit der Funktion distance() die Distanz zu jedem Baum berechnen:\n\n\nbaeume_dist &lt;- distance(baeume_rast)\nplot(baeume_dist)\nplot(baeume, add = TRUE, pch = \"x\")",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Rauman 2: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html",
    "href": "rauman/Rauman2_Uebung_B.html",
    "title": "Rauman 2: Übung B",
    "section": "",
    "text": "Aufgabe 1\nIn der letzten Übung habt ihr nun eine erste Erfahrung mit Raster Daten gemacht. Dabei haben wir zuerst einen Vektordatensatz rasterisiert. Häufig arbeiten wir aber mit Geodaten, die bereits im Rasterformat erfasst werden.\nIn dieser Übung werden wir weiter mit terra arbeiten, um zu zeigen, wie wir einen Rasterdatensatz importieren, visualisieren und weiter verarbeiten können. In euren Daten findet ihr einen Datensatz namens dhm250m.tif, der das “Digitale Höhenmodell” (DHM) des Kantons Schwyz darstellt. Führen Sie den angegebenen Code aus.\nlibrary(\"terra\")\nImportieren Sie Ihr Raster mit der Funktion rast\ndhm_schwyz &lt;- rast(\"datasets/rauman/dhm250m.tif\")\nSie erhalten einige wichtige Metadaten über den Rasterdatensatz, wenn Sie den Variablennamen in die Konsole eingeben.\ndhm_schwyz\n## class       : SpatRaster \n## dimensions  : 150, 186, 1  (nrow, ncol, nlyr)\n## resolution  : 250, 250  (x, y)\n## extent      : 2672175, 2718675, 1193658, 1231158  (xmin, xmax, ymin, ymax)\n## coord. ref. : CH1903+ / LV95 (EPSG:2056) \n## source      : dhm250m.tif \n## name        :   dhm250m \n## min value   :  389.1618 \n## max value   : 2850.0203\nUm einen schnellen Überblick eines Rasterdatensatz zu erhalten, können wir einfach die plot() Funktion verwenden.\nplot(dhm_schwyz)\nLeider ist das Verwenden von Rastern in ggplot nicht sehr einfach. Da ggplot ein universelles Plot-Framework ist, stossen wir schnell an die Grenzen des Möglichen, wenn wir etwas so Spezielles wie Karten erstellen. Mit plot können wir zwar sehr schnell plotten, aber auch hier stossen wir schnell an Grenzen.\nAus diesem Grund werden wir ein neues Plot-Framework einführen, das auf Karten spezialisiert ist und in einem sehr ähnlichen Design wie ggplot gebaut wurde: tmap. Laden Sie dieses Paket jetzt in Ihre Session:\nlibrary(\"tmap\")\nGenau wie ggplot basiert tmap auf der Idee von “Ebenen”, die durch ein + verbunden sind. Jede Ebene hat zwei Komponenten:\ntm_shape(dhm_schwyz) +\n  tm_raster()\nBeachten Sie, dass tm_shape() und tm_raster() (in diesem Fall) zusammengehören. Das eine kann nicht ohne das andere leben.\nWenn Sie die Hilfe von ?tm_raster konsultieren, werden Sie eine Vielzahl von Optionen sehen, mit denen Sie die Visualisierung Ihrer Daten verändern können. Zum Beispiel ist der Standardstil von tm_raster() die Erstellung von “Bins” mit einer diskreten Farbskala. Wir können dies mit col.scale = tm_scale_continuous() ausser Kraft setzen.\ntm_shape(dhm_schwyz) +\n  tm_raster(col.scale = tm_scale_continuous())\nDas sieht schon ziemlich toll aus, aber vielleicht wollen wir die Standard-Farbpalette ändern. Ein nettes GUI, die einem die Auswahl der Farben erleitern soll, erhält ihr mit dem Befehl cols4all::c4a_gui().\ntm_shape(dhm_schwyz) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.spectral\"))\nEine grosse Stärke von tmap ist die Tatsache, dass mit dem gleichen Befehl sowohl statische wie auch interative Plots erstellt werden können. Dafür muss der Modus von statisch auf interaktiv gewechselt werden.\ntmap_mode(\"view\") # wechselt auf interakive Plots\n\ntm_shape(dhm_schwyz) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.spectral\"))\n\n\n\n\n\n\ntmap_mode(\"plot\") # wechselt zurück auf statische Plots",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Rauman 2: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html#aufgabe-1",
    "href": "rauman/Rauman2_Uebung_B.html#aufgabe-1",
    "title": "Rauman 2: Übung B",
    "section": "",
    "text": "eine Datensatzkomponente, die immer tm_shape(dataset) ist (ersetzen Sie dataset durch Ihre Variable)\neine Geometriekomponente, die beschreibt, wie das vorangegangene tm_shape() visualisiert werden soll. Dies kann tm_dots() für Punkte, tm_polygons() für Polygone, tm_lines() für Linien usw. sein. Für Einzelbandraster (was bei dhm_schwyz der Fall ist) ist es tm_raster()",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Rauman 2: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html#sec-raster-slope",
    "href": "rauman/Rauman2_Uebung_B.html#sec-raster-slope",
    "title": "Rauman 2: Übung B",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nMit terra können wir eine Vielzahl von Rasteroperationen über unser Höhenmodell laufen lassen. Eine klassische Rasteroperation ist zum Beispiel das Berechnen der Hangneigung (“slope”) oder dessen Orientierung (“aspect”). Nutzen Sie die Funktion terrain() aus terra, um die Hangneigung und Orientierung zu berechnen. Visualisieren Sie die Resultate.\n\n\nMusterlösung\nterrain(dhm_schwyz, \"slope\") |&gt;\n  plot()\n\n\n\n\n\nHangneigung des Kantons Schwyz (v = \"slope\")\n\n\n\n\n\n\nMusterlösung\nschwyz_aspect &lt;- terrain(dhm_schwyz, \"aspect\")\n\ntm_shape(schwyz_aspect) +\n  tm_raster(\n    col.scale = tm_scale_continuous(\n      values = c(\"#EF476F\", \"#FFD166\", \"#06D6A0\", \"#118AB2\", \"#EF476F\"),\n      ticks = seq(0, 360, 90),values.repeat = TRUE\n      )\n    )\n\n\n\n\n\nHangausrichtung des Kantons Schwyz (v = \"aspect\"). Hier wurde eine eigene Farbpalette erstellt, da sich die Extremwerte 0 und 360 (Norden) sich farblich nicht unterscheiden sollten.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Rauman 2: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html#aufgabe-3",
    "href": "rauman/Rauman2_Uebung_B.html#aufgabe-3",
    "title": "Rauman 2: Übung B",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nMit Hangneigung und -ausrichtung können wir einen Hillshading-Effekt berechnen. Hillshading bedeutet, dass der Schattenwurf des Oberflächenmodells bei gegebenen Einfallswinkel der Sonne (Höhe und Azimut) berechnet wird. Der typische Einfallswinkel liegt bei 45° über dem Horizont und von Nordwesten bei 315°.\nUm einen Hillshading Effekt zu erzeugen, berechne zuerst slope und aspect von dhm_schwyz analog der letzten Aufgabe, achte aber darauf, dass die Einheit radians entspricht. Nutze diese beiden Objekte in der Funktion shade(), um den Hillshade zu berechnen. Visualisiere den Output anschliessend mit plot oder tmap.\n\n\nMusterlösung\ndhm_slope &lt;- terrain(dhm_schwyz, \"slope\", unit = \"radians\")\ndhm_aspect &lt;- terrain(dhm_schwyz, \"aspect\", unit = \"radians\")\n\ndhm_hillshade &lt;- shade(dhm_slope, dhm_aspect, 45, 315)\n\ntm_shape(dhm_hillshade) +\n  tm_raster(\n    col.scale = tm_scale_continuous(values = \"matplotlib.cividis\")\n  )\n\n\n\n\n\nFür diese Visualisierung verwende ich tmap und als die Farbpalette values = \"matplotlib.cividis\".",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Rauman 2: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html",
    "href": "rauman/Rauman3_Uebung_A.html",
    "title": "Rauman 3: Übung A",
    "section": "",
    "text": "Aufgabe 1: Rotmilan Bewegungsdaten visualisieren\nFür die heutige Übung benötigen wir folgende Datensätze. Importiere diese in R. Prüfe, ob das CRS korrekt gesetzt wurde und setze es wenn nötig. Mache dich mit den Daten vertraut (visualieren, durchscrollen usw).\nAls erstes wollen wir für die Datensätze luftqualitaet und rotmilan Dichteschätzungen durchführen. Lade vorgängig die dafür notwendigen Packages in deine R-Session.\nDie erste Frage, die bei solchen Bewegungsstudien typischerweise gestellt wird, lautet: Wo hält sich das Tier hauptsächlich auf? Um diese Frage zu beantworten, kann man als erstes einfach die Datenpunkte in einer einfachen Karte visualisieren. Erstellt zur Beantwortung dieser Frage nachstehende Karte.\nMusterlösung\nggplot(schweiz) +\n  geom_sf() +\n  geom_sf(data = rotmilan) +\n  theme_void()",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Rauman 3: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-2-kernel-density-estimation-berechnen",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-2-kernel-density-estimation-berechnen",
    "title": "Rauman 3: Übung A",
    "section": "Aufgabe 2: Kernel Density Estimation berechnen",
    "text": "Aufgabe 2: Kernel Density Estimation berechnen\nIn einer ersten Annäherung funktioniert dies, doch wir sehen hier ein klassisches Problem des “Overplotting”. Das heisst, dass wir durch die Überlagerung vieler Punkte in den dichten Regionen nicht abschätzen können, wie viele Punkte dort effektiv liegen und ggf. übereinander liegen. Es gibt hier verschiedene Möglichkeiten, die Punktdichte klarer zu visualisieren. Eine unter Biologen sehr beliebte Methode ist die Dichteverteilung mit einer Kernel Density Estimation (KDE). Dies v.a. darum, weil mit KDE das Habitat (Streifgebiet) eines Tieres abgeschätzt werden kann. Homeranges werden oft mit KDE95 und Core Areas mit KDE50 definiert (Fleming C., Calabrese J., 2016).\nFür die Berechnung der Density verwenden wir die Funktion density.ppp aus spatstat. Diese library ist etwas komplex in der Anwendung. Damit wir dieses Verfahren aber dennoch auf unsere Rotmilan-Daten anwenden können, haben wir eine eigene KDE-Funktion erstellt, die wir Euch zur Verfügung stellen.\nWir ermutigen alle, die dafür Kapazität haben, unsere Funktion eingehend zu studieren und allenfalls ganz auf die Funktion zu verzichten und stattdessen direkt spatstat zu verwenden. Wenn ihr mit unserer Funktion arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\nmy_kde &lt;- function(points, cellsize, bandwidth, extent = NULL){\n  library(\"spatstat.geom\")    # um sf in ppp zu konvertieren\n  library(\"spatstat.explore\") # um die Dichte zu berechnen\n  \n  points_ppp &lt;- as.ppp(points) # konvertiert sf &gt; ppp\n\n  if(!is.null(extent)){\n    # falls ein extent angegeben ist, wird dieser verwendet\n    # um das \"Beobachtungsfenster\" zu setzen\n    Window(points_ppp) &lt;- as.owin(st_bbox(extent))\n  } \n\n  # macht eine Dichteschätzung\n  points_density &lt;- density.ppp(x = points_ppp, sigma = bandwidth, eps = cellsize)\n\n  # konvertiert den Output in ein DataFrame\n  points_density_df &lt;- as.data.frame(points_density)\n\n  points_density_df\n}\n\nDie Parameter der Funktion sollten relativ klar sein:\n\npoints: Ein Punktdatensatz aus der Class sf\ncellsize: Die Zellgrösse des Output-Rasters\nbandwidth: Der Suchradius für die Dichteberechnung\nextent (optional): Der Perimeter, in dem die Dichteverteilung berechnet werden soll. Wenn kein Perimeter angegeben wird, wird die “bounding box” von points genutzt.\n\nWenn wir nun mit my_kde() die Dichteverteilung berechnen, erhalten wir ein data.frame mit X und Y Koordinaten sowie eine Spalte value zurück. Nutzt diese drei Spalten mit geom_raster(), um eure Daten mit ggplot zu visualisieren (aes(x = X, y = Y, fill = value).\n\nrotmilan_kde &lt;- my_kde(points = rotmilan, cellsize = 1000, bandwidth = 10000, extent = schweiz)\n\nhead(rotmilan_kde)\n##         x       y        value\n## 1 2485909 1075767 5.632740e-24\n## 2 2485909 1076766 8.290197e-23\n## 3 2485909 1077764 3.032138e-23\n## 4 2485909 1078763 6.522508e-23\n## 5 2485909 1079761 9.599267e-23\n## 6 2485909 1080760 1.183168e-22\n\n\n\nMusterlösung\nggplot() +\n  geom_raster(data = rotmilan_kde, aes(x, y, fill = value)) +\n  geom_sf(data = schweiz, fill = NA) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\n\n\n\n\n\nDie Kernel Density Estimation ist nun sehr stark von den tiefen Werten dominiert, da die Dichte in den meisten Zellen unseres Untersuchungsgebiets nahe bei Null liegt. Wie erwähnt sind Wissenschaftler häufig nur an den höchsten 95% der Werte interessiert. Folge folgenden Schritten, um das Resultat etwas besser zu veranschaulichen:\n\nBerechne die 95. Perzentile aller Werte mit der Funktion quantile und bennene diesen q95\nErstelle eine neue Spalte in rotmilan_kde, wo alle Werte tiefer als q95 NA entsprechen\n(Optional): Transformiere die Werte mit log10, um einen differenzierteren Farbverlauf zu erhalten\n\nWir können die tiefen Werte ausblenden, indem wir nur die höchsten 5% der Werte darstellen. Dafür berechnen wir mit raster::quantile die 95. Perzentile aller Werte und nutzen diesen Wert als “Grenzwert” für die Darstellung.\nZusätzlich hilft eine logarithmische Transformation der Werte, die Farbskala etwas sichtbarer zu machen.\n\n\nMusterlösung\nq95 &lt;- quantile(rotmilan_kde$value, probs = 0.95)\n\nrotmilan_kde &lt;- rotmilan_kde |&gt;\n  mutate(\n    value_new = ifelse(value &gt; q95, value, NA),\n    value_new = log10(value_new)\n  )\n\nggplot() +\n  geom_raster(data = rotmilan_kde, aes(x, y, fill = value_new)) +\n  geom_sf(data = schweiz, inherit.aes = FALSE, fill = NA) +\n  scale_fill_viridis_c(na.value = NA) +\n  theme_void()",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Rauman 3: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "title": "Rauman 3: Übung A",
    "section": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen",
    "text": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen\nThiessen Polygone bieten eine spannende Alternative, um Unterschiede in der Dichteverteilung von Punktdatensätzen zu visualisieren. Wir wollen dies nun ausprobieren und konstruieren zum Schluss die Thiessenpolygone für die Rotmilan-Daten für das Untersuchungsgebiet Schweiz. Nutze die Anleitung für das Erstellen von Thiessenpolygonen aus der Übung B, um Thiessenpolygone für die Rotmilanpositionen zu erstellen.\n\n\nMusterlösung\nthiessenpolygone &lt;- rotmilan |&gt;\n  st_union() |&gt;\n  st_voronoi()\nschweiz &lt;- st_union(schweiz)\n\nthiessenpolygone &lt;- st_cast(thiessenpolygone)\n\nthiessenpolygone_clip &lt;- st_intersection(thiessenpolygone, schweiz)\n\n\n\n\nMusterlösung\nggplot() +\n  geom_sf(data = schweiz) +\n  geom_sf(data = thiessenpolygone_clip, fill = NA) +\n  theme_void()\n\n\n\n\n\n\n\n\nAbbildung 37.1: Wenn wir jetzt die Thiessenpolygone (ohne Punkte) darstellen, wird deutlicher, wie die Dichteverteilung im Innern des Clusters aussieht.\n\n\n\n\n\n\n\n\n\nScherler, Patrick. 2020. „Drivers of Departure and Prospecting in Dispersing Juvenile Red Kites (Milvus milvus).“ Phdthesis, University of Zurich.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Rauman 3: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman3_Uebung_B.html",
    "href": "rauman/Rauman3_Uebung_B.html",
    "title": "Rauman 3: Übung B",
    "section": "",
    "text": "In dieser Übung geht es darum, zwei verschiedene Interpolationsverfahren in R umzusetzen. Im ersten Interpolationsverfahren verwenden wir die inverse distance weighted interpolation, später verwenden wir die nearest neighbour Methode. Dazu braucht ihr die folgenden Packages:\n\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\n\nWeiter benötigt ihr die nachstehenden Datensätze:\nluftqualitaet &lt;- read_sf(\"datasets/rauman/luftqualitaet.gpkg\")\nschweiz &lt;- read_sf(\"datasets/rauman/schweiz.gpkg\")\nDie Library gstat bietet verschiedene Möglichkeiten, Datenpunkte zu interpolieren, unter anderem auch die inverse distance weighted Methode. Leider ist das Package noch nicht so benutzerfreundlich wie sf: Das Package wird aber aktuell überarbeitet und in mittlerer Zukunft sollte es ebenso einfach zugänglich sein. Damit Ihr Euch nicht mit den Eigenheiten dieser Library herumschlagen müsst, haben wir eine Function vorbereitet, die Euch die Verwendung der IDW-Interpolation erleichtern soll.\nWir nehmen Euch damit etwas Komplexität weg und liefern Euch ein pfannenfertiges Werkzeug. Das hat auch Nachteile und wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Function zu verzichten und stattdessen direkt gstat zu verwenden. Wenn ihr mit unserer Function arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\nmy_idw &lt;- function(groundtruth,column,cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){\n  library(\"gstat\")\n  library(\"sf\")\n  \n  if(is.null(extent)){\n    extent &lt;- groundtruth\n  }\n  \n  samples &lt;- st_make_grid(extent,cellsize,what = \"centers\")\n  my_formula &lt;- formula(paste(column,\"~1\"))\n  idw_sf &lt;- gstat::idw(formula = my_formula,groundtruth, newdata = samples, nmin = 1, nmax = nmax, maxdist = maxdist, idp = idp) \n  \n  idw_matrix &lt;- cbind(as.data.frame(st_coordinates(idw_sf)),pred = st_drop_geometry(idw_sf)[,1])\n  idw_matrix\n}\n\nNun könnt Ihr mit my_idw() den Datensatz luftqualitaet folgendermassen interpolieren.\n\nmy_idw(groundtruth = luftqualitaet, column = \"value\", cellsize = 10000, extent = schweiz)\n\nFolgende Parameter stehen Euch zur Verfügung:\n\nNotwendige Parameter:\n\ngroundtruth: Punktdatensatz mit den Messwerten (sf-Objekt)\ncolumn: Name der Spalte mit den Messwerten (in Anführungs- und Schlusszeichen)\ncellsize: Zellgrösse des Output-Rasters\n\nOptionale Parameter\n\nnmax: Maximale Anzahl Punkte, die für die Interpolation berücksichtigt werden sollen. Default: Inf (alle Werte im gegebenen Suchradius)\nmaxdist: Suchradius, welcher für die Interpolation verwendet werden soll. Default Inf (alle Werte bis nmax)\nidp: Inverse Distance Power: die Potenz, mit der der Nenner gesteigert werden soll. Default: 2. Werte werden im Kehrwert des Quadrates gewichtet: \\(\\frac{1}{dist^{idp}}\\).\nextent: Gebiet, für welches die Interpolation durchgeführt werden soll. Wenn nichts angegeben wird (Default NULL), wird die Ausdehnung von groundtruth verwendet.\n\nOuput\n\nder Output der Funktion ist eine data.frame mit 3 Spalten:\n\nX, Y Koordinaten der interpolierten Werte\npred: der interpolierte Wert\n\n\n\nBeim Output handelt sich hier um einen Raster-ähnlichen Datentyp (siehe Vorlesung Spatial DataScience 1). Diesen können wir mit geom_raster mit ggplot visualisieren. Dafür müsst ihr in aes die X und Y Koordinaten angeben und der interpolierte Wert mit fill einfärben.\n\nAufgabe 1: Raeumliche Interpolation mit IDW\nRechnet so den IDW für die Luftqualitätsmessungen mit verschiedenen Parametern und visualisiert jeweils die Resultate. Experimentiert mit nmax sowie maxdist. Was stellt ihr fest?\nTips:\n\nWählt am Anfang eine etwas konvervative (grosse) cellsize und verringert diesen nur, wenn euer Rechner damit gut klar kommt\nDa der Output aus der Interpolation im gleichen Koordinatenbezugssystem ist wie schweiz.gpkg, kann man diese beiden Datensätze im gleichen ggplot darstellen. Dafür müsst ihr die aesthetics (aes()) für jeden Layer einzeln setzen und nicht auf der Ebene von ggplot().\n\n\n\n\nMusterlösung\n\n\nidw_1 &lt;- my_idw(groundtruth = luftqualitaet,column = \"value\",cellsize = 1000, nmax = Inf,maxdist = Inf,idp = 1,extent = schweiz) \n## [inverse distance weighted interpolation]\n\ntitle &lt;- paste(c(\"Inverse Distance Power (IDP): 1\", \"maxdist: Inf\", \"nmax: Inf\"), collapse = \"\\n\")\n\nggplot() +\n    geom_raster(data = idw_1, aes(X,Y, fill = pred)) +\n    geom_sf(data = schweiz, fill = NA) +\n    geom_sf(data = luftqualitaet, size = 1, shape = 3, alpha = 0.3) +\n    scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,\"RdYlBu\")),limits = c(0, 60), na.value = NA) +\n    labs(fill = \"μg/m3\", title = title) +\n    theme_void() +\n    theme(legend.position = \"bottom\", legend.title = element_blank(),\n      legend.key.width = unit(0.10, 'npc'),\n      legend.key.height = unit(0.02, 'npc'))\n\n\n\n\n\n\n\n\n\nMusterlösung\n\n\nidw_2 &lt;- my_idw(groundtruth = luftqualitaet,column = \"value\",cellsize = 1000, nmax = 10, maxdist = Inf,idp = 2,extent = schweiz) \n## [inverse distance weighted interpolation]\n\ntitle &lt;- paste(c(\"Inverse Distance Power (IDP): 2\", \"maxdist: Inf\", \"nmax: 10\"), collapse = \"\\n\")\n\nggplot() +\n    geom_raster(data = idw_2, aes(X,Y, fill = pred)) +\n    geom_sf(data = schweiz, fill = NA) +\n    geom_sf(data = luftqualitaet, size = 1, shape = 3, alpha = 0.3) +\n    scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,\"RdYlBu\")),limits = c(0, 60), na.value = NA) +\n    labs(fill = \"μg/m3\", title = title) +\n    theme_void() +\n    theme(legend.position = \"bottom\", legend.title = element_blank(),\n      legend.key.width = unit(0.10, 'npc'),\n      legend.key.height = unit(0.02, 'npc'))\n\n\n\n\n\n\n\n\n\nMusterlösung\n\nidw_3 &lt;- my_idw(groundtruth = luftqualitaet,column = \"value\",cellsize = 1000, nmax = 10, maxdist = 30000,idp = 3,extent = schweiz) \n## [inverse distance weighted interpolation]\n\ntitle &lt;- paste(c(\"Inverse Distance Power (IDP): 3\", \"maxdist: 30km\", \"nmax: 10\"), collapse = \"\\n\")\n\nggplot() +\n    geom_raster(data = idw_3, aes(X,Y, fill = pred)) +\n    geom_sf(data = schweiz, fill = NA) +\n    geom_sf(data = luftqualitaet, size = 1, shape = 3, alpha = 0.3) +\n    scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,\"RdYlBu\")),limits = c(0, 60), na.value = NA) +\n    labs(fill = \"μg/m3\", title = title) +\n    theme_void() +\n    theme(legend.position = \"bottom\", legend.title = element_blank(),\n      legend.key.width = unit(0.10, 'npc'),\n      legend.key.height = unit(0.02, 'npc'))\n\n\n\n\n\n\n\n\n\n\n\n\nAufgabe 2: Interpolation mit Nearest Neighbour\nEine weitere einfache Möglichkeit zur Interpolation bietet die Erstellung eines Voronoi-Diagrammes, auch als Thiessen-Polygone oder Dirichlet-Zerlegung bekannt. sf liefert dazu die Funktion st_voronoi(), die einen Punktdatensatz annimmt und eben um die Punkte die Thiessenpolygone konstruiert. Dazu braucht es lediglich einen kleinen Vorverarbeitungsschritt: sf möchte für jedes Feature, also für jede Zeile in unserem Datensatz, ein Voronoidiagramm. Das macht bei uns wenig Sinn, weil jede Zeile nur aus einem Punkt besteht. Deshalb müssen wir vorher luftqualitaet mit st_union() von einem POINT- in ein MULTIPOINT-Objekt konvertieren, in welchem alle Punkte in einer Zeile zusammengefasst sind.\n\n\nMusterlösung\nluftqualitaet_union &lt;- st_union(luftqualitaet)\n\nthiessenpolygone &lt;- st_voronoi(luftqualitaet_union)\n\n\n\n\nMusterlösung\nggplot() +\n  geom_sf(data = schweiz) +\n  geom_sf(data = thiessenpolygone, fill = NA)\n\n\n\n\n\n\n\n\n\nst_voronoi hat die Thiessenpolygone etwas weiter gezogen als wir sie wollen. Dies ist allerdings eine schöne Illustration der Randeffekte von Thiessenpolygonen, die zum Rand hin (wo es immer weniger Punkte hat) sehr gross werden können. Wir können die Polygone auf die Ausdehnung der Schweiz mit st_intersection() clippen. Vorher konvertieren wir mit st_collection_extract() die GEOMETRYCOLLECTION in POLYGON.\n\n\nMusterlösung\nthiessenpolygone &lt;- st_collection_extract(thiessenpolygone)\n\nthiessenpolygone_clip &lt;- st_intersection(thiessenpolygone, schweiz)\n\n\n\n\nMusterlösung\nggplot() +\n  geom_sf(data = schweiz) +\n  geom_sf(data = thiessenpolygone_clip, fill = NA)\n\n\n\n\n\n\n\n\n\nJetzt müssen wir nur noch den jeweiligen Wert für jedes Polygon ermitteln. Dies erreichen wir wieder durch st_join. Auch hier ist noch ein kleiner Vorverarbeitungsschritt nötig: Wir konvertieren das sfc-Objekt (nur Geometrien) in ein sf-Objekt (Geometrien mit Attributtabelle).\n\n\nMusterlösung\nthiessenpolygone_clip &lt;- st_as_sf(thiessenpolygone_clip)\nthiessenpolygone_clip &lt;- st_join(thiessenpolygone_clip, luftqualitaet)\n\nggplot() +\n  geom_sf(data = schweiz) +\n  geom_sf(data = thiessenpolygone_clip, aes(fill = value)) +\n  geom_sf(data = luftqualitaet) +\n  scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11, \"RdYlBu\"))) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\", legend.title = element_blank(),\n    legend.key.width = unit(0.10, \"npc\"),\n    legend.key.height = unit(0.02, \"npc\")\n  )\n\n\n\n\n\n\n\n\nAbbildung 38.1: Stickstoffdioxid (NO2) in μg/m3, Interpoliert über die ganze Schweiz nach der Nearest Neighbour Methode.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Rauman 3: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman3_Uebung_C.html",
    "href": "rauman/Rauman3_Uebung_C.html",
    "title": "Rauman 3: Übung C (Optional)",
    "section": "",
    "text": "Aufgabe 1\nIn dieser optionalen Übung wollen wir die G-Function für Luftqualitäts-Messstellen und Rotmilan Bewegungen berechnen und vergleichen.\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\n\nrotmilan &lt;- read_sf(\"datasets/rauman/rotmilan.gpkg\")\n\nschweiz &lt;- read_sf(\"datasets/rauman/schweiz.gpkg\")\n\nluftqualitaet &lt;- read_sf(\"datasets/rauman/luftqualitaet.gpkg\")\nggplot(rotmilan) +\n  geom_sf(data = schweiz) +\n  geom_sf(aes(colour = timestamp), alpha = 0.2) +\n  scale_color_datetime(low = \"blue\", high = \"red\")\n\n\n\n\n\n\n\nAbbildung 39.1: Eine solche Visualisierung zeigt dir beispielsweise die räumliche Ausdehnung der Datenpunkte",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Rauman 3: Übung C (Optional)</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman3_Uebung_C.html#aufgabe-2",
    "href": "rauman/Rauman3_Uebung_C.html#aufgabe-2",
    "title": "Rauman 3: Übung C (Optional)",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nAls erstes berechnen wir die G-Function für die Rotmilanpositionen:\n\nSchritt 1\nMit st_distance() können Distanzen zwischen zwei sf-Datensätze berechnet werden. Wird nur ein Datensatz angegeben, wird eine Kreuzmatrix erstellt, wo die Distanzen zwischen allen Features zu allen anderen Features dargestellt werden. Wir nützen diese Funktion zur Berechnung der nächsten Nachbarn.\n\n\nMusterlösung\nrotmilan_distanzmatrix &lt;- st_distance(rotmilan)\n\nnrow(rotmilan_distanzmatrix)\nncol(rotmilan_distanzmatrix)\n# zeige die ersten 6 Zeilen und Spalten der Matrix\n# jeder Wert ist 2x vorhanden (vergleiche Wert [2,1] mit [1,2])\n# die Diagonale ist die Distanz zu sich selber (gleich 0)\nrotmilan_distanzmatrix[1:6, 1:6]\n\n\n\n\nSchritt 2\nNun wollen wir wissen, wie gross die kürzeste Distanz von jedem Punkt zu seinem nächsten Nachbarn ist, also die kürzeste Distanz pro Zeile. Bevor wir diese ermitteln, müssen wir die diagonalen Werte noch entfernen, denn diese stellen ja jeweils die Distanz zu sich selber dar und sind immer 0. Danach kann mit apply() eine Funktion (FUN = min) über die Zeilen (MARGIN = 1) einer Matrix (X = rotmilan_distanzmatrix) gerechnet werden. Zusätzlich müssen wir noch na.rm = TRUE setzen, damit NA Werte von der Berechnung ausgeschlossen werden. Das Resultat ist ein Vektor mit gleich vielen Werten wie Zeilen in der Matrix.\n\n\nMusterlösung\ndiag(rotmilan_distanzmatrix) &lt;- NA # entfernt alle diagonalen Werte\n\nrotmilan_distanzmatrix[1:6, 1:6]\n\nrotmilan_mindist &lt;- apply(rotmilan_distanzmatrix, 1, min, na.rm = TRUE)\n\n\n\n\nSchritt 3\nNun müssen wir die Distanzen nach ihrer Grösse sortieren.\n\n\nMusterlösung\nrotmilan_mindist &lt;- sort(rotmilan_mindist)\n\n\n\n\nSchritt 4\nJetzt berechnen wir die kummulierte Häufigkeit von jeder Distanz. Die kummulierte Häufikgeit vom ersten Wert ist 1 (der Index des ersten Wertes) dividiert durch die Anzahl Werte insgesamt. Mit seq_along erhalten wir die Indizes aller Werte, mit length die Anzahl Werte insgesamt.\n\n\nMusterlösung\nkumm_haeufgikeit &lt;- seq_along(rotmilan_mindist) / length(rotmilan_mindist)\n\n\n\n\nSchritt 5\nNun wollen wir die kumulierte Häufigkeit der Werte in einer Verteilungsfunktion (engl: Empirical Cumulative Distribution Function, ECDF) darstellen. Dafür müssen wir die beiden Vektoren zuerst noch in einen Dataframe packen, damit ggplot damit klar kommt.\n\n\nMusterlösung\nrotmilan_mindist_df &lt;- data.frame(\n  distanzen = rotmilan_mindist,\n  kumm_haeufgikeit = kumm_haeufgikeit\n)\n\np &lt;- ggplot() +\n  geom_line(data = rotmilan_mindist_df, aes(distanzen, kumm_haeufgikeit)) +\n  labs(x = \"Distanz (Meter)\", y = \"Häufigkeit (kummuliert)\")\np\n\n\n\n\n\n\n\n\n\nLesehilfe:\n\n\nMusterlösung\nprob &lt;- 0.95\nres &lt;- quantile(ecdf(rotmilan_mindist_df$distanzen), prob)\nres2 &lt;- quantile(ecdf(rotmilan_mindist_df$distanzen), 0.99)\nxlim &lt;- c(5000, NA)\nylim &lt;- c(.5, .75)\np +\n  geom_segment(aes(x = res, xend = res, y = -Inf, yend = prob), colour = \"lightblue\") +\n  geom_segment(aes(x = -Inf, xend = res, y = prob, yend = prob), colour = \"lightblue\") +\n  geom_point(aes(x = res, y = prob), size = 3, colour = \"lightblue\") +\n  ggrepel::geom_label_repel(aes(x = 0, y = prob, label = paste0(prob * 100, \"% der Werte...\")),\n    xlim = xlim, ylim = ylim, hjust = 0, min.segment.length = 0, fill = \"lightblue\"\n  ) +\n  ggrepel::geom_label_repel(aes(x = res, y = 0, label = paste0(\"... sind kleiner als \", round(res, 0), \"m\")),\n    xlim = xlim, ylim = ylim, hjust = 0, vjust = 1, fill = \"lightblue\", min.segment.length = 0, inherit.aes = FALSE\n  ) +\n  scale_y_continuous(breaks = c(0, .25, .5, .75, prob, 1))",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Rauman 3: Übung C (Optional)</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman3_Uebung_C.html#aufgabe-3",
    "href": "rauman/Rauman3_Uebung_C.html#aufgabe-3",
    "title": "Rauman 3: Übung C (Optional)",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nFühre nun die gleichen Schritte mit luftqualitaet durch und vergleiche die ECDF-Plots.\n\n\nMusterlösung\nluftqualitaet_distanzmatrix &lt;- st_distance(luftqualitaet)\n\ndiag(luftqualitaet_distanzmatrix) &lt;- NA\n\nluftqualitaet_mindist &lt;- apply(luftqualitaet_distanzmatrix, 1, min, na.rm = TRUE)\n\nluftqualitaet_mindist &lt;- sort(luftqualitaet_mindist)\n\nkumm_haeufgikeit_luftquali &lt;- seq_along(luftqualitaet_mindist) / length(luftqualitaet_mindist)\n\nluftqualitaet_mindist_df &lt;- data.frame(\n  distanzen = luftqualitaet_mindist,\n  kumm_haeufgikeit = kumm_haeufgikeit_luftquali\n)\n\nluftqualitaet_mindist_df$data &lt;- \"Luftqualitaet\"\nrotmilan_mindist_df$data &lt;- \"Rotmilan\"\n\nmindist_df &lt;- rbind(luftqualitaet_mindist_df, rotmilan_mindist_df)\n\nggplot(mindist_df, ) +\n  geom_line(aes(distanzen, kumm_haeufgikeit, colour = data)) +\n  labs(x = \"Distanz (Meter)\", y = \"Häufigkeit (kummuliert)\", colour = \"Datensatz\")",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Rauman 3: Übung C (Optional)</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html",
    "href": "rauman/Rauman4_Uebung_A.html",
    "title": "Rauman 4: Übung",
    "section": "",
    "text": "Aufgabe 1: Morans \\(I\\) für Kantone\nHeute berechnen wir Morans \\(I\\), also ein globales Mass für Autokorrelation, für die Abstimmungsresultate der Zweitwohnungsinitiative. Dieser Wert beschreibt, ob Kantone, die nahe beieinanderliegen, ähnliche Abstimmungswerte haben. Hierfür verwenden wir den Datensatz zweitwohnungsinitiative.gpkg.\nDas Geopackage beinhaltet 3 Layers (siehe st_layers(zweitwohnung_kanton)). In jedem Layer sind die Abstimmungsresultate auf eine andere politische Ebene aggregiert. Wir starten mit der Aggregationsstufe “kanton”.\nFür die Berechnung von Morans \\(I\\) benutzen wir kein externes Package, sondern erarbeiten uns alles selber, basierend auf der Formel von Moran’s \\(I\\):\n\\[\n\\begin{aligned}\n\\text{Morans } I &= \\frac{\\color{cyan}n}{\\color{cyan}\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\color{red}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\color{cyan}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}} \\\\\n\\\\\n&= \\frac{\\color{cyan}\\text{zaehler1}}{\\color{cyan}\\text{nenner1}}\\times\\frac{\\color{red}\\text{zaehler2}}{\\color{cyan}\\text{nenner2}}\n\\end{aligned}\n\\tag{40.1}\\]\nRot markiert entpricht der Summe der gewichteten Ähnlichkeitsmatrix aus der Vorlesung. Alles blaue ist relativ trivial und dient lediglich der Normalisierung auf die Werte -1 bis +1. Die Begriffe zaehler1, nenner1 usw. sind die Variablen, die wir in R für die jeweiligen Berechnungen nutzen werden und dienen lediglich der Orientierung. Zudem gilt:",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Rauman 4: Übung</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html#aufgabe-1-morans-i-für-kantone",
    "href": "rauman/Rauman4_Uebung_A.html#aufgabe-1-morans-i-für-kantone",
    "title": "Rauman 4: Übung",
    "section": "",
    "text": "Gewichtete Ähnlichkeitsmatrix\nWidmen wir uns dem Kern von Morans \\(I\\), der Berechnung der gewichteten Ähnlichkeitsmatrix.\n\nNachbarschaftsmatrix \\(w_{ij}\\)\n\\[\\text{Morans } I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n {\\color{red}w_{ij}}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\n\\(w\\) beschreibt die räumlichen Gewichte der Kantone (den “Schalter” aus der Vorlesung). \\(w_{ij}\\) ist das Gewicht vom Kanton \\(i\\) im Vergleich zum Kanton \\(j\\). Sind Kantone \\(i\\) und \\(j\\) räumlich nah, gilt ein Gewicht von 1, sind sie weit entfernt, gilt ein Gewicht von 0. Dabei ist die Definition von “räumlich nah” nicht festgelegt. Denkbar wären verschiedene Optionen (siehe Vorlesung). Wir werden es mit die Bedigungen touches verwenden. Die Funktion st_touches prüft zwischen allen Kantonen, ob sie sich berühren. Mit der Option sparse = TRUE wird eine 26x26 Kreuzmatrix erstellt, wo jeder Kanton mit jedem anderen verglichen wird. Berühren sie sich, steht in der entsprechenden Stelle der Wert TRUE, was in R gleichbedeutend ist wie 1. Berühren sie sich nicht, steht der Wert FALSE, was gleichbedeutend ist wie 0.\n\n# st_touches berechnet eine Kreuzmatrix aller Objekte\nw_ij &lt;- st_touches(zweitwohnung_kanton, sparse = FALSE)\n\n# Schauen wir uns die Matrix mal an\n# (aus Platzmangen beschränken wir uns auf die ersten 5 Zeilen und Spalten\n# in RStudio könnt ihr mit View(w_ij) die gesamte Matrix anschauen)\nw_ij[1:5, 1:5]\n##       [,1]  [,2]  [,3]  [,4]  [,5]\n## [1,] FALSE FALSE FALSE FALSE  TRUE\n## [2,] FALSE FALSE  TRUE  TRUE FALSE\n## [3,] FALSE  TRUE FALSE FALSE  TRUE\n## [4,] FALSE  TRUE FALSE FALSE  TRUE\n## [5,]  TRUE FALSE  TRUE  TRUE FALSE\n\n\nDie erste Zeile entspricht dem ersten Kanton in zweitwohnung_kanton, die zweite Zeile dem zweiten Kanton usw. Das gleiche gilt für die Spalten. Um die Kreuzmatrix besser interpretieren zu können, können wir die Namen aus der Spalte KANTONSNAME verwenden, um die Zeilen und Spalten unserer Kreuzmatrix zu benennen.\n\nrownames(w_ij) &lt;- zweitwohnung_kanton$kuerzel\ncolnames(w_ij) &lt;- zweitwohnung_kanton$kuerzel\n\nw_ij[1:5, 1:5]\n##       ZH    BE    LU    UR    SZ\n## ZH FALSE FALSE FALSE FALSE  TRUE\n## BE FALSE FALSE  TRUE  TRUE FALSE\n## LU FALSE  TRUE FALSE FALSE  TRUE\n## UR FALSE  TRUE FALSE FALSE  TRUE\n## SZ  TRUE FALSE  TRUE  TRUE FALSE\n# Alterantiv: mit View(w_ij)\n\n\n\nAttributs-Ähnlichkeitsmatrix \\(c_{ij}\\)\n\\[\\text{Morans } I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}{\\color{red}(y_i - \\bar{y})(y_j - \\bar{y})}}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nUm die Attributs-Ähnlichkeit zwischen zwei Kantonen zu bestimmen, subtrahieren wir von jedem Kanton den Mittelwert aller Kantone und multiplizieren die beiden Differenzen. Die Funktion tcrossprod() erstellt diese Kreuzmatrix mit den multiplizierten Differenzen.\n\n# speichere die Variable in einem neuen Vektor\ny &lt;- zweitwohnung_kanton$ja_in_percent\n\ny_diff &lt;- y - mean(y) # erstellt ein Vector mit 26 Werten\nc_ij &lt;- tcrossprod(y_diff) # erstellt eine Matrix 26x26\n\n# Zeilen- und Spaltennamen hinzufügen\nrownames(c_ij) &lt;- zweitwohnung_kanton$kuerzel\ncolnames(c_ij) &lt;- zweitwohnung_kanton$kuerzel\n\nc_ij[1:5, 1:5]\n##            ZH        BE         LU        UR        SZ\n## ZH   9.094864  16.48350  -6.677609 -32.70370 -20.09778\n## BE  16.483499  29.87463 -12.102474 -59.27206 -36.42515\n## LU  -6.677609 -12.10247   4.902818  24.01163  14.75614\n## UR -32.703697 -59.27206  24.011629 117.59734  72.26846\n## SZ -20.097782 -36.42515  14.756145  72.26846  44.41197\n\n\n\nBerechnung von zaehler2\n\\[\\text{Morans } I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\color{red}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nDer gesamte Term zaehler2 ist die Summe aus der Multiplikation von w_ij und c_ij.\n\n# Matrix multiplikation\ncw_ij &lt;- w_ij * c_ij\n\n# Summe bilden\nzaehler2 &lt;- sum(cw_ij)\n\nzaehler2\n## [1] 1980.689\n\n\n\n\nNormalisieren\nUm das Resultat aus der bisherigen Berechung auf einen Wert von -1 bis +1 zu normalisieren, müssen wir noch folgende Terme berechnen:\n\\[\\text{Morans } I = \\frac{\\color{cyan}n}{\\color{cyan}\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\color{cyan}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}} \\]\n\nBerechnung von \\(n\\) (zaehler1)\nDer Term zaehler1 resp. n entspricht der Anzahl Objekte (hier: Kantone) in unserem Datensatz.\n\nzaehler1 &lt;- n &lt;- nrow(zweitwohnung_kanton)\n\nzaehler1\n## [1] 26\n\n\n\nAbweichung vom Mittelwert (nenner1)\nWir haben bereits in der Berechnung der Attributs-Ähnlichkeit die Differenz zum Mittelwert berechnet. Für nenner1 müssen wir diesen lediglich quadrieren und die Resultate summieren.\n\\[\\text{Morans } I = \\frac{n}{\\color{cyan}\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\n\n# Di bereits berechneten Abweichungen müssen wir quadrieren:\ny_diff2 &lt;- y_diff^2\n\n# Und danach die Summe bilden:\nnenner1 &lt;- sum(y_diff2)\n\n\n\nSumme der Gewichte (nenner2)\n\\[\\text{Morans } I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\color{cyan}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nIm Term nenner2 müssen wir lediglich die Gewichte w_ij summieren.\n\nnenner2 &lt;- sum(w_ij)\n\n\n\n\nAuflösung der Formel\nNun haben wir alle Bestandteile von Morans \\(I\\) berechnet und müssen diese nur noch zusammenrechnen.\n\nMI_kantone &lt;- zaehler1 / nenner1 * zaehler2 / nenner2\n\nMI_kantone\n## [1] 0.3126993\n\nDer Global Morans \\(I\\) für die Abstimmungsdaten beträgt auf Kantonsebene also 0.31. Wie interpretiert ihr dieses Resultate? Was erwartet ihr für eine Resultat auf Bezirksebene?",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Rauman 4: Übung</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html#aufgabe-2-morans-i-für-bezirke-berechnen",
    "href": "rauman/Rauman4_Uebung_A.html#aufgabe-2-morans-i-für-bezirke-berechnen",
    "title": "Rauman 4: Übung",
    "section": "Aufgabe 2: Morans I für Bezirke berechnen",
    "text": "Aufgabe 2: Morans I für Bezirke berechnen\nNun könnt ihr Morans \\(I\\) auf der Ebene der Bezirke und untersuchen, ob und wie sich Morans \\(I\\) verändert. Importiert dazu den Layer bezirk aus dem Datensatz zweitwohnungsinitiative.gpkg. Visualisiert in einem ersten Schritt die Abstimmungsresultate. Formuliert nun eine Erwartungshaltung: Ist Morans \\(I\\) auf der Ebene Bezirke tiefer oder Höher als auf der Ebene Kantone?\n\n\n\n\n\n\nFür Fortgeschrittene\n\n\n\nErstellt aus dem erarbeiten Workflow eine function um Morans I auf der Basis von einem sf Objekt sowie einer Spalte dessen zu berechnen.\n\n\nMusterlösung\nmorans_i &lt;- function(sf_object, col) {\n  library(\"sf\")\n  w_ij &lt;- st_touches(sf_object, sparse = FALSE)\n  y &lt;- sf_object[, col, drop = TRUE]\n  y_diff &lt;- y - mean(y)\n  c_ij &lt;- tcrossprod(y_diff)\n  cw_ij &lt;- w_ij * c_ij\n  zaehler2 &lt;- sum(cw_ij)\n  zaehler1 &lt;- n &lt;- nrow(sf_object)\n  y_diff2 &lt;- y_diff^2\n  nenner1 &lt;- sum(y_diff2)\n  nenner2 &lt;- sum(w_ij)\n  morans_i_result &lt;- zaehler1 / nenner1 * zaehler2 / nenner2\n  return(morans_i_result)\n}\n\n# Kommentar\n# Wir können hier nicht das $ Zeichen verwenden, weil \"col\" ein String ist.\n# Mit der doppelten, eckigen klammer stellen wir sicher, dass y erstens ein\n# Vektor ist (schau dir \"y\" an wenn du nur eine Klammer verwendest)\n\n\n\n\n\n\nMusterlösung\nzweitwohnung_bezirke &lt;- read_sf(\"datasets/rauman/zweitwohnungsinitiative.gpkg\", \"bezirk\")\nMI_bezirke &lt;- morans_i(zweitwohnung_bezirke, \"ja_in_percent\")\n\n\n\nMusterlösung\np2 &lt;- ggplot(zweitwohnung_bezirke) +\n  geom_sf(aes(fill = ja_in_percent), colour = \"white\", lwd = 0.2) +\n  scale_fill_gradientn(\"Ja Anteil\", colours = RColorBrewer::brewer.pal(11, \"RdYlGn\"), limits = c(0, 100)) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = paste(\"Zweitwohnungsinitiative (2012), Ja-Stimmen-Anteil auf Ebene Bezirk\"),\n    subtitle = paste(\"Global Morans I: \", \"??\")\n  )\n\np1 &lt;- p + labs(\n  title = paste(\"Zweitwohnungsinitiative (2012), Ja-Stimmen-Anteil auf Ebene Kanton\"),\n  subtitle = paste(\"Global Morans I: \", round(MI_kantone, 2))\n) +\n  theme(legend.position = \"none\")\n\np1\n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\nlibrary(\"cowplot\")\n\n(p +\n  theme(legend.key.width = unit(2, \"cm\"), legend.title = element_blank())\n) |&gt;\n  get_legend() |&gt;\n  ggdraw()",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Rauman 4: Übung</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html",
    "href": "rauman/Rauman5_Uebung_A.html",
    "title": "Rauman 5: Übung A",
    "section": "",
    "text": "Aufgabe 1: Vektor Daten laden und anzeigen\nIn der folgenden Übung möchten wir potentielle Standorte für Windkraftanlagen im Kanton Schwyz ermitteln. Zu diesem Zweck führen wir eine Multikriterien-Evaluation durch. Für diese Analyse stehen uns folgende Datensätze zur Verfügung:\nIn diesem ersten Teil (Übung A) widmen wir uns, basierend auf Gilgen und Sartoris\" (\"2010\") und Tegou, Polatidis, und Haralambopoulos (2010), folgenden Parametern:\nDiese Eignungsbewertung setzen wir in den folgenden Aufgaben in R um. Starte R und lade die nötigen Libraries in Eure Session (s.u.)\nBetrachte das File windkraft_geodata.gpkg (siehe Tabelle 41.1). Dieses beinhaltet alle Vektordaten, die für die Bearbeitung der Multikriterien-Evaluation benötigt wird (Bewohnte Flächen, Nationale Schutzgebiete, Seeflächen, Strassen, Waldgebiete sowie die Kantonsgrenze von Schwyz). Die Namen der verfügbaren Listen können Sie mit sf::st_layers() ermitteln.\nImportiere die benötigten Vektordatensätze und exploriere die Daten. Zur Visualisierung könnt ihr die Funktionen plot oder die Packages tmap oder ggplot2 verwenden.\nSchau dir auch das Koordinatensystem an. Was fällt dir auf? Wir würden gerne mit dem neuen Schweizer Koordinatensystem arbeiten (LV95). Um ein Koordinatensystem umzuwandeln, benutze die Funktion st_transform().\nMusterlösung\ngpkg_path &lt;- \"datasets/rauman/windkraft_geodata.gpkg\"\n\n# Vector data\nst_layers(gpkg_path)\n## Driver: GPKG \n## Available layers:\n##                   layer_name     geometry_type features fields      crs_name\n## 1           Bewohnte_Flaeche     Multi Polygon      326      1 CH1903 / LV03\n## 2    Nationale_Schutzgebiete     Multi Polygon        1      1 CH1903 / LV03\n## 3                Seeflaechen     Multi Polygon      205      5 CH1903 / LV03\n## 4                   Strassen Multi Line String    28682     13 CH1903 / LV03\n## 5 Untersuchungsgebiet_Schwyz     Multi Polygon        1      1 CH1903 / LV03\n## 6                Waldgebiete     Multi Polygon     5580      3 CH1903 / LV03\n\nkt_schwyz &lt;- read_sf(gpkg_path, \"Untersuchungsgebiet_Schwyz\") |&gt; st_transform(2056)\nschutzgebiete &lt;- read_sf(gpkg_path, \"Nationale_Schutzgebiete\") |&gt; st_transform(2056)\nstrassen &lt;- read_sf(gpkg_path, \"Strassen\") |&gt; st_transform(2056)",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Rauman 5: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-raster-distance",
    "href": "rauman/Rauman5_Uebung_A.html#sec-raster-distance",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 2: Erschliessung berechnen",
    "text": "Aufgabe 2: Erschliessung berechnen\nBeginnen wir mit dem Kriterium “Erschliessung”. Wir müssen für den ganzen Kanton Schwyz wissen, wie weit die nächste Strasse entfernt ist. Wie wir bereits in Aufgabe 5 erläutert haben, lässt sich diese Information am besten in einem Raster abbilden.\nAnalog Aufgabe 5 müssen wir hierfür den Vektordatensatz auf der Basis eines Templates in ein Raster konvertieren. Für die Erstellung des Templates verwenden wir an dieser Stelle den Datensatz wind250m.tif.\n\ntemplate &lt;- rast(\"datasets/rauman/wind250m.tif\")\n\nstrassen_raster &lt;- rasterize(strassen, template)\nstrassen_dist &lt;- distance(strassen_raster)\n\nNutze der obige Code, um den Strassen-Datensatz zu rasterisieren und die Distanz zur Strassen mit der Funktion distance() zu berechnen. Plausibilisiere den Output, indem du ihn visualisierst.\n\nMusterlösung\nplot(strassen_raster, col = \"green\")\nplot(strassen_dist)\n\n\n\n\n\n\n\nDie rasterisierte Form des “Strassen” Datensatzes\n\n\n\n\n\n\n\nDie Distanz zur nächstgelegenen Strasse für jeden Punkt im Kanton Schwyz”\n\n\n\n\n\nFühre nun die gleiche Operation durch, um die Entfernung zu nationalen Schutzgebieten zu ermitteln.\n\nMusterlösung\nschutzgebiete_raster &lt;- rasterize(schutzgebiete, template)\nschutzgebiete_dist &lt;- distance(schutzgebiete_raster)\n\nplot(schutzgebiete_raster, col = \"green\")\nplot(schutzgebiete_dist)\n\n\n\n\n\n\n\nDie rasterisierte Form des “Schutzgebiete” Datensatzes\n\n\n\n\n\n\n\nDie Distanz zum nächstgelegenen Schutzgebiet für jeden Punkt im Kanton Schwyz”",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Rauman 5: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-classify1",
    "href": "rauman/Rauman5_Uebung_A.html#sec-classify1",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 3: Distanzkriterien bewerten",
    "text": "Aufgabe 3: Distanzkriterien bewerten\nJetzt haben wir die Distanzen zu den relevanten Gebieten berechnet. Nun müssen wir diese Distanzen bewerten. Dafür teilen wir die kontinuierlichen Distanzwerte in diskrete Kategorien ein. Wir verwenden für die Aufgabe folgende Einteilung aus der Tabelle 41.2.\nUm die Klassifizierung durchzuführen, brauchen wir die Function classify(). Wie wir aus der Dokumentation der Funktion (mit ?classify) entnehmen können, gibt es verschiedene Wege, wie wir einen Raster reklassifizieren können (siehe Beschreibung für das Argument rcl). Eine sehr explizite Variante ist, dass wir für rcl eine Matrix mit 3 Spalten verwenden. Diese drei Spalten stellen from, to und becomes dar.\nWir könnten diese Tabelle in einem Spreadsheet-Programm schreiben und in R einlesen. Alternativ können wir sie auch “von Hand” in R erstellen. Um in R tabellarische Daten zu schreiben, empfehlen wir die Funktion tribble(), welche eine sehr anschauliche Art bietet, Tabellen in R zu generieren. Da classify eine Matrix als Input will, müssen wir unser data.frame noch in eine Matrix umwandeln.\n\n\nMusterlösung\nstrassen_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n  0, 250, 1.0,\n  250, 500, 0.8,\n  500, 750, 0.6,\n  750, 1000, 0.4,\n  1000, 1250, 0.2,\n  1250, Inf, 0.0\n)\n\n# Mit geom_rect können wir unsere Distanzklassen visualisieren\nggplot(strassen_klassen, aes(xmin = von, xmax = bis, ymax = zu, fill = zu)) +\n  geom_rect(ymin = 0) +\n  scale_x_continuous(breaks = strassen_klassen$bis) +\n  scale_y_continuous(breaks = strassen_klassen$zu)\n\n\n\n\n\nBeispiel Distanzklassen anhand der Strassen\n\n\n\n\nMusterlösung\n\n# tribble erstellt eine data.frame, wir brauchen aber eine matrix\nstrassen_klassen &lt;- as.matrix(strassen_klassen)\n\n\nJetzt, wo wir diese Matrix haben, können wir sie nutzen, um den Kanton Schwyz hinsichtlich der Distanz zu Strassen zu bewerten. Dafür verwenden wir die Funktion classify() mit dem Argument include.lowest = TRUE, damit eine Distanz von 0m ebenfalls in 1 reklassifiziert wird.\n\n\nMusterlösung\nstrassen_classify &lt;- classify(strassen_dist, strassen_klassen, include.lowest = TRUE)\n\n# Visualisierung des Resultats\ntm_shape(strassen_classify) +\n  tm_raster(palette = \"-Spectral\") +\n  tm_layout(legend.outside = TRUE) +\n  tm_shape(strassen) +\n  tm_lines()\n\n\n\n\n\n\n\n\n\nBewerte auf die gleiche Art die Distanz zu den Schutzgebieten. Wir nutzen die Schwellenwerte, wie sie in der nachstehenden Tabelle ersichtlich ist (Tabelle 41.2). Du kannst diese aber frei wählen.\n\n\nMusterlösung\nschutzgebiete_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n  0, 250, 0.0,\n  250, 500, 0.2,\n  500, 750, 0.4,\n  750, 1000, 0.6,\n  1000, 1250, 0.8,\n  1250, Inf, 1.0\n)\n\nschutzgebiete_klassen &lt;- as.matrix(schutzgebiete_klassen)\n\nschutzgebiete_classify &lt;- classify(schutzgebiete_dist, schutzgebiete_klassen)\n\n\n\n\n\n\n\nTabelle 41.2: Bewertungstabelle für die Distanz zu Strassen und Wildschutzgebieten (Meter)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistanz zu Strassen\n\n\nDistanz zu Wildschutzgebieten\n\n\n\nvon\nbis\nzu\nvon\nbis\nzu\n\n\n\n\n0\n250\n1.0\n0\n250\n0.0\n\n\n250\n500\n0.8\n250\n500\n0.2\n\n\n500\n750\n0.6\n500\n750\n0.4\n\n\n750\n1000\n0.4\n750\n1000\n0.6\n\n\n1000\n1250\n0.2\n1000\n1250\n0.8\n\n\n1250\nInf\n0.0\n1250\nInf\n1.0",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Rauman 5: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1",
    "href": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 4: Raster Overlay",
    "text": "Aufgabe 4: Raster Overlay\nWir haben zwar erst zwei der Kriterien (Distanzen zu Strassen und Naturschutzgebiete) berechnet, die wir für unsere Standortsuche berücksichtigen wollen, doch mit denen können wir schon einmal eine erste, unvollständige Beurteilung wagen.\nWeil wir für alle Raster das gleiche Template verwendet haben, sind diese perfekt aneinander ausgerichtet. So können wir auf die denkbar einfachste Art die einezelnen Zellen miteinander verrechnen. Auf folgende Weise können wir beispielsweise den Mittlwert pro Zelle berechnen:\n\noverlay_prelim_1 &lt;- (strassen_classify + schutzgebiete_classify) / 2\n\ntm_shape(overlay_prelim_1) +\n  tm_raster(palette = \"-Spectral\") +\n  tm_shape(kt_schwyz) +\n  tm_borders(lwd = 5, col = \"black\")",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Rauman 5: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-rauman5-mask",
    "href": "rauman/Rauman5_Uebung_A.html#sec-rauman5-mask",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 5: Mask Raster",
    "text": "Aufgabe 5: Mask Raster\nIm letzten Plot fällt auf, dass wir auch eine Bewertung für Gebiete ausserhalb des Untersuchungsgebiets haben. Da wir für diese Gebiete keine Geodaten verwendet haben, sind die Resultate ausserhalb des Untersuchungsgebiets nicht gültig. Deshalb ist es sinnvoll, die Werte ausserhalb des Untersuchungsgebeits zu entfernen. Dafür verwenden wir die Funktion mask() zusammen mit dem Vektordatensatz kt_schwzy. Diese setzt alle Werte ausserhalb des Polygons zu NA:\n\n\nMusterlösung\noverlay_prelim_1 &lt;- mask(overlay_prelim_1, kt_schwyz)\n\ntm_shape(overlay_prelim_1) +\n  tm_raster()",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Rauman 5: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1b",
    "href": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1b",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 6: Weighted Raster Overlay",
    "text": "Aufgabe 6: Weighted Raster Overlay\nIm obigen Raster-Overlay haben wir alle Kriterien gleich stark gewichtet. Wir können aber auch eine gewichtete Verschneidung durchführen: Wenn wir beispielsweise die Distanz zu den Strassen stärker gewichten wollen als die Distanz zu den Schutzgebieten. Auch das ist sehr einfach:\n\n\nMusterlösung\n# Wir dividieren durch die Summe der Gewichte, um wieder Werte zwischen 0 und 1 zu erhalten.\noverlay_prelim_2 &lt;- (strassen_classify * 5 + schutzgebiete_classify * 1) / (5 + 1)\n\n# Resultate ausserhalb des Kantons entfernen:\noverlay_prelim_2 &lt;- mask(overlay_prelim_2, kt_schwyz)\n\n# Resultate visualisieren:\ntm_shape(overlay_prelim_2) +\n  tm_raster()\n\n\n\n\n\n\n\n\n\n\n\n\n\nGilgen, \"Kurt, und Alma Sartoris\". \"2010\". „\"Empfehlung zur Planung von Windenergieanlagen: Die Anwendung von Raumplanungsinstrumenten und Kriterien zur Standortwahl\"“. \"Eidgenössisches Departement für Umwelt, Verkehr, Energie und Kommunikation UVEK\".\n\n\nTegou, Leda-Ioanna, Heracles Polatidis, und Dias A. Haralambopoulos. 2010. „Environmental management framework for wind farm siting: Methodology and case study“. Journal of Environmental Management 91 (11): 2134–47. https://doi.org/10.1016/j.jenvman.2010.05.010.",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Rauman 5: Übung A</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html",
    "href": "rauman/Rauman5_Uebung_B.html",
    "title": "Rauman 5: Übung B",
    "section": "",
    "text": "Aufgabe 1: Rasterdaten einlesen\nIn der letzten Übung (Übung A) haben wir die potentielle Standorte für Windkraftanlagen hinsichtlich zweier Distanzkriterien bewertet. In dieser Übung (Übung B) schliessen wir die Multikriterien-Evaluation ab, indem wir:\nFür Punkt 1 müssen wir zusätzliche Daten einlesen, die im Rasterformat daherkommen. Punkt 2 beruht im Wesentlichen auf Daten, die wir bereits verwendet haben.\nZur Bewertung der Standorte hinsichtlich Windgeschwindigkeit steht uns der Datensatz wind250m.tif zur Verfügung (siehe Tabelle 41.1). Lade den Datensatz mit der Funktion rast() in R ein. Exploriere den Datensatz visuell und versuche ein Verständnis für die Datensätze zu bekommen.\nMusterlösung\nwind250m &lt;- rast(\"datasets/rauman/wind250m.tif\")\nMusterlösung\ntm_shape(wind250m) +\n  tm_raster(col.scale = tm_scale_continuous())\n\n\n\n\n\nDatensatz ‘wind250m’ zur Windgeschwindigkeit in m pro Sekunde",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Rauman 5: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#aufgabe-2-wind-bewerten",
    "href": "rauman/Rauman5_Uebung_B.html#aufgabe-2-wind-bewerten",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 2: Wind bewerten",
    "text": "Aufgabe 2: Wind bewerten\nDiese Rasterdaten müssen wir nicht weiter verarbeiten, wir können sie direkt bewerten. Führe diese Bewertung aufgrund nachstehender Tabelle durch. Nutze dafür die Funktion classify() analog Aufgabe 3: Distanzkriterien bewerten. Ihr könnt die Schwellenwerte frei wählen, wir werden diejenigen verwenden, die in Tabelle 42.1 festgehalten sind.\n\n\nMusterlösung\n#### reclassify wind\nwind_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n  0, 20, 0.0,\n  20, 30, 0.2,\n  30, 40, 0.4,\n  40, 50, 0.6,\n  50, 60, 0.8,\n  60, Inf, 1.0\n)\n\nwind_klassen &lt;- as.matrix(wind_klassen)\n\nwind_classify &lt;- classify(wind250m, wind_klassen)",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Rauman 5: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#aufgabe-3-slope-berechnen-und-bewerten",
    "href": "rauman/Rauman5_Uebung_B.html#aufgabe-3-slope-berechnen-und-bewerten",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 3: Slope berechnen und bewerten",
    "text": "Aufgabe 3: Slope berechnen und bewerten\nFür die Berechnung und anschliessende Bewertung der Hangneigung brauchen wir ein Höhenmodell. Lade das Höhenmodell dhm250m.tif herunter (siehe Tabelle 41.1) und in R ein. Berechne anschliessend die Hangneigung mit der Funktion terrain() analog Aufgabe 2 (beachte die Einheit des Output!).\nBewerte die Hangneigung danach gemäss Tabelle Tabelle 42.1.\n\n\nMusterlösung\ndhm250m &lt;- rast(\"datasets/rauman/dhm250m.tif\")\n\nneigung &lt;- terrain(dhm250m, v = \"slope\", unit = \"degrees\")\n\n#### reclassify slope\nneigung_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n  0, 4, 1.0,\n  4, 8, 0.8,\n  8, 12, 0.6,\n  12, 16, 0.4,\n  16, 20, 0.2,\n  20, 90, 0.0\n)\n\nneigung_klassen &lt;- as.matrix(neigung_klassen)\n\nneigung_classify &lt;- classify(neigung, neigung_klassen)\n\n\n\n\n\n\n\nTabelle 42.1: Bewertungstabelle die Windgeschwindigkeit (m/s) und Hangneigung (Grad)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWindgeschwindigkeit\n\n\nHangneigung\n\n\n\nvon\nbis\nzu\nvon\nbis\nzu\n\n\n\n\n0\n20\n0.0\n0\n4\n1.0\n\n\n20\n30\n0.2\n4\n8\n0.8\n\n\n30\n40\n0.4\n8\n12\n0.6\n\n\n40\n50\n0.6\n12\n16\n0.4\n\n\n50\n60\n0.8\n16\n20\n0.2\n\n\n60\nInf\n1.0\n20\n90\n0.0",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Rauman 5: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#sec-raster-overlay2",
    "href": "rauman/Rauman5_Uebung_B.html#sec-raster-overlay2",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 4: Raster Overlay",
    "text": "Aufgabe 4: Raster Overlay\nAnalog Aufgabe 4: Raster Overlay können wir an dieser Stelle eine vorläufige Beurteilung der Gebiete durchführen. Berechne dafür ein (gewichtetes) Mittel der Kriterien strassen_classify, schutzgebiete_classify, wind_classify und neigung_classify. Du kannst die Gewichte wieder so anpassen, wie du für sinnvoll hälst.\n\n\nMusterlösung\noverlay_prelim_3 &lt;- (strassen_classify + schutzgebiete_classify + wind_classify + neigung_classify) / 4\n\ntm_shape(overlay_prelim_3) +\n  tm_raster(col.scale = tm_scale_continuous(ticks = seq(0, 1, 0.2), values = \"brewer.spectral\")) +\n  tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\nAbbildung 42.1: Ungewichtetes Überlagern aller Kriterien mit Ausnahme der Ausschlussgebiete",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Rauman 5: Übung B</span>"
    ]
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#aufgabe-5-ausschlusskriterien",
    "href": "rauman/Rauman5_Uebung_B.html#aufgabe-5-ausschlusskriterien",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 5: Ausschlusskriterien",
    "text": "Aufgabe 5: Ausschlusskriterien\nAls Auschlussgebiete gelten Flächen, wo keine Windkraftanlagen gebaut werden können. Dazu gehören bewohnte Flächen, nationale Schutzgebiete, Waldgebiete und Seen. (Zwar werden Schutzgebiete in unserer Analyse bereits berücksichtigt, aber nicht kategorisch vom Resultat ausgeschlossen.)\n\n\nMusterlösung\nschutzgebiete &lt;- read_sf(gpkg_path, \"Nationale_Schutzgebiete\") |&gt; st_transform(2056)\nsiedlungsgebiet &lt;- read_sf(gpkg_path, \"Bewohnte_Flaeche\") |&gt; st_transform(2056)\nwald &lt;- read_sf(gpkg_path, \"Waldgebiete\") |&gt; st_transform(2056)\nseen &lt;- read_sf(gpkg_path, \"Seeflaechen\") |&gt; st_transform(2056)\nkt_schwyz &lt;- read_sf(gpkg_path, \"Untersuchungsgebiet_Schwyz\") |&gt; st_transform(2056)\n\n\nUm diese Flächen aus unserem Resultat auzuschliessen, können wir wieder die Funktion mask() verwenden (siehe Aufgabe 5: Mask Raster). Doch diesmal möchten wir nicht die Flächen ausserhalb der Polygone mit NA ersetzen, sondern die Flächen innerhalb der Polygone. Deshalb verwenden wir mask() mit dem Argument inverse = TRUE.\nVersuche mit mask(), den oben erwähnten Vektordatensätze sowie der Option inverse = TRUE die Ausschlussgebiete vom Raster-Overlay zu entfernen und visualisiere das Resultat.\n\n\nMusterlösung\noverlay_prelim_4 &lt;- overlay_prelim_3 |&gt;\n  mask(schutzgebiete, inverse = TRUE) |&gt;\n  mask(siedlungsgebiet, inverse = TRUE) |&gt;\n  mask(wald, inverse = TRUE) |&gt;\n  mask(seen, inverse = TRUE) |&gt;\n  mask(kt_schwyz)\n\ntmap_mode(\"view\")\n\ntm_shape(overlay_prelim_4) +\n  tm_raster(col.scale = tm_scale_continuous(ticks = seq(0, 1, 0.2), values = \"brewer.spectral\")) +\n  tm_basemap(\"Esri.WorldImagery\")",
    "crumbs": [
      "Räumliche Analysen",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Rauman 5: Übung B</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/0_Vorbereitung.html",
    "href": "fallstudie_s/0_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen der Fallstudie werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog der Vorbereitungsübung in Prepro1 könnt ihr mit nachstehendem Code alle noch nicht installierten Bibliotheken automatisch installieren.\n\nipak &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages &lt;- c(\n  \"readr\", \"ggplot2\", \"dplyr\", \"lubridate\", \"ggpubr\", \"PerformanceAnalytics\",\n  \"MuMIn\", \"AICcmodavg\", \"fitdistrplus\", \"lme4\", \"DHARMa\", \"blmeco\", \"sjPlot\", \"lattice\",\n   \"suncalc\", \"glmmTMB\"\n)\n\nipak(packages)\n\nZudem könnt ihr alle für die Fallstudie Profil S benötigten Daten unter auf Moodle im Abschnitt Fallstudie “Ecosystems & Biodiversity” herunterladen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Vorbereitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html",
    "href": "fallstudie_s/1_Einführung.html",
    "title": "Einführung",
    "section": "",
    "text": "Hintergrund\nDas rund 1100 ha grosse Naturschutzgebiet Wildnispark Zürich Sihlwald, welches im periurbanen Raum südlich von Zürich liegt, gilt seit dem 1. Januar 2010 als erster national anerkannter Naturerlebnispark. Er ist Teil des Wildnisparks Zürich und wichtiges Naherholungsgebiet für die Stadt Zürich.\nDas Schutzgebiet befindet sich im Spannungsfeld zwischen Schutz und Nutzen, denn einerseits sollen die Besuchenden den Wald erleben dürfen, andererseits soll sich dieser, in der Kernzone, frei entwickeln dürfen. Im Perimeter gelten darum verschiedene Regeln. So darf z. B. nur auf bestimmten Wegen mit den Velo gefahren werden.\nDas Management braucht solide, empirisch erhobene Daten zur Natur und zu den Besuchenden damit die Ziele von Nutzen und Schürzen erreicht werden können. Das Besuchermonitoring deckt den zweiten Teil dieser notwendigen Daten ab. Im Wildnispark Zürich sind dazu mehrere automatische Zählstellen in Betrieb. Die Zählstellen erfassen stundenweise die Besuchenden auf den Wegen. Einige Zählstellen erfassen richtungsgetrennt und / oder können zwischen verschiedenen Nutzergruppen wie Personen, die zu Fuss gehen, und Velofahrenden unterscheiden.\nIm Rahmen des Moduls Research Methods werden in dieser Fallstudie mehrere dieser automatischen Zählstellen genauer untersucht. Die Daten, welche im Besitz des WPZ sind, wurden bereits kalibriert. Das heisst, Zählungen während Wartungsarbeiten, bei Felhbetrieb o.ä. wurden bereits ausgeschlossen. Dies ist eine zeitintensive Arbeit und wir dürfen hier mit einem sauber aufbereiteten “Datenschatz” arbeiten.\nPerimeter des Wildnispark Zürichs mit den ungefähren Standorten von zwei ausgewählten automatischen Zählstellen.\nHinweis:\nDer Wildnispark wertet die Zahlen auf verschiedene Weise aus. So sind z. B. Jahresgänge (an welchen Monaten herrscht besonders viel Betrieb?) und die absoluten Nutzungszahlen bekannt. Vertiefte Auswertungen, die beispielsweise den Zusammenhang zwischen Besuchszahlen und dem Wetter untersuchen, werden nicht gemacht.\nUnsere Analysen in diesem Modul helfen dem Management, ein besseres Verständnis zum Verhalten der Besuchenden zu erlangen und bilden Grundlagen für Managemententscheide in der Praxis.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#hintergrund",
    "href": "fallstudie_s/1_Einführung.html#hintergrund",
    "title": "Einführung",
    "section": "",
    "text": "Die Zähler 211 und 502 erfassen sowohl Fussgänger:innen als auch Fahrräder. Die Erfassung erfolgt richtungsgetrennt.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#ziel",
    "href": "fallstudie_s/1_Einführung.html#ziel",
    "title": "Einführung",
    "section": "Ziel",
    "text": "Ziel\nIn dieser Fallstudie zeigen wir, welche Einflüsse die Covid19-Pandemie im Frühjahr 2020 auf die täglichen Besuchszahlen im Wildnispark Zürich hatte. Dabei setzen wir den Fokus auf die Dämmerung und die Nacht, den in diesen Zeiten sind Wildtiere (u.a. Rehe) besonders sensibel gegenüber Störungen. Wir untersuchen ebenfalls, wie sich die Besuchszhalen seit der Pandemie entwickelt haben und ob sie sich wieder dem Muster von vor der Pandemie annähern. Auch dabei ist die “dunkle” Tageszeit im Fokus.\nIn unsere Analysen ziehen wir auch weitere erklärende Faktoren wie Wetter, Wochentag, Kalenderwoche und Schulferien mit ein. Die statistischen Auswertungen erlauben und somit klare Rückschlüsse auf die Effekte der Faktoren und deren Stärke zu ziehen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#grundlagen",
    "href": "fallstudie_s/1_Einführung.html#grundlagen",
    "title": "Einführung",
    "section": "Grundlagen",
    "text": "Grundlagen\nZur Verfügung stehen:\n\ndie stündlichen Zählungen von Fussgänger:innen und Velos an den Zählstellen\nMeteodaten (Temperatur, Sonnenscheindauer, Niederschlagssumme)\nR-Skripte mit Hinweisen zur Auswertung",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/1_Einführung.html#aufbau-der-fallstudie",
    "href": "fallstudie_s/1_Einführung.html#aufbau-der-fallstudie",
    "title": "Einführung",
    "section": "Aufbau der Fallstudie",
    "text": "Aufbau der Fallstudie\nIn dieser Fallstudie erheben wir zuerst selbst Daten auf dem Grüntal, welche wir dann deskriptiv auswerten. Anschliessend beschäftigen wir uns mit den Daten aus dem Wildnispark Zürich, welche wir ebenfalls deskriptiv auswerten und auch sttistische Modelle damit programmieren. Diese Ergebnisse werden dann im Abschlussbericht dokumentiert.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/2_Besuchermonitoring.html",
    "href": "fallstudie_s/2_Besuchermonitoring.html",
    "title": "Monitoring",
    "section": "",
    "text": "Einführung\nEs gibt eine Vielzahl an möglichen Methoden zur Erfassung der Besuchszahlen. Automatische Zählgeräte bieten die Möglichkeit, lange und durchgehende Zeitreihen zu erfassen. Inputs dazu, wie diese ausgewertet werden können, erhält ihr in dieser Aufgabe.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Monitoring</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/2_Besuchermonitoring.html#ziele",
    "href": "fallstudie_s/2_Besuchermonitoring.html#ziele",
    "title": "Monitoring",
    "section": "Ziele",
    "text": "Ziele\n\nIhr könnt das eingesetzte Zählgerät installieren und kennt die Vor- und Nachteile verschiedener Methoden.\nIhr könnt die generierten Daten explorativ und deskriptiv auswerten.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Monitoring</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/2_Besuchermonitoring.html#grundlagen",
    "href": "fallstudie_s/2_Besuchermonitoring.html#grundlagen",
    "title": "Monitoring",
    "section": "Grundlagen",
    "text": "Grundlagen\nDie Geräte werden gemeinsam auf dem Campus Grüntal platziert.\nDatenschutz ist ein wichtiges Thema. Die eingesetzten Geräte erfassen keine Personendaten, sondern nur Bewegungen. Es handelt sich um Pyroelektrische Infrarotsensoren, welche auf den Temperaturunterschied reagieren, wenn sich eine Person vor der Linse bewegt. Insofern handelt es sich, vereinfacht gesagt, um Bewegungsmelder.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Monitoring</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "",
    "text": "Ziele\nIhr habt selbst ein (kleines) Besuchermonitoring auf dem Grüental durchgeführt und euch bereits mit dem WPZ beschäftigt. Die Aufgaben im Zusammenhang mit dem Grüental sind abgeschlossen und wir beschäftigen uns ab jetzt ausschliesslich mit dem WPZ.\nIm Rahmen unserer Analyse programmieren wir multivariate Modelle, welche den Zusammenhang zwischen der Anzahl Besuchenden und verschiedenen Einflussfaktoren beschreiben. Dank den Modellen können wir sagen, wie die Besuchenden auf die untersuchten Faktoren reagiert haben (siehe dazu auch euren Forschungsplan sowie [Einführung], Ziele).\nFür meine Analysen habe ich untenstehende Fragestellungen formuliert. Ihr könnt aber auch eure Fragestellungen aus eurem Forschungsplan übernehmen, falls ihr das möchtet und falls diese mit den zur Verügung stehenden Daten bearbeitet werden können.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#ziele",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#ziele",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "",
    "text": "Welchen Einfluss haben zeitliche Variablen (Wochentag, Ferien, Monat, Jahr, Phasen der Covid-Pandemie) und Wetterparameter (Sonnenscheindauer, Höchsttemperatur, Niederschlagssumme) auf die Besuchszahlen im WPZ?\nWie stark sind die jeweiligen Einflüsse, welche Effektrichtungen sind beobachtbar und welche der untersuchten Parameter sind signifikant?\nGibt es in den Effektrichtungen deutliche Unterschiede zwischen den Tageszeiten und wie können diese erklärt werden?\nWelches sind die mutmasslichen Auswirkungen dieser Nutzung auf das Verhalten der Rege im WPZ?\n\n\n\nJede Gruppe wertet Daten von einem Zählgerät aus. Sprecht miteinander ab, wer welchen Zähler behandelt (211 oder 502; Spezifikationen siehe [Einführung], Hinweis). Die Daten von jeder Zählstelle sollen nur von einer Gruppe ausgewertet werden!\nBezieht in eure Auswertungen den gesamten zur Verfügung stehenden Zeitraum ein.\nFür euren Zähler stehen Zahlen zu Fussgänger:innen und Fahrrädern zur Verfügung (siehe [Einführung], Hinweis). Entscheidet euch selbst, ob ihr Fussgänger:innen ODER Fahrräder auswerten wollt. Die anderen Daten dürft ihr vernachlässigen.\nIm Bericht sollen die Informationen und Erfahrungen aus dem gesamten Verlauf der Fallstudie in geeigneter Weise einfliessen. Bezüglich der Felderhebung Grüental erwarten wir keine Angaben.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#erwartungen",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#erwartungen",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "Erwartungen",
    "text": "Erwartungen\n\nStruktur / Aufbau\n\n\nAbstract / Zusammenfassung (wenn Bericht auf Deutsch, dann Zusammenfassung auch auf Deutsch). Im Abstract sollen alle Bestandteile des Berichts aufgenommen sein.\nEinleitung (hier können allenfalls Elemente / Teile aus den Forschungsplänen übernommen werden)\nFragestellung (siehe oben; die Fragestellung ist vorgegeben, darf aber für den Bericht geschärft / ausformuliert und konkretisiert werden.)\nMethoden (aufschlussreiches Kapitel mit den statistischen Analysen)\nResultate (deskriptive Statistik, multivariates Modell; kurzer Fliesstext sowie die notwendigen Tabellen und eine Auswahl möglichst informativer Grafiken)\nDiskussion (Diskussion der deskriptiven Analysen und der Modellergebnisse; dieser Abschnitt sollte die eigenen Resultate auch im Zusammenhang mit aktueller Fachliteratur reflektieren.)\nLiteraturverzeichnis (Tipp: Das Literaturverzeichnis sollte vollständig sein, sowie formal korrekt und einheitlich daherkommen. Wir erwarten speziell in der Diskussion eine Abstützung auf aktuelle Fachliteratur. Auf Moodle haben wir euch eine Auswahl relevanter Studien bereitgestellt.)\nAnhang (für alle Auswertungen relevanter R-Code in geeigneter Form)\n\n\nGesamtumfang max. 12’000 Zeichen (inkl. Leerzeichen; exkl. Tabellen, Literaturverzeichnis und Anhang)\nAbgabe am 12.1.2025 per Mail an hoce@zhaw.ch",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#bewertungskriterien",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#bewertungskriterien",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "Bewertungskriterien",
    "text": "Bewertungskriterien\n\nIst die Methode klar und verständlich formuliert?\nSind die deskriptiven Analysen klar beschrieben und geeignet visualisiert?\nIst die Variablenselektion klar beschrieben, plausibel und nachvollziehbar?\nSind die Modellresultate in Text- und Tabellenform korrekt beschrieben und geeignet visualisiert?\nIst die Diskussion klar formuliert und inhaltlich schlüssig?\nWie gut ist die Diskussion auf relevante und aktuelle Fachliteratur abgestützt?\nZusätzliche bewerten wir die inhaltliche Dichte der Arbeit und die formale Qualität (Sprache, Struktur, Aufbau, Darstellung, Literaturverzeichnis, Umgang mit Literatur im Text)",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung_WPZ.html#zusammensetzung-note-fallstudie",
    "href": "fallstudie_s/3_Aufgabenstellung_WPZ.html#zusammensetzung-note-fallstudie",
    "title": "Aufgabenstellung Abschlussbericht",
    "section": "Zusammensetzung Note Fallstudie:",
    "text": "Zusammensetzung Note Fallstudie:\n\nFallstudie-Leistungsnachweis 1 - Forschungsplan: Testatplichtig\nFallstudie-Leistungsnachweis 2 - Multivariate Analyse: 100 %\n\nDie Note der Fallstudie fliesst in die Gesamtbewertung des Moduls Research Methods mit ein.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Aufgabenstellung Abschlussbericht</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html",
    "href": "fallstudie_s/4_Projektierung.html",
    "title": "Projek erstellen",
    "section": "",
    "text": "Vorbereitung\nVor den eigentlichen Auswertungen müssen einige Vorbereitungen unternommen werden. Die Zeit, die man hier investiert, wird in den späteren Phasen um ein Mehrfaches eingespart.\nWie im Unterricht am Morgen empfehle auch ich mit Projekten zu arbeiten, da diese sehr einfach ausgetauscht (auf verschiedene Rechner) und somit auch reproduziert werden können. Wichtig ist, dass es keine absoluten Arbeitspfade sondern nur relative gibt. Der Datenimport (und -export) kann mithilfe dieser relativen Pfade stark vereinfacht werden. –&gt; Kurz gesagt: Projekte helfen alles am richtigen Ort zu behalten (mehr zur Arbeit mit Projekten: Link).\n–&gt; File / New Project",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#vorbereitung",
    "href": "fallstudie_s/4_Projektierung.html#vorbereitung",
    "title": "Projek erstellen",
    "section": "",
    "text": "Erstellt an einem passenden Speicherort (evtl. onedrive für das gemeinsame Arbeiten an einem Projekt) ein neues Projekt mit einem treffenden Namen:",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "title": "Projek erstellen",
    "section": "Aufgabe 1: Projektaufbau",
    "text": "Aufgabe 1: Projektaufbau\nHinweise:\nNutzt für allen Text, welcher nicht im Code integriert ist, das Symbol #. Wenn ihr den Text als Titel definieren wollt, so dass er in der Übersicht erscheint, können vor dem Wort “#” und nach dem Wort “####” eingefügt werden.\n\n# Texte, vor denen ein # und nach denen #### stehen, sind Überschriften\n\n# Ich bin eine first level Überschrift ####\n\n## Ich bin eine second level Überschrift ####\n\n# Texte, vor denen ein # steht, erklaeren den Ablauf (nicht was, sondern warum)\n\n# Dann folgen die Arbeitsschritte\n1 + 1\n\n# Wenn man auf \"Outline\" klickt (oder CTRL + SHIFT + O),\n# öffnet sich die Übersicht zu den Überschriften\n\nTipps:\n\nAlt + - = &lt;-\nCtrl + Shift + C = # vor der ausgewählten Zeile / den ausgewählten Zeilen hinzufügen oder wieder löschen\n\nAufbau eines Skripts\nZuerst immer den Titel des Projekts sowie den Autor/ die Autorin des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer Dateneigentümer ist (WPZ und Meteo Schweiz).\nIm Skript soll immer die Ordnerstruktur des Projekts genannt werden. So kann der Arbeitsvorgang auf verschiedenen Rechnern einfach reproduziert werden (ich verwende hier ein Projektordner mit den Unterordnern __scripts, data, results).\nBeschreibt zudem kurz die verwendeten Meteodaten (siehe dazu Metadata Meteodaten, –&gt; order_XXX_legend.txt)\nEin Skript kann in R eigentlich immer (mehr oder weniger) nach dem selbem Schema aufgebaut sein. Dieses Schema enthält bei uns folgende Kapitel:\n\nMetadaten und Definitionen\nDatenimport\nVorbereitung\nDeskriptive Analyse und Visualisierung\nMultifaktorielle Analyse und Visualisierung\n\nBereitet euer Skript mit diesen Kapitel vor.\n\n# .###########################################################################################\n# TITEL ####\n# Fallstudie Modul Research Methods, HS24. Autor/in ####\n# .##########################################################################################\n\n# .##########################################################################################\n# METADATA UND DEFINITIONEN ####\n# .##########################################################################################\n\n# Datenherkunft ####\n# ...\n\n# .##########################################################################################\n# 1. DATENIMPORT #####\n# .##########################################################################################\n\nIn einem professionellen Bericht ist es angebracht, wenn alle Abbildung einheitlich sind. Dafür braucht es u.a. eine Farbpalette. Ich definiere meine Auswahl bereits hier; das hat den Vorteil, dass man die Farbnamen nur einmal schreiben muss und später die selbst definierte Palette unter der Variable “mycolors” abrufen kann.\n\nmycolors &lt;- c(\"orangered\", \"gold\", \"mediumvioletred\", \"darkblue\")",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "title": "Projek erstellen",
    "section": "Aufgabe 2: Laden der Bibliotheken",
    "text": "Aufgabe 2: Laden der Bibliotheken\n\nLädt nun die nachfolgenden, benötigten Bibliotheken.\nDiese habt ihr, falls nötig, im Kapitel Vorbereitung installiert.\n\n\n# Benoetigte Bibliotheken ####\nlibrary(\"readr\") # read data into r\nlibrary(\"ggplot2\") # plot nice graphs\nlibrary(\"dplyr\") # select data\nlibrary(\"lubridate\") # Arbeiten mit Datumsformaten\nlibrary(\"suncalc\") # berechne Tageszeiten abhaengig vom Sonnenstand\nlibrary(\"ggpubr\") # to arrange multiple plots in one graph\nlibrary(\"PerformanceAnalytics\") # Plotte Korrelationsmatrix\nlibrary(\"MuMIn\") # Multi-Model Inference\nlibrary(\"AICcmodavg\") # Modellaverageing\nlibrary(\"fitdistrplus\") # Prueft die Verteilung in Daten\nlibrary(\"lme4\") # Multivariate Modelle\nlibrary(\"DHARMa\") # Modeldiagnostik\nlibrary(\"blmeco\") # Bayesian data analysis using linear models\nlibrary(\"sjPlot\") # Plotten von Modellergebnissen (tab_model)\nlibrary(\"lattice\") # einfaches plotten von Zusammenhängen zwischen Variablen\nlibrary(\"glmmTMB\")# zero-inflated model",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "title": "Projek erstellen",
    "section": "Aufgabe 3: Zeitliche Definitionen",
    "text": "Aufgabe 3: Zeitliche Definitionen\nWir lesen später zwei verschiedene Datensätze ein. Beide sollen exakt denselben Zeitraum umfassen. Definiert dazu den ersten und letzten Tag gemäss den bei euch vorhandenen Zähldaten (das unterscheidet sich von Gruppe zu Gruppe).\n\ndepo_start &lt;- as.Date(\"2017-01-01\")\ndepo_end &lt;- as.Date(\"2023-12-31\")\n\nEin Teil unserer Auswertungen ist der Einfluss der Corona-Lockdown auf das Besuchsverhalten.\n-Wir müssen also Start und Ende der beiden Lockdowns in der Schweiz definieren:\n\nlock_1_start_2020 &lt;- as.Date(\"2020-03-16\")\nlock_1_end_2020 &lt;- as.Date(\"2020-05-11\")\n\nlock_2_start_2021 &lt;- as.Date(\"2020-12-22\")\nlock_2_end_2021 &lt;- as.Date(\"2021-03-01\")\n\nEbenfalls müssen die erste und letzte Kalenderwoche der Untersuchungsfrist definiert werden. Diese werden bei wochenweisen Analysen ausgeklammert da sie i.d.R. unvollständig sind (das ist ein späterer Arbeitsschritt). Geht wie oben vor. Tipp: der Befehl isoweek() liefert euch die Kalenderwoche.\nFerienzeiten können einen grossen Einfluss auf das Besucheraufkommen haben. Die relevanten Ferienzeiträume müssen daher bekannt sein (heruntergeladen von https://www.schulferien.org/schweiz/ferien/2020/).\nLest das bereitgestellte .csv mit den Ferienzeiträumen ein und speichert es unter schulferien.\n\nschulferien &lt;- read_delim(\"datasets/fallstudie_s/ferien.csv\", \",\")\n\nNun sind alle Vorbereitungen gemacht, die Projektstruktur aufgebaut und die eigentliche Arbeit kann im nächsten Schritt beginnen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Projek erstellen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html",
    "href": "fallstudie_s/5_Datenverarbeitung.html",
    "title": "Datenverarbeitung",
    "section": "",
    "text": "Aufgabe 1: Zähldaten",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-1-zähldaten",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-1-zähldaten",
    "title": "Datenverarbeitung",
    "section": "",
    "text": "1a)\nDie Projektstruktur steht. Nun können die Daten eingelesen und die nötigen Datentypen definiert werden.\nLädt die Daten zuerst von Moodle herunter:\n\nZähldaten zu eurem Standort (211_sihlwaldstrasse_2017_2024.csv, 502_sihluferweg_2016_2024.csv)\nMeteodaten und Legende\n\nHinweis: Siehe [Einführung] für den Standort der Zähler 211 und 502.\nDie Zähldaten des WPZ wurden vorgängig bereinigt (z.B. wurden Stundenwerte entfernt, an denen am Zähler Wartungsarbeiten stattgefunden haben). Das macht es für uns einfach, denn wir können die Daten ohne vorgängige Bereinigung einlesen. Behaltet aber im Hinterkopf, dass die Datenaufbereitung, die Datenbereinigung mit viel Aufwand verbunden ist.\n\nLest die Zählaten ein, speichert ihn unter der Variable depo und sichtet den Datensatz (z.B. str(), head(), view() usw.).\n\n\n\nMusterlösung\n# Speicherort sowie Dateiname anpassen\ndepo &lt;- read_delim(\"./HIER RELATIVEN DATEIPFAD EINGEBEN\", \"HIER SEPERATOR EINGEBEN\")\n\n\nHinweis: Im Stundenformat zeigen die Werte bei 11:00 die Zähldaten zwischen 11:00 bis 12:00 Uhr.\n\n\nMusterlösung\n# lese die Daten ein\ndepo &lt;- read_delim(\"datasets/fallstudie_s/211_sihlwaldstrasse.csv\", \";\")\n\n# erstes Sichten und anpassen der Datentypen\nstr(depo)\n\n\n\n\n1b)\n\nNun muss das Datum als solches definiert werden. Ich nutze dazu as.POSIXct(). Welches Format hat das Datum im csv? Das muss im Code angepasst werden.\n\n\n\nMusterlösung\ndepo &lt;- depo |&gt;\n  mutate(\n    Datetime = as.POSIXct(DatumUhrzeit, format = \"HIER STEHT DAS DATUMSFORMAT\", tz = \"CET\"),\n    # nun schreiben wir uns das Datum in eine seperate Spalte\n    Datum = as.Date(Datetime)\n  )\n\n\n\n\nMusterlösung\n# hier der code mit dem richtigen Format\ndepo &lt;- depo |&gt;\n  mutate(\n    Datetime = as.POSIXct(as.character(Datetime), format = \"%Y%m%d%H\", tz = \"CET\"),\n    Datum = as.Date(Datetime)\n  )\n\n\n\n\n1c)\nIhr könnt selbst wählen, ob ihr Fussgänger:innen oder Fahrräder untersuchen wollt (je nachdem ob sie in eurem Datensatz vorhanden sind).\n\nEntfernt die überflüssigen Spalten aus dem Datensatz. Ich schlage vor, dass ihr dafür den Befehl dplyr::select() verwendet.\nDamit kann man entweder Spalten behalten oder eben auch Spalten entfernen (-c(SPALTENNAMEN)).\n\nHinweis: mit select() können Spalten gewählt werden, mit filter() Zeilen.\n\n\nMusterlösung\n# In dieser Auswertung werden nur Personen zu Fuss betrachtet!\n# it select werden spalten ausgewaehlt oder eben fallengelassen\ndepo &lt;- depo |&gt;\n  dplyr::select(-c(Velo_IN, Velo_OUT))\n\n\n\n\n1d)\n\nBerechnen des Totals (IN + OUT), da dieses in den Daten nicht vorhanden ist.\n\nTipp: Wenn man R sagt: “addiere mir Spalte x mit Spalte y”, dann macht R das für alle Zeilen in diesen zwei Spalten. Wenn man nun noch sagt: “speichere mir das Ergebnis dieser Addition in einer neuen Spalte namens Total”, dann hat man die Aufgabe bereits gelöst. Arbeitet mit mutate()).\n\nEntfernt nun alle NA-Werte mit na.omit().\n\n\n\nMusterlösung\n# Berechnen des Totals, da dieses in den Daten nicht vorhanden ist\ndepo &lt;- depo |&gt;\n  mutate(Total = Fuss_IN + Fuss_OUT)\n\n# Entferne die NA's in dem df.\ndepo &lt;- na.omit(depo)",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-2-meteodaten",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-2-meteodaten",
    "title": "Datenverarbeitung",
    "section": "Aufgabe 2: Meteodaten",
    "text": "Aufgabe 2: Meteodaten\n\n2a)\n\nLest die Meteodaten ein und speichert sie unter meteo.\n\n\n\nMusterlösung\n# Einlesen\nmeteo &lt;- read_delim(\"datasets/fallstudie_s/order_124839_data.txt\", \";\")\n\n\n\n\n2b)\n\nAuch hier müssen die Datentypen manuell gesetzt werden.\n\nTipp: Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewandelt werden aus dem dann das eigentliche Datum herausgelesen werden kann. Das ist mühsam - darum hier der Code.\n\n\nMusterlösung\nmeteo &lt;- mutate(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\n\nHinweise:\nDie Zeitangaben sind in UTC: 00:40 UTC = 02:40 Sommerzeit = 01:40 Winterzeit, Beispiel: 13 = beinhaltet Messperiode von 12:01 bis 13:00\nDa wir mit Tageshöchstwerten oder -summen rechnen, können wir zum Glück ignorieren, dass das nicht mit den Daten der Zählstellen übereinstimmt. Learning: es ist zentral immer die Metadaten zu checken.\nWas ist eigentlich Niederschlag:\nLink Meteo Schweiz\n\nWerden den anderen Spalten die richtigen Typen zugewiesen? Falls nicht, ändert die Datentypen.\nNun schneiden wir den Datensatz auf die Untersuchungsdauer zu.\n\n\n\nMusterlösung\n    ... |&gt;\n    filter(time &gt;= depo_start, time &lt;= depo_end)\n\n\n\nDann müssen auch hier alle nicht verfügbare Werte (NA’s) herausgefiltert werden. Macht das wieder mit na.omit()\nPrüft nun, wie die Struktur des data.frame (df) aussieht und ob alle NA Werte entfernt wurden:\n\n\n\nMusterlösung\nsum(is.na(df$Variable))\n\n\n\nStimmen alle Datentypen? str()\n\n\n\nMusterlösung\n# Die eigentlichen Messwerte sind alle nummerisch\nmeteo &lt;- meteo |&gt;\n    mutate(\n        tre200nx = as.numeric(tre200nx),\n        tre200jx = as.numeric(tre200jx),\n        rre150n0 = as.numeric(rre150n0),\n        rre150j0 = as.numeric(rre150j0),\n        sremaxdv = as.numeric(sremaxdv)\n    ) |&gt;\n    filter(time &gt;= depo_start, time &lt;= depo_end) # schneide dann auf Untersuchungsdauer\n\n# Was ist eigentlich Niederschlag:\n# https://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\n# Filtere Werte mit NA\nmeteo &lt;- na.omit(meteo)\n# Pruefe ob alles funktioniert hat\nstr(meteo)\nsum(is.na(meteo)) # zeigt die Anzahl NA's im data.frame an",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-3-datenvorverarbeitung-mutationen",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-3-datenvorverarbeitung-mutationen",
    "title": "Datenverarbeitung",
    "section": "Aufgabe 3: Datenvorverarbeitung (Mutationen)",
    "text": "Aufgabe 3: Datenvorverarbeitung (Mutationen)\n\n3a)\nJetzt fügen wir viele Convenience Variablen hinzu. Wir brauchen:\n\nWochentag; der Befehl dazu ist wday(). Danach als Faktor speichern.\nWerktag oder Wochenende, ebebfalls als Faktor.\n\nDer Code dazu könnte so aussehen:\n\n\nMusterlösung\n  ...|&gt;\n  mutate(Wochenende = ifelse(Wochentag %in% c(6,7), \"Wochenende\", \"Werktag\")) |&gt;\n  # 1 means Monday and 7 means Sunday (default)\n  mutate(Wochenende = as.factor(Wochenende)) |&gt;\n  ...\n\n\nje als Faktor:\n\nKalenderwoche: isoweek()\nMonat: month()\nJahr: year()\n\n\n\nMusterlösung\ndepo &lt;- depo |&gt;\n  # wday sortiert die Wochentage automatisch in der richtigen Reihenfolge\n  mutate(\n    Wochentag = wday(Datetime, week_start = 1),\n    Wochentag = factor(Wochentag),\n    # Werktag oder Wochenende hinzufuegen\n    Wochenende = ifelse(Wochentag %in% c(6, 7), \"Wochenende\", \"Werktag\"),\n    Wochenende = as.factor(Wochenende),\n    # Kalenderwoche hinzufuegen\n    KW = isoweek(Datetime),\n    KW = factor(KW),\n    # monat und Jahr\n    Monat = month(Datetime),\n    Monat = factor(Monat),\n    Jahr = year(Datetime),\n    Jahr = factor(Jahr))\n\n\nDies machen wir auch mit dem “meteo” Datensatz. Wiederum bitte Wochentag, Werktag oder Wochenende, Kalenderwoche, Monat und Jahr. Ebenfalls alles als Faktor speichern.\n\n\nMusterlösung\n# Wir gruppieren die Meteodaten noch nach Kalenderwoche und Werktag / Wochenende\n# Dafür brauchen wir zuerst diese als Convenience Variablen\nmeteo &lt;- meteo |&gt;\n  # wday sortiert die Wochentage automatisch in der richtigen Reihenfolge\n  mutate(\n    Wochentag = wday(time, week_start = 1),\n    Wochentag = factor(Wochentag),\n    # Werktag oder Wochenende hinzufuegen\n    Wochenende = ifelse(Wochentag %in% c(6, 7), \"Wochenende\", \"Werktag\"),\n    Wochenende = as.factor(Wochenende),\n    # Kalenderwoche hinzufuegen\n    KW = isoweek(time),\n    KW = factor(KW),\n    # monat und Jahr\n    Monat = month(time),\n    Monat = factor(Monat),\n    Jahr = year(time),\n    Jahr = factor(Jahr))\n\n\nWieder zurück zum depo-Datensazt.\nIch mache den folgenden Punkt nachgelagert zu den voerherigen Convenience Variablen, da zu viele Operationen in einem Schritt auch schon mal etwas durcheinander erzeugen können.\nPhasen der Covid-Pandemie (Code untenstehend). Wir definieren 5 Phasen:\n\nvon Anfang Untersuchungsperiode bis vor Lockdown 1\nLockdown 1\nzwischen den Lockdowns\nLockdown 2\nEnde 2. Lockdown bis Ende Untersuchungsperiode\nWir packen alle Phasen in eine Spalte –&gt; long-format ist praktischer für das plotten als wide-format.\nSpäter im multivariaten Modell werden die Levels der Variablen per “default” alphabetisch geordnet und die Effektstärken der einzelnen Levels gegenüber dem ersten Level gerechnet. Das macht wenig Sinn, den die Levels sind nicht alphabetisch, sondern gemäss der Liste oben (später mehr dazu). Das passen wir ebenfalls an.\nHier der Code dazu:\n\n\n\nMusterlösung\ndepo &lt;- depo |&gt;\n    mutate(Phase = case_when(\n        Datetime &lt; lock_1_start ~ \"Pre\",\n        Datetime &gt;= lock_1_start & Datetime &lt;= lock_1_end ~ \"Lockdown_1\",\n        Datetime &gt; lock_1_end & Datetime &lt; lock_2_start ~ \"Inter\",\n        Datetime &gt;= lock_2_start & Datetime &lt;= lock_2_end ~ \"Lockdown_2\",\n        Datetime &gt; lock_2_end ~ \"Post\"\n    ))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\ndepo &lt;- depo |&gt;\n    # mit factor() koennen die levels direkt einfach selbst definiert werden.\n    # wichtig: speizfizieren, dass aus R base, ansonsten kommt es zu einem\n    # mix-up mit anderen packages\n    mutate(Phase = base::factor(Phase, levels = c(\"Pre\", \"Lockdown_1\", \"Inter\", \"Lockdown_2\", \"Post\")))\n\nstr(depo)\n\n\nNeben dem Lockdown können auch die Schulferien einen Einfluss auf die Besuchszahlen haben. Wir haben die Schulferien bereits als .csv eingelesen. Allerdings können wir die Schulferien nicht mit der case_when()-Funktion zuweisen, da diese mit dieser Funktion alle Vektoren im Datensatz “schulferien” verglichen werden, und nicht elementweise für jede Zeile im “depo”-Datensatz. Dies führt dazu, dass die Bedingungen nur einmal überprüft werden und dann auf den gesamten Vektor angewendet werden, anstatt Zeile für Zeile.\n\nWeil dies etwas kompliziert ist, hier eine Funktion zur Zuweisung der Ferien, welche ihr kopieren könnt:\n\n\n\nMusterlösung\n# schreibe nun eine Funktion zur zuweisung Ferien. WENN groesser als start UND kleiner als\n# ende, DANN schreibe ein 1\nfor (i in 1:nrow(schulferien)) {\n  depo$Ferien[depo$Datum &gt;= schulferien[i, \"Start\"] & depo$Datum &lt;= schulferien[i, \"Ende\"]] &lt;- 1\n}\ndepo$Ferien[is.na(depo$Ferien)] &lt;- 0\n\n# als faktor speichern\ndepo$Ferien &lt;- factor(depo$Ferien)\n\n\n\n\n3b)\n\nNun soll noch die volle Stunde als Integer im Datensatz stehen. Macht das mit dem Befehl hour()\n\n\n\nMusterlösung\n# Fuer einige Auswertungen muss auf die Stunden als nummerischer Wert zurueckgegriffen werden\ndepo$Stunde &lt;- hour(depo$Datetime)\n# hour gibt uns den integer\ntypeof(depo$Stunde)\n\n\n\n\n3c)\nDie Daten wurden durch den WPZ kalibriert (Nachkommastellen). Unser späteres Modell kann nicht mit Nachkommastellen in der abhängigen Variable umgehen (später dazu mehr).\n\nRundet die Zähldaten in der Spalte “Total” auf 0 Nachkommastellen. Der Befehl lautet round()\nDefiniert sie sicherheitshalber als Integer (= Ganzzahl)\nMacht das nun noch für IN und OUT.\n\n\n\nMusterlösung\ndepo$Total &lt;- as.integer(round(depo$Total, digits = 0))\n\ndepo$Fuss_IN &lt;- as.integer(round(depo$Fuss_IN, digits = 0))\n\ndepo$Fuss_OUT &lt;- as.integer(round(depo$Fuss_OUT, digits = 0))\n\n\n\n\n3d) Tageszeit\nWir setzen den Fokus unserer Untersuchung auf die Veränderung der Besuchszahlen in der Abend- und Morgendämmerung sowie der Nacht. Dafür müssen wir diese tageszeitliche Einteilung der Daten erst machen. Da dies über den Umfang dieser Fallstudie hinaus geht, liefere ich euch hier den Code dazu.\nDie wichtigsten Punkte:\n\nDie Tageslänge wurde für den Standort Zürich (Zeitzone CET) mit dem Package “suncalc” berechnet. Dabei wurden Sommer- und Winterzeit berücksichtigt.\nDie Einteilung der Tageszeit beruht auf dem Start und dem Ende der astronomischen Dämmerung sowie der Golden Hour. Der Morgen und der Abend wurden nach dieser Definition berechnet und um je eine Stunde Richtung Tag verlängert.\nUntenstehenden Code könnt ihr einfach kopieren.\nBeschreibt in eurem Bericht später, dass ihr die Einteilung der Tageszeit gemäss den Dämmerungszeiten in Zürich und gemäss meinem Code gemacht habt.\n\nHinweis: damit case_when() funktioniert, müsst ihr dplyr Version als 1.1.1 oder neuer haben. Das könnt ihr unter “Packages” (neben dem Reiter “Plots”, unten rechts) prüfen.\n\n\nMusterlösung\n# Einteilung Standort Zuerich\nLatitude &lt;- 47.38598\nLongitude &lt;- 8.50806\n\n# Start und das Ende der Sommerzeit:\n# https://www.schulferien.org/schweiz/zeit/zeitumstellung/\n\n\n# Welche Zeitzone haben wir eigentlich?\n# Switzerland uses Central European Time (CET) during the winter as standard time,\n# which is one hour ahead of Coordinated Universal Time (UTC+01:00), and\n# Central European Summer Time (CEST) during the summer as daylight saving time,\n# which is two hours ahead of Coordinated Universal Time (UTC+02:00).\n# https://en.wikipedia.org/wiki/Time_in_Switzerland\n\n# Was sind Astronomische Dämmerung und Golden Hour ueberhaupt?\n# https://sunrisesunset.de/sonne/schweiz/zurich-kreis-1-city/\n# https://www.rdocumentation.org/packages/suncalc/versions/0.5.0/topics/getSunlightTimes\n\n# Wir arbeiten mit folgenden Variablen:\n# \"nightEnd\" : night ends (morning astronomical twilight starts)\n# \"goldenHourEnd\" : morning golden hour (soft light, best time for photography) ends\n# \"goldenHour\" : evening golden hour starts\n# \"night\" : night starts (dark enough for astronomical observations)\n\nlumidata &lt;-\n    getSunlightTimes(\n        date = seq.Date(depo_start, depo_end, by = 1),\n        keep = c(\"nightEnd\", \"goldenHourEnd\", \"goldenHour\", \"night\"),\n        lat = Latitude,\n        lon = Longitude,\n        tz = \"CET\"\n    ) |&gt;\n    as_tibble()\n\n# jetzt haben wir alle noetigen Angaben zu Sonnenaufgang, Tageslaenge usw.\n# diese Angaben koennen wir nun mit unseren Zaehldaten verbinden:\ndepo &lt;- depo |&gt;\n    left_join(lumidata, by = c(Datum = \"date\"))\n\ndepo &lt;- depo |&gt;\n    mutate(Tageszeit = case_when(\n        Datetime &gt;= nightEnd & Datetime &lt;= goldenHourEnd ~ \"Morgen\",\n        Datetime &gt; goldenHourEnd & Datetime &lt; goldenHour ~ \"Tag\",\n        Datetime &gt;= goldenHour & Datetime &lt;= night ~ \"Abend\",\n        .default = \"Nacht\"\n    )) |&gt;\n    mutate(Tageszeit = factor(Tageszeit, levels = c(\"Morgen\", \"Tag\", \"Abend\", \"Nacht\"), ordered = TRUE))\n\n# behalte die relevanten Var\ndepo &lt;- depo |&gt; dplyr::select(-nightEnd, -goldenHourEnd, -goldenHour, -night, -lat, -lon)\n\n# Plotte zum pruefn ob das funktioniert hat\nggplot(depo, aes(y = Datetime, color = Tageszeit, x = Stunde)) +\n    geom_jitter() +\n    scale_color_manual(values = mycolors)\n\nsum(is.na(depo))\n\n# bei mir hat der Zusatz der Tageszeit noch zu einigen NA-Wertren gefueht.\n# Diese loesche ich einfach:\ndepo &lt;- na.omit(depo)\n# hat das funktioniert?\nsum(is.na(depo))",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-4-aggregierung-der-stundendaten",
    "href": "fallstudie_s/5_Datenverarbeitung.html#aufgabe-4-aggregierung-der-stundendaten",
    "title": "Datenverarbeitung",
    "section": "Aufgabe 4: Aggregierung der Stundendaten",
    "text": "Aufgabe 4: Aggregierung der Stundendaten\n\n4a)\nUnsere Daten liegen, wie ihr wisst, im Stundenformat vor. Für einige Auswertungen müssen wir aber auf ganze Tage zurückgreifen.\n\nDie Stundendaten werden zu ganzen Tagen aggregiert. Bezieht nur die Gruppierung (group_by()) Datum mit ein und speichert das Resultat unter depo_d (“_d” für “day”).\n\nHinweis: Wir gruppieren nur nach Datum, da ich mit den vielen weiteren Gruppierungen hier Probleme hatte, eine korrekte Summe zu erhalten.\n\n\nMusterlösung\ndepo_d &lt;- depo |&gt; \n  group_by(Datum) |&gt;   # Gruppieren nach der Variable Datum\n  summarise(Total = sum(Fuss_IN + Fuss_OUT),# Berechnen der gewünschten Werte\n            Fuss_IN = sum(Fuss_IN),\n            ...\n\n\n\n\nMusterlösung\n# hier werden also pro Nutzergruppe und Richtung die Stundenwerte pro Tag aufsummiert\ndepo_d &lt;- depo |&gt;\n  group_by(Datum) |&gt;\n  summarise(\n    Total = sum(Fuss_IN + Fuss_OUT),\n    Fuss_IN = sum(Fuss_IN),\n    Fuss_OUT = sum(Fuss_OUT)\n  )\n\n\n\nBerechne die Anzahl Tage bis Neujahr, wir brauchen sie später in den Modellen\n\n\n\nMusterlösung\ndepo_d &lt;- depo_d |&gt; \n  mutate(Tage_bis_Neujahr = as.numeric(difftime(ymd(paste0(year(Datum), \"-12-31\")), Datum, units = \"days\")))\n\n\n\nund füge nochmals alle Convenience Variablen gemäss oben ein:\n\n\n\nMusterlösung\ndepo_d &lt;- depo_d |&gt;\n  mutate(\n  Wochentag = wday(Datum, week_start = 1),\n  Wochentag = factor(Wochentag),\n  # Werktag oder Wochenende hinzufuegen\n  Wochenende = ifelse(Wochentag %in% c(6, 7), \"Wochenende\", \"Werktag\"),\n  Wochenende = as.factor(Wochenende),\n  # Kalenderwoche hinzufuegen\n  KW = isoweek(Datum),\n  KW = factor(KW),\n  # monat und Jahr\n  Monat = month(Datum),\n  Monat = factor(Monat),\n  Jahr = year(Datum),\n  Jahr = factor(Jahr))\n\ndepo_d &lt;- depo_d |&gt;\n  mutate(Phase = case_when(\n    Datum &lt; lock_1_start ~ \"Pre\",\n    Datum &gt;= lock_1_start & Datum &lt;= lock_1_end ~ \"Lockdown_1\",\n    Datum &gt; lock_1_end & Datum &lt; lock_2_start ~ \"Inter\",\n    Datum &gt;= lock_2_start & Datum &lt;= lock_2_end ~ \"Lockdown_2\",\n    Datum &gt; lock_2_end ~ \"Post\"\n  ))\n\ndepo_d &lt;- depo_d |&gt;\n  mutate(Phase = base::factor(Phase, levels = c(\"Pre\", \"Lockdown_1\", \"Inter\", \"Lockdown_2\", \"Post\")))\n\nfor (i in 1:nrow(schulferien)) {\n  depo_d$Ferien[depo_d$Datum &gt;= schulferien[i, \"Start\"] & depo_d$Datum &lt;= schulferien[i, \"Ende\"]] &lt;- 1\n}\ndepo_d$Ferien[is.na(depo_d$Ferien)] &lt;- 0\n\ndepo_d$Ferien &lt;- factor(depo_d$Ferien)\n\n# pruefe das df\nhead(depo_d)\n\n\n\nErstellt nun. ähnlich wie oben, einen Datensatz depo_daytime, in welchem ihr gruppiert nach:\n\n\nJahr\nMonat\nKalenderwoche\nPhase\nFerien\nWochenende oder Werktag\nTageszeit\n\n\n\nMusterlösung\ndepo_daytime &lt;- depo |&gt;\n  group_by(Jahr, Monat, KW, Phase, Ferien, Wochenende, Tageszeit) |&gt;\n  summarise(\n    Total = sum(Fuss_IN + Fuss_OUT),\n    Fuss_IN = sum(Fuss_IN),\n    Fuss_OUT = sum(Fuss_OUT))\n\n\n\nWeiter benötigen wir für das Aufzeigen der Verteilung der Besuchenden über den Tag die durchschnittliche Besucheranzahl pro Stunde (mean), aufgeteilt nach Tageszeit und Phase (group_by Tageszeit, Phase). Speichert das unter “mean_phase_d”.\n\n\n\nMusterlösung\nmean_phase_d &lt;- depo_daytime |&gt;\n  group_by(Phase, Tageszeit) |&gt;\n  summarise(\n    Total = mean(Total),\n    IN = mean(Fuss_IN),\n    OUT = mean(Fuss_OUT))\n\n\n\n\n4b)\n\nAggregiere die Stundenwerte nach dem Monat (group_by Monat, Jahr). Nun brauchen wir nur noch das Total, keine Richtungstrennung mehr. Speichert das neue df unter depo_m (“_m” für “Monat”).\n\nTipp: Braucht wiederum group_by() und summarise().\n\n\nMusterlösung\ndepo_m &lt;- depo |&gt;\n    group_by(Jahr, Monat) |&gt;\n    summarise(Total = sum(Total))\n\n\n\nFügt dem neu erstellten df depo_m eine Spalte mit Jahr + Monat hinzu.\n\nHier der fertige Code dazu (da etwas umständlich):\n\n\nMusterlösung\ndepo_m &lt;- depo_m |&gt;\n    mutate(\n        Ym = paste(Jahr, Monat), # und mache eine neue Spalte, in der Jahr und Monat sind\n        Ym = lubridate::ym(Ym)\n    ) # formatiere als Datum\n\n\n\nWiederholt diesen Schritt, diesmal aber mit der Gruppierung “Tageszeit” neben “Jahr” und “Monat” (wiederum sollen Jahr und Monat auch in einer Spalte stehen).\nSpeichert das Resultat unter “depo_m_daytime”.\n\n\n\nMusterlösung\n# Gruppiere die Werte nach Monat und TAGESZEIT\ndepo_m_daytime &lt;- depo |&gt;\n    group_by(Jahr, Monat, Tageszeit) |&gt;\n    summarise(Total = sum(Total))\n# sortiere das df aufsteigend (nur das es sicher stimmt)\n\ndepo_m_daytime &lt;- depo_m_daytime |&gt;\n    mutate(\n        Ym = paste(Jahr, Monat), # und mache eine neue Spalte, in der Jahr und Monat sind\n        Ym = lubridate::ym(Ym)\n    ) # formatiere als Datum\n\n\n\n\n4c)\nMacht euch mit den Daten vertraut. Plottet sie, seht euch die df’s an, versteht, was sie repräsentieren.\nZ.B. sind folgende Befehle und Plots wichtig:\n\nstr()\nsummarize()\nhead()\nScatterplot, x = Datum, y = Anzahl pro Zeiteinheit\nHistrogram\nusw.\n\nHinweis: Geht noch nicht zu weit mit euren Plots. Die Idee ist, dass man sich einen Überblick über die Daten verschafft und noch keine “analysierenden” Plots erstellt.\nNachdem nun alle Daten vorbereitet sind folgt im nächsten Schritt die deskriptive Analyse.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html",
    "title": "Deskriptive Analysen",
    "section": "",
    "text": "Aufgabe 1: Verlauf der Besuchszahlen nach Monat",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-1-verlauf-der-besuchszahlen-nach-monat",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-1-verlauf-der-besuchszahlen-nach-monat",
    "title": "Deskriptive Analysen",
    "section": "",
    "text": "1a)\nNachdem wir die Projektstruktur aufgebaut haben und die Daten vorbereitet (inkl. aggregiert) sind, machen wir uns an die deskriptive Analyse. Dies macht immer Sinn. Bevor mach sich an die schliessende Statistik macht, muss man ein “Gefühl” für die Daten bekommen. Dies funktioniert am einfachsten mit explorativen Analysen.\nWir interessieren uns in den Analysen für 5 Zeitabschnitte:\n\nvon Anfang Untersuchungsperiode bis vor dem 1. Lockdown (pre)\nLockdown 1\nZwischen beiden Lockdowns\nLockdown 2\nEnde 2. Lockdown bis Ende Untersuchungsperiode\n\nUnd dabei immer auch für die verschiedenen Tageszeiten (Morgen, Tag, Abend, Nacht).\n\nPlottet den Verlauf der monatlichen Besuchszahlen an eurer Zählstelle.\nAuf der x-Achse steht dabei dabei Jahr und Monat (gespeichert im df depo_m), auf der y-Achse die monatlichen Besuchszahlen.\nZeichnet auch die beiden Lockdown ein (Hinweis: rundet das Start- und Enddatum der Lockdowns auf den Monat, da im Plot die monatlichen Zahlen gezeigt werden).\n\nHaltet euch dabei an untenstehenden Plot:\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\nggplot(depo_m, mapping = aes(Ym, Total, group = 1)) + # group = 1 braucht R, dass aus den Einzelpunkten ein Zusammenhang hergestellt wird\n  # zeichne Lockdown 1\n  geom_rect(\n    mapping = aes(\n      xmin = ym(\"2020-3\"), xmax = ym(\"2020-5\"),\n      ymin = 0, ymax = max(Total + (Total / 100 * 10))),\n    fill = \"lightskyblue\", alpha = 0.2, colour = NA) +\n  # zeichne Lockdown 2\n  geom_rect(\n    mapping = aes(\n      xmin = ym(\"2020-12\"), xmax = ym(\"2021-3\"),\n      ymin = 0, ymax = max(Total + (Total / 100 * 10))),\n    fill = \"darkolivegreen2\", alpha = 0.2, colour = NA) +\n  geom_line(alpha = 0.6, linewidth = 1) +\n  scale_x_date(date_labels = \"%b%y\", date_breaks = \"6 months\") +\n  labs(title = \"\", y = \"Fussgänger:innen pro Monat\", x = \"Jahr\") +\n  theme_classic(base_size = 15) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\n\nMusterlösung\nggplot(data = depo_m, mapping = aes(Ym, Total, group = 1)) + # group 1 braucht R, dass aus den Einzelpunkten ein Zusammenhang hergestellt wird\n    # zeichne Lockdown 1; ein einfaches Rechteck. bestimme mit min und max die Dimensionen\n  geom_rect(\n    mapping = aes(\n      xmin = ym(\"2020-3\"), xmax = ym(\"2020-5\"),\n      ymin = 0, ymax = max(Total + (Total / 100 * 10))\n    ), # das Rechteck soll 10 % grösser als die maximale Besuchszahl sein\n    fill = \"lightskyblue\", alpha = 0.2, colour = NA\n  ) +\n  # zeichne Lockdown 2\n  ... +\n  # zeichne die Linie\n  geom_line(...) +\n  scale_x_date(...)+\n  theme_linedraw(base_size = 15) +\n  ...\n\n\n\nExportiert euren Plot mit ggsave() nach results. Breite = 20, Höhe = 10, Einheiten = cm, dpi = 1000\n\n\n\n1b)\nDer erste Plot zeigt, wie sich die Besuchszahlen allgemein entwickelt haben. Interessant ist aber auch, wie sie während den einzelnen Monaten zueinander stehen (z.B. “Waren im Mai 2020 mehr Menschen unterwegs als im Mai 2017?”). Dies zeigt folgender Plot:\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\nggplot(depo_m, aes(Monat, Total, group = Jahr, color = Jahr, linetype = Jahr)) +\n  geom_line(size = 2) +\n  geom_point() +\n  scale_colour_viridis_d() +\n  scale_linetype_manual(values = c(rep(\"solid\", 3), \"twodash\", \"twodash\", \"solid\", \"solid\")) +\n  scale_x_discrete(breaks = c(seq(0, 12, by = 1))) +\n  geom_vline(xintercept = c(seq(1, 12, by = 1)), linetype = \"dashed\", color = \"gray\") +\n  labs(title = \"\", y = \"Fussgänger:innen pro Monat\", x = \"Monat\") +\n  theme_classic(base_size = 15) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\nErstellt diesen Plot und speichert ihn.\n\nHinweis: Dieser Plot wird erstellt mit dem Argument group, linetype und color innerhalb des _aes()__ Arguments. geom_line() wiederum zeichnet die Linien.\n\n\n1c)\nUnser Fokus liegt auf der dunklen Tageszeit. Wie verteilen sich die Besuchenden also auf Morgen, Tag, Abend und Nacht?\n\nErstellt und speichert folgenden Plot:\n\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\nggplot(depo_m_daytime, aes(Ym, Total, fill = Tageszeit)) +\n  geom_area(position = \"fill\", alpha = 0.8) +\n  scale_fill_manual(values = mycolors) +\n  scale_x_date(date_labels = \"%b%y\", date_breaks = \"6 months\", \n               limits = c(min(depo_m_daytime$Ym), max = max(depo_m_daytime$Ym)), expand = c(0, 0)) +\n  geom_vline(xintercept = seq(as.Date(min(depo_m_daytime$Ym)), as.Date(max(depo_m_daytime$Ym)), \n                              by = \"6 months\"), linetype = \"dashed\", color = \"black\")+\n  theme_classic(base_size = 15) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), \n        legend.position = \"bottom\") +\n  labs(title = \"\", y = \"Verteilung Fussgänger:innen / Monat [%]\", x = \"Jahr\")\n\n\nDiese Codeschnipsel helfen euch:\n\n\nMusterlösung\nggplot(depo_m_daytime, aes(Ym, Total, fill = Tageszeit)) +\n  geom_area(...) +\n  scale_x_date(date_labels = \"%b%y\", date_breaks = \"6 months\", \n               limits = c(min(depo_m_daytime$Ym), max = max(depo_m_daytime$Ym)), expand = c(0, 0)) +\n  geom_vline(xintercept = seq(as.Date(min(depo_m_daytime$Ym)), as.Date(max(depo_m_daytime$Ym)), \n                              by = \"6 months\"), linetype = \"dashed\", color = \"black\")+\n  ...",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-2-wochengang",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-2-wochengang",
    "title": "Deskriptive Analysen",
    "section": "Aufgabe 2: Wochengang",
    "text": "Aufgabe 2: Wochengang\nNun möchten wir genauer untersuchen, wie sich die Besuchszahlen je nach Phase (Pre, Lockdown 1, Inter, Lockdown 2 und Post) auf die Wochentage und Tageszeiten verteilen.\n\n2a)\n\nErstellt dazu einen Violinplot nach untenstehender Vorgabe. Was sagt uns dieser komplexe Plot?\n\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\nggplot(data = depo, aes(x = Wochentag, y = Total, fill = Tageszeit)) +\n  geom_violin(alpha = 0.5) +\n  labs(title = \"\", y = \"Fussgänger:innen pro Tag [log10]\") +\n  facet_grid(cols = vars(Tageszeit), rows = vars(Phase))+\n  scale_y_log10()+\n  scale_fill_manual(values = mycolors) +\n  theme_classic(base_size = 15) +\n  theme(\n    panel.background = element_rect(fill = NA, color = \"black\"),\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    legend.title = element_blank(), \n    legend.position = \"none\")\n\n\nHinweis:\n\nNutzt zum plotten ggplot()\nfolgende Codeschnipsel helfen euch:\n\n\n\nMusterlösung\nggplot(data = depo, aes(x = Wochentag, y = Total, fill = Tageszeit)) +\n  geom_violin() +\n  facet_grid(cols = vars(...), rows = vars(...))+\n  scale_y_log10()+ \n  ...\n\n\n\nWarum macht es Sinn, hier die y-Achse zu logarithmieren?\nGibt es alternative Darstellungsformen, welche besser geeignet wären?\nExportiert auch diesen Plot mit ggsave(). Welche Breite und Höhe passt hier?",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-3-tagesgang",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-3-tagesgang",
    "title": "Deskriptive Analysen",
    "section": "Aufgabe 3: Tagesgang",
    "text": "Aufgabe 3: Tagesgang\nVon der Übersicht ins Detail. Jetzt widmen wir uns dem Tagesgang, das heisst der Verteilung der Besuchenden auf die 24 Tagesstunden je nach Phase.\n\n3a)\n\nBerechnet zuerst den Mittelwert der totalen Besuchszahlen pro Wochentag, pro Stunde pro Phase und speichert das df unter Mean_h.\n\n\n\nMusterlösung\nMean_h &lt;- depo |&gt;\n  group_by(Wochentag, Stunde, Phase) |&gt;\n  summarise(Total = mean(Total))\n\n\nVergewissert euch vor dem Plotten, dass der Datensatz im long-Format vorliegt.\n\n\n3b)\n\nPlottet den Tagesgang, unterteilt nach den 7 Wochentagen nun für die verschiedenen Phasen.\n\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\nggplot(Mean_h, aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag)) +\n  geom_line(size = 1) +\n  scale_colour_viridis_d() +\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\")) +\n  scale_x_continuous(breaks = c(seq(0, 23, by = 1)), labels = c(seq(0, 23, by = 1))) +\n  facet_grid(rows = vars(Phase)) +\n  labs(x = \"Uhrzeit [h]\", y = \"Durchscnnitt Fussganger_Innen / h\", title = \"\") +\n  lims(y = c(0, 25)) +\n  theme_linedraw(base_size = 15) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\n\nMusterlösung\nggplot(Mean_h, aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag)) +\n  geom_line(...) +\n  facet_grid(...)\n...",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-4-kennzahlen",
    "href": "fallstudie_s/6_Deskriptive_Analysen.html#aufgabe-4-kennzahlen",
    "title": "Deskriptive Analysen",
    "section": "Aufgabe 4: Kennzahlen",
    "text": "Aufgabe 4: Kennzahlen\nBis hier hin haben wir in diesem Kapitel v.a. visuell gearbeitet. Für den Bericht kann es aber sinnvoll sein, auch einige Kennzahlen in der Hinterhand zu haben. Wir haben das bereits im Kapitel [Datenverarbeitung] berechnet.\n\nReflektiert, welche Zahlen ihr habt und was für den Bericht spannend sein könnte, resp. eure Abbildungen unterstützen oder ergänzen.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Deskriptive Analysen</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html",
    "href": "fallstudie_s/7_Multivariate_Modelle.html",
    "title": "Multivariate Modelle",
    "section": "",
    "text": "Aufgabe 1: Verbinden von Daten\nNachdem die deskriptiven Resultate vorliegen, kann jetzt die Berechnung eines multivariaten Modells angegangen werden.\nDas Ziel ist es (siehe dazu auch [Aufgabenstellung Abschlussbericht]), - den Zusammenhang zwischen der gesamten Anzahl Besucher:innen (Total; entweder Fussgänger:innen ODER Fahrräder, je nach dem für was ihr euch entschieden habt) - und verschiedenen erklärenden Variablen (Wochentag, Ferien, Monat, Jahr, Phasen der Covid-Pandemie, Sonnenscheindauer, Höchsttemperatur, Niederschlagssumme) aufzuzeigen.\nAktuell haben wir noch zwei verschiedene Datensätze von Interesse:",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-1-verbinden-von-daten",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-1-verbinden-von-daten",
    "title": "Multivariate Modelle",
    "section": "",
    "text": "Einen mit den Besuchszahlen pro Tag von Besucher:innen mit den dazugehörigen Convenience Variablen (Datensatz “depo_d” - zu Tagen aggregierte Stunden und Convenience Variablen )\nund einen mit den Wetterparametern pro Tag (“meteo”).\n\n\nDiese beiden Datensätze müssen miteinander verbunden werden. Ziel: Ein Datensatz mit den Zähldaten und Convenience Variablen wie Phase Covid, Ferien Ja oder Nein, Jahr, Monat, KW, Wochenendtag oder Werktag, angereichert mit Meteodaten. Welcher join-Befehl eignet sich dazu?\nDer neue Datensatz soll ” umwelt ” heissen.\n\n\nSind durch das Zusammenführen NA’s entstanden? Falls ja, müssen alle für die weiteren Auswertungen ausgeschlossen werden.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-2-skalieren",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-2-skalieren",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 2: Skalieren",
    "text": "Aufgabe 2: Skalieren\n\n2a)\n\nVergewissert euch, dass die numerischen Messwerte zu den Meteodaten auch in numerischer Form vorliegen. (is.numeric())\nNachfolgende Schritte funktionieren nur, wenn umwelt als data.frame vorliegt! Prüft das und ändert das, falls noch kein data.frame (Hinweis: auch ein “tibble” funktioniert nicht, obwohl bei der Abfrage is.data.frame() TRUE angegeben wird. Damit ihr beim scalen keine NaN Werte erhaltet, wendet ihr darum am besten sowieso zuerst den Befehl as.data.frame() auf umwelt an).\n\n\nUnser Modell kann in der abhängigen Variabel nur mit Ganzzahlen (Integer) umgehen. Daher müssen Kommazahlen in Integer umgewandelt werden. Zum Glück haben wir das schon gemacht in der Datenvorverarbeitung (Aufgabe 3c) und uns bleibt nichts weiter zu tun.\n\n\n\n2b)\n\nProblem: verschiedene Skalen der Variablen (z.B. Temperatur in Grad Celsius, Niederschlag in Millimeter und Sonnenscheindauer in %)\nLösung: Skalieren aller Variablen mit Masseinheiten gemäss unterstehendem Code:\n\n\n\nMusterlösung\numwelt &lt;- umwelt |&gt; \n  mutate(tre200jx_scaled = scale(tre200jx)|&gt;\n  ...",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-3-korrelationen-und-variablenselektion",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-3-korrelationen-und-variablenselektion",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 3: Korrelationen und Variablenselektion",
    "text": "Aufgabe 3: Korrelationen und Variablenselektion\n\n3a)\nKorrelierende Variablen können das Modellergebnis verfälschen. Daher muss vor der Modelldefinition auf Korrelation zwischen den Messwerten getestet werden. Welches sind die erklärenden Variablen, welches ist die Abhängige? (Ihr müsst nicht prüfen, ob die Voraussetzungen zur Berechnung von Korrelationen erfüllt sind)\n\nTeste mittels folgendem Code auf eine Korrelation zwischen den Messwerten.\n\n\n\nMusterlösung\ncor &lt;- cor(subset(umwelt, select = c(ERSTE SPALTE MIT ERKLAERENDEN MESSWERTEN : \n                     LETZTE SPALTE MIT ERKLAERENDEN MESSWERTEN)))\n\n\n\n\n3b)\nMit dem folgenden Code kann eine Korrelationsmatrix (mit den Messwerten) aufgebaut werden. Hier kann auch die Schwelle für die Korrelation gesetzt werden (0.7 ist liberal / 0.5 konservativ).\n\n\nMusterlösung\ncor[abs(cor) &lt; 0.7] &lt;- 0 # Setzt alle Werte kleiner 0.7 auf 0\n\n\nZur Visualisierung kann ein Plot erstellt werden.\n\n\nMusterlösung\nchart.Correlation(subset(umwelt, select = c(ERSTE SPALTE MIT ERKLAERENDEN MESSWERTEN : \n                     LETZTE SPALTE MIT ERKLAERENDEN MESSWERTEN)), \n                  histogram = TRUE, pch = 19)\n\n\nWo kann eine kritische Korrelation beobachtet werden? Kann man es verantworten, trotzdem alle (Wetter)parameter in das Modell zu geben?\nFalls ja: warum? Falls nein: schliesst den betreffenden Parameter aus. Wenn ihr Parameter ausschliesst: welchen der beiden korrelierenden Parameter behaltet ihr im Modell?",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-4-optional-automatische-variablenselektion",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-4-optional-automatische-variablenselektion",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 4 (OPTIONAL): Automatische Variablenselektion",
    "text": "Aufgabe 4 (OPTIONAL): Automatische Variablenselektion\nFühre die dredge-Funktion und ein Modelaveraging durch. Der Code dazu ist unten. Was passiert in der Funktion? Macht es Sinn, die Funktion auszuführen?\nHinweis: untenstehender Code ist sehr rechenentensiv.\n\n\nMusterlösung\n# Hier wird die Formel für die dredge-Funktion vorbereitet\nf &lt;- Total ~ Wochentag + Ferien + Phase + Monat +\n    tre200jx_scaled + rre150j0_scaled + rre150n0_scaled +\n    sremaxdv_scaled\n# Jetzt kommt der Random-Factor hinzu und es wird eine Formel daraus gemacht\nf_dredge &lt;- paste(c(f, \"+ (1|Jahr)\"), collapse = \" \") |&gt;\n    as.formula()\n# Das Modell mit dieser Formel ausführen\nm &lt;- glmer.nb(f_dredge, data = umwelt, na.action = \"na.fail\")\n# Das Modell in die dredge-Funktion einfügen (siehe auch ?dredge)\nall_m &lt;- dredge(m)\n# suche das beste Modell\nprint(all_m)\n# Importance values der Variablen\n# hier wird die wichtigkeit der Variablen in den verschiedenen Modellen abgelesen\nMuMIn::sw(all_m)\n\n# Schliesslich wird ein Modelaverage durchgeführt\n# Schwellenwert für das delta-AIC = 2\navgmodel &lt;- model.avg(all_m, rank = \"AICc\", subset = delta &lt; 2)\nsummary(avgmodel)",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-5-verteilung-der-abhängigen-variabel-pruefen",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-5-verteilung-der-abhängigen-variabel-pruefen",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 5: Verteilung der abhängigen Variabel pruefen",
    "text": "Aufgabe 5: Verteilung der abhängigen Variabel pruefen\nDie Verteilung der abhängigen Variabel bestimmt, was für ein Modell geschrieben werden kann. Alle Modelle gehen von einer bestimmten gegebenen Verteilung aus. Wenn diese Annahme verletzt wird, kann es sein, dass das Modellergebnis nicht valide ist.\nUntenstehender Codeblock zeigt, wie unsere Daten auf verschiedene Verteilungen passen.\nHinweis: es kann sein, dass nicht jede Verteilung geplottet werden kann, es erscheint eine Fehlermeldung. Das ist nicht weiter schlimm, die betreffende Verteilung kann gelöscht werden. Analog muss das auch im Befehl gofstat() passieren.\n\nDie besten drei Verteilungen (gemäss AIC) sollen zur Visualisierung geplottet werden. Dabei gilt, je besser die schwarze Punktlinie (eure Daten) auf die farbigen Linien (theoretische Verteilungen) passen, desto besser ist diese Verteilung geeignet.\n\nHinweis: CDF = Cumulative distribution function; Wikipedia = “Anschaulich entspricht dabei der Wert der Verteilungsfunktion an der Stelle x der Wahrscheinlichkeit, dass die zugehörige Zufallsvariable X einen Wert kleiner oder gleich x annimmt.” Ihr müsst aber nicht weiter verstehen, wie das berechnet wird, wichtig für euch ist, dass ihr den Plot interpretieren könnt.\n\n\nMusterlösung\nf1 &lt;- fitdist(umwelt$Total, \"norm\") # Normalverteilung\nf1_1 &lt;- fitdist((umwelt$Total + 1), \"lnorm\") # log-Normalvert (beachte, dass ich +1 rechne.\n# log muss positiv sein; allerdings kann man die\n# Verteilungen dann nicht mehr miteinander vergleichen).\nf2 &lt;- fitdist(umwelt$Total, \"pois\") # Poisson\nf3 &lt;- fitdist(umwelt$Total, \"nbinom\") # negativ binomial\nf4 &lt;- fitdist(umwelt$Total, \"exp\") # exponentiell\nf5&lt;-fitdist(umwelt$Total,\"gamma\")  # gamma (berechnung mit meinen Daten nicht möglich)\nf6 &lt;- fitdist(umwelt$Total, \"logis\") # logistisch\nf7 &lt;- fitdist(umwelt$Total, \"geom\") # geometrisch\nf8&lt;-fitdist(umwelt$Total,\"weibull\")  # Weibull (berechnung mit meinen Daten nicht möglich)\n\ngofstat(list(f1, f2, f3, f4, f6, f7),\n  fitnames = c(\n    \"Normalverteilung\", \"Poisson\",\n    \"negativ binomial\", \"exponentiell\", \"logistisch\",\n    \"geometrisch\"))\n\n# die 2 besten (gemaess Akaike's Information Criterion) als Plot + normalverteilt,\nplot.legend &lt;- c(\"Normalverteilung\", \"exponentiell\", \"negativ binomial\")\n# vergleicht mehrere theoretische Verteilungen mit den empirischen Daten\ncdfcomp(list(f1, f4, f3), legendtext = plot.legend)\n\n\n\n\n\n\n\n\n\n\n\n\nWie sind unsere Daten verteilt? Welche Modelle können wir anwenden?",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-6-multivariates-modell-berechnen",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-6-multivariates-modell-berechnen",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 6: Multivariates Modell berechnen",
    "text": "Aufgabe 6: Multivariates Modell berechnen\nJetzt geht’s ans Eingemachte!\nIch verwende hauptsächlich die Funktion glmmTMB(). Es ist wahnsinnig schnell und erlaubt viele Spezifikationen: Link\nAuch glmer() aus der Bibliothek lme4 ist recht neu und praktisch (diese Bibliothek wird auch in vielen wissenschaftlichen Papern im Feld Biologie / Wildtiermamagement zitiert). Link\n\n6a) Modelle berechnen\nHinweis: Auch wenn wir gerade herausgefunden haben, dass die Verteilung negativ binominal (in meinem Fall) ist, berechne ich für den Vergleich zuerst ein “einfaches Modell” der Familie poisson. Alternative Modelle rechnen wir in später.\n\nDie Totale Besucheranzahl pro Tag soll durch die abhängigen Variablen erklärt werden (Datensatz “umwelt”). Die Tage bis Neujahr sollen hierbei nicht beachtet werden, sie werden als “random factor” bestimmt.\n\nFrage: Warum bestimmen wir die Tage bis Neujahr als random factor?\nDie Modellformel lautet:\n\n\nMusterlösung\npoisson_model &lt;- glmer(Total ~ Monat + Ferien + Phase + Wochenende +\n                       tre200jx_scaled + rre150j0_scaled + rre150n0_scaled +\n                         sremaxdv_scaled +\n                         (1 | Tage_bis_Neujahr), family = poisson, data = umwelt)\n\nsummary(poisson_model) # zeigt das Ergebins des Modells\n\n\nFrage: Was bedeutet “family = poisson”?\nLöst zuerst Aufgabe 6b bevor ihr alternative (besser passende) Modelle rechnet; das kommt in Aufgabe 6c!\n\n\n6b) Modelldiagnostik\n\nPrüft ob euer Modell valide ist, mit dem Package DHARMa: Link\n\nBitte unbedingt diese obenstehende Vignette des DHARMa-Package konsultieren!\nHinweis: Wir verwenden etwas andere Funktionen als in der Vorlesung am morgen. Sie sind unten aufgeführt, und die Funktionen analog zu den Funktionen aus der Vorlesung, aber halt etwas anders.\n\n\nMusterlösung\n# Residuals werden über eine Simulation auf eine Standard-Skala transformiert und\n# können anschliessend getestet werden. Dabei kann die Anzahl Simulationen eingestellt\n# werden (dauert je nach dem sehr lange)\n\n# wenn faktoren drin sind, dann gibt Anova einen einfachen überblick, welche faktoren signifikant sind\ncar::Anova(poisson_model)\n\n# dann kommt dharma\nsimulationOutput &lt;- simulateResiduals(fittedModel = poisson_model, n = 1000)\n\n# plotting and testing scaled residuals\n\nplot(simulationOutput)\n\ntestResiduals(simulationOutput)\n\ntestUniformity(simulationOutput)\n\n# The most common concern for GLMMs is overdispersion, underdispersion and\n# zero-inflation.\n\n# separate test for dispersion\n\ntestDispersion(simulationOutput)\n\n# test for Zeroinflation\n\ntestZeroInflation(simulationOutput)\n\n# Testen auf Multicollinearität (dh zu starke Korrelationen im finalen Modell, zB falls\n# auf Grund der ökologischen Plausibilität stark korrelierte Variablen im Modell)\n# use VIF values: if values less then 5 is ok (sometimes &gt; 10), if mean of VIF values\n# not substantially greater than 1 (say 5), no need to worry.\n\ncar::vif(poisson_model) # funktioniert nicht mit glmmTMB\nmean(car::vif(poisson_model))\n\n# erklaerte varianz\n# The marginal R squared values are those associated with your fixed effects,\n# the conditional ones are those of your fixed effects plus the random effects.\n# Usually we will be interested in the marginal effects.\nperformance::r2(poisson_model)\n\n\nSind die Voraussetzungen des Modells erfüllt?\n\n\n6c) Alternative Modelle\nWir sind auf der Suche nach dem minimalen adäquaten Modell. Das ist ein iterativer Prozess. Wir schreiben ein Modell, prüfen ob die Voraussetzungen erfüllt sind und ob die abhängige Variable besser erklärt wird als im Vorhergehenden. Und machen das nochmals und nochmals und nochmals…\n\nglmmTMB() ist eine sehr schnelle und kompatible Funktion, auch für negativ binomiale Daten. Ich empfehle (spätestens ab dem exponierten Modell weiter unten) mit ihr zu arbeiten.\nUnsere (meine) Daten sind negativ binominal verteilt. Daher sollte wir unbedingt ein solches Modell programmieren. –&gt; Funktion glmer.nb()\n\n\n\nMusterlösung\nnb_model &lt;- glmmTMB(Total ~ Monat + Ferien + ..., \n                        family =nbinom1,\n                        data = umwelt)\n\n\n\nschliesst im Modell Variablen aus, welche nicht signifikant sind. In euren “besten” Modellen sollen nur signifikante Variablen verbleiben.\nÜber family = kann in der Funktion _glmer()__ einiges (aber leider nicht alles so einfach [z.B. negativ binominale Modelle]) angepasst werden: Link\nAuch über link = kann man in _glmer()__ anpassen: Link\nFalls die Daten exponentiell Verteilt sind, hier der Link zu einem Blogeintrag dazu: Link\n\n\n\nMusterlösung\nglmmTMB((Total + 1) ~ ... \n                         family = Gamma(link = \"log\"), data = umwelt_night)\n\n\n\nHypothese: “Es gehen weniger Leute in den Wald, wenn es zu heiss ist” –&gt; auf quadratischen Term Temperatur testen! Hinweis: ich welchsel hier auf glmmTBM, da diese funktion beudeutend schneller ist und das Ergeniss besser wird (in meinem Fall).\n\n\n\nMusterlösung\nnb_quad_model &lt;- glmmTMB(Total ~ Monat + Ferien + Phase + Wochenende +\n                               tre200jx_scaled + I(tre200jx_scaled^2) + # hier ist der quadratische Term\n                               rre150j0_scaled +  sremaxdv_scaled +\n                               (1 | Tage_bis_Neujahr), \n                             family =nbinom1, # es ist ein negativ binomiales Modell\n                               data = umwelt)\n\n\n\nKönnte es zwischen einzelnen Variablen zu Interaktionen kommen, die plausible sind? (z. B.: Im Winter hat Niederschlag einen negativeren Effekt als im Sommer, wenn es heiss ist) –&gt; Falls ja: testen!\n\nHinweis: Interaktionen berechnen ist sehr rechenintensiv. Auch die Interpretation der Resultate wird nicht unbedingt einfacher. Wenn ihr auf Interaktionen testet, dann geht “langsam” vor, probiert nicht zu viel auf einmal und verwendet glmmTMB.\n\n\nMusterlösung\n  ...\nMonat * rre150j0_scaled +\n  ...\n\n\n\nHabt ihr ein Problem mit zeroinflation? (Dies wisst ihr aus dem Test testZeroInflation() und testResiduals())\n\n\n\nMusterlösung\nnb_model_zi &lt;- glmmTMB(..., \n                           # The basic glmmTMB fit — a zero-inflated Poisson model with a single zero-\n                           # inflation parameter applying to all observations (ziformula~1)\n                           ziformula=~1,\n                           family = nbinom2) # family nbinom1 oder nbinom2\n\n\n\nWenn ihr verschiedene Modelle gerechnet habt, können diese über den AICc verglichen werden. Folgender Code kann dazu genutzt werden:\n\nHinweis: Nur Modelle mit demselben Datensatz können miteinander verglichen werden. D.h., dass die Modelle mit den originalen Daten nicht mit logarithmiertem oder exponierten Daten verglichen werden können und glmer kann nicht mit glmmTMB verglichen werden. –&gt; Untenstehende Funktion hat für uns also einen eingeschränkten Wert…\n\n\n6d) (OPTIONAL) Transformationen\nBei meinen Daten waren die Modellvoraussetzungen überall mehr oder weniger verletzt. Das ist ein Problem, allerdings auch nicht ein so grosses (man sollte es aber trotzdem ernst nehmen). Mehr dazu unter:\nSchielzeth u. a. (2020) - Robustness of linear mixed‐effects models to violations of distributional assumptions. Link\nLo und Andrews (2015) - To transform or not to transform: using generalized linear mixed models to analyse reaction time data Link\nFalls die Voraussetzungen stark verletzt werden, wäre eine Transformation angezeigt. Mehr dazu unter: Link\n\nWenn ihr das machen wollt, berechnet zuerst den skewness coefficient\n\n\n\nMusterlösung\nlibrary(\"moments\")\nskewness(umwelt$Anzahl_Total)\n## A positive value means the distribution is positively skewed (rechtsschief).\n## The most frequent values are low; tail is toward the high values (on the right-hand side)\n\n\n\nWelche Transformation kann angewandt werden?\nWas spricht gegen eine Transformation (auch im Hinblick zur Visualisierung und Interpretation)? Was spricht dafür?\n\n\n\n6d) Exportiere die Modellresultate (der besten Modelle)\nWelches ist euer bestes Modell? Meines ist jenes ohne Interaktionen und zero-inflated-adjusted.\nModellresultate können mit summary() angezeigt werden. Ich verwende aber lieber die Funktion tab_model()! Die Resultate werden gerundet und praktisch im separaten Fenster angezeigt. Von dort kann man sie via copy + paste ins (z.B.) Word bringen.\n\n\nMusterlösung\ntab_model(MODELLNAME, \n          transform = NULL, # To plot the estimates on the linear scale, use transform = NULL.\n          show.se = TRUE) # zeige die Standardabweichung\n## The marginal R squared values are those associated with your fixed effects,\n## the conditional ones are those of your fixed effects plus the random effects.\n## Usually we will be interested in the marginal effects.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-7-modellvisualisierung",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#aufgabe-7-modellvisualisierung",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 7: Modellvisualisierung",
    "text": "Aufgabe 7: Modellvisualisierung\n\nVisualisiert die (signifikanten) Ergebnisse eures Modells. Sabrina Harsch hat im HS21 eine sehr nützliche Funktion dafür geschriben (welche ich etwas weiter ausgebaut habe). Es gibt für die kontinuierlichen Variablen und für die diskreten Variablen je eine separate Funktion.\n\n\n\nMusterlösung\n# schreibe fun fuer continuierliche var\nrescale_plot_num &lt;- function(input_df, input_term, unscaled_var, scaled_var, num_breaks, x_lab, y_lab, x_scaling, x_nk) {\n  plot_id &lt;- plot_model(input_df, type = \"pred\", terms = input_term, axis.title = \"\", title = \"\", color = \"orangered\")\n  labels &lt;- round(seq(floor(min(unscaled_var)), ceiling(max(unscaled_var)), length.out = num_breaks + 1) * x_scaling, x_nk)\n  \n  custom_breaks &lt;- seq(min(scaled_var), max(scaled_var), by = ((max(scaled_var) - min(scaled_var)) / num_breaks))\n  custom_limits &lt;- c(min(scaled_var), max(scaled_var))\n  \n  plot_id &lt;- plot_id +\n    scale_x_continuous(breaks = custom_breaks, limits = custom_limits, labels = c(labels), labs(x = x_lab)) +\n    scale_y_continuous(labs(y = y_lab), limits = c(0, 25)) +\n    theme_classic(base_size = 20)\n  \n  return(plot_id)\n}\n\n# schreibe fun fuer diskrete var\nrescale_plot_fac &lt;- function(input_df, input_term, unscaled_var, scaled_var, num_breaks, x_lab, y_lab, x_scaling, x_nk) {\n  plot_id &lt;- plot_model(input_df, type = \"pred\", terms = input_term, axis.title = \"\", title = \"\", color = \"orangered\")\n  \n  plot_id &lt;- plot_id +\n    scale_y_continuous(labs(y = y_lab), limits = c(0, 40)) +\n    theme_classic(base_size = 20) +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n  \n  return(plot_id)\n}\n\n\nNun können die einzelnen Variabeln aus den besten Modellen in der Funktion jeweils für die Plots angepasst werden:\n\n\nMusterlösung\n## Tagesmaximaltemperatur\ninput_df &lt;- nb_model_zi\ninput_term &lt;- \"tre200jx_scaled [all]\"\nunscaled_var &lt;- umwelt$tre200jx\nscaled_var &lt;- umwelt$tre200jx_scaled\nnum_breaks &lt;- 10\nx_lab &lt;- \"Temperatur [°C]\"\ny_lab &lt;- \"Fussgänger:innen pro Tag\"\nx_scaling &lt;- 1 # in prozent\nx_nk &lt;- 0 # x runde nachkommastellen\n\n\np_temp &lt;- rescale_plot_num(\n  input_df, input_term, unscaled_var, scaled_var, num_breaks,\n  x_lab, y_lab, x_scaling, x_nk\n)\np_temp\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\n## Wochentag \ninput_df &lt;- nb_model_zi\ninput_term &lt;- \"Wochenende [all]\"\nunscaled_var &lt;- umwelt$Wochenende\nscaled_var &lt;- umwelt$Wochenende\nnum_breaks &lt;- 10\nx_lab &lt;- \"Wochentag\"\ny_lab &lt;- \"Fussgänger:innen pro Tag\"\nx_scaling &lt;- 1 # in prozent\nx_nk &lt;- 0 # x runde nachkommastellen\n\n\np_wd &lt;- rescale_plot_fac(\n  input_df, input_term, unscaled_var, scaled_var, num_breaks,\n  x_lab, y_lab, x_scaling, x_nk)\np_wd\n\n\n\n\n\n\n\n\n\n\nExportiert die Ergebnisse via ggsave().\n\nHinweis: damit unsere Plots verglichen werden können, sollen sie alle dieselbe Skalierung (limits) auf der y-Achse haben. Das wird erreicht, indem man bei jedem Plot die limits in scale_y_continuous() gleichsetzt.\nHinweis: Es könnten auch interaction-plots erstellt werden: Link",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Modelle.html#abschluss",
    "href": "fallstudie_s/7_Multivariate_Modelle.html#abschluss",
    "title": "Multivariate Modelle",
    "section": "Abschluss",
    "text": "Abschluss\nNun habt ihr verschiedenste Ergebnisse vorliegen. In einem wissenschaftlichen Bericht sollen aber niemals alle Ergebnisse abgebildet werden. Eine Faustregel besagt, dass nur signifikante Ergebnisse visualisiert werden. Entscheidet euch daher, was ihr in eurem Bericht abbilden wollt und was lediglich besprochen werden soll.\nStellt im Bericht die Ergebnisse des Tages, der Dämmerung und der Nacht gegenüber und beschreibt die Gemeinsamkeiten und Unterschieden. Behaltet dabei immer die Forschungsfragen in Erinnerung.\n\n\n\n\nLo, Steson, und Sally Andrews. 2015. „To transform or not to transform: using generalized linear mixed models to analyse reaction time data“. Frontiers in Psychology 6. https://doi.org/10.3389/fpsyg.2015.01171.\n\n\nSchielzeth, Holger, Niels J Dingemanse, Shinichi Nakagawa, David F Westneat, Hassen Allegue, Céline Teplitsky, Denis Réale, Ned A Dochtermann, László Zsolt Garamszegi, und Yimen G Araya-Ajoy. 2020. „Robustness of linear mixed-effects models to violations of distributional assumptions“. Methods in ecology and evolution 11 (9): 1141–52.",
    "crumbs": [
      "Fallstudie S",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/1_Vorbemerkung.html",
    "href": "fallstudie_n/1_Vorbemerkung.html",
    "title": "Vorbemerkung",
    "section": "",
    "text": "Aktuell dient diese Plattform für die BiEc Fallstudie - Profil N einzig der Bereitstellung von Aufgaben die von euch im Rahmen dieses Fallstudienprojekts erarbeitet werden sollen. Die Aufgaben werden in den meisten Fällen mit Code-Beispielen erläutert oder benötigten Code-snippets resp. Funktionen werden mitgeliefert. Im Laufe des Semesters werden hier ausserdem häppchenweise (mögliche) Lösungen zu den Aufgaben aufgeschaltet. Alles grundlegende Material und alle Unterlagen zu den theoretischen Inputs sind weiterhin und ausschliesslich im Moodlekurs Research Methods - Fallstudie BiEc zu finden. Die für die Aufgaben benötigten Datengrundlagen sind ebenfalls im entsprechenden Abschnitt auf Moodle zu finden. Frohes Schaffen!\n\nIm Rahmen der Fallstudie werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog der Vorbereitungsübung in Prepro1 könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\nipak &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages &lt;- c(\n  \"adehabitatHR\", \"bbmle\", \"car\", \"cowplot\", \"DHARMa\", \"dplyr\",\n  \"ggeffects\", \"ggplot2\", \"ggspatial\", \"glmmTMB\", \"gstat\", \"kableExtra\", \"lme4\",\n  \"MASS\", \"MuMIn\", \"pastecs\", \"performance\", \"PerformanceAnalytics\", \"psych\",\n  \"readr\", \"rms\", \"ROCR\", \"sf\", \"sjPlot\", \"sjstats\", \"terra\"\n)\n\nipak(packages)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Vorbemerkung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html",
    "href": "fallstudie_n/2_Datenverarbeitung.html",
    "title": "Daten(vor)verarbeitung",
    "section": "",
    "text": "Projektaufbau RStudio-Projekte\nVor den eigentlichen Auswertungen müssen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach untereinander ausgetauscht und somit auch reproduziert werden können; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: Link)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufbau-von-r-skripten",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufbau-von-r-skripten",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufbau von R-Skripten",
    "text": "Aufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n# .##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS24. Autor/in ####\n# .##################################################################################\n\nBeschreibt zudem folgendes:\n\nOrdnerstruktur; ich verwende hier den Projektordner mit den Unterordnern:\n\nSkripts\nData\nResults\nPlots\n\nVerwendete Daten\n\nEin Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. Dieses Schema beinhaltet (nach dem bereits erwähnten Kopf des Skripts) 4 Kapitel:\n\nDatenimport\nDatenvorverarbeitung\nAnalyse\nVisualisierung\n\nBereitet euer Skript also nach dieser Struktur vor. Nutzt für den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n# .###################################################################################\n# METADATA ####\n# .###################################################################################\n# Datenherkunft ####\n# ...\n\n# .###################################################################################\n# 1. DATENIMPORT ####\n# .###################################################################################",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#libraries-laden",
    "href": "fallstudie_n/2_Datenverarbeitung.html#libraries-laden",
    "title": "Daten(vor)verarbeitung",
    "section": "Libraries laden",
    "text": "Libraries laden\n\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"purrr\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#daten-laden",
    "href": "fallstudie_n/2_Datenverarbeitung.html#daten-laden",
    "title": "Daten(vor)verarbeitung",
    "section": "Daten laden",
    "text": "Daten laden\nHerunterladen der Daten der Feldaufnahmen von Moodle (Aufgabe3_Feldaufnahmen_alle_Gruppen.zip), Einlesen, Sichtung der Datensätze und der Datentypen.\nVerschiedene Dinge funktionierten nicht auf Anhieb:\n\nDaten Gruppe 1: leere Zeilen zwischen jedem Dateneintrag (R ist intelligent genug dies beim Einlesen zu erkennen)\nDaten Gruppe 5: leere Zeilen zwischen jedem Dateneintrag (R ist intelligent genug dies beim Einlesen zu erkennen)\nDaten Gruppe 6:\n\nExcelfile –&gt; csv daraus machen\nKoordinaten fehlen, diese werden benötigt um die Daten über die Kreise eindeutig mit den LIDAR-Daten zusammenzuführen –&gt; einfügen aus Zuteilung_Kreise_Aufnahmen_Landforst_HS24.docx\nKreise als Datentyp character, müssen numeric sein\n\nDaten Gruppe 7: Koordinaten fehlen, diese werden benötigt um die Daten über die Kreise eindeutig mit den LIDAR-Daten zusammenzuführen –&gt; einfügen aus Zuteilung_Kreise_Aufnahmen_Landforst_HS24.docx\n\nVersucht wenn möglich solche Dinge jeweils direkt mit R zu lösen, dies ist vorallem bei grösseren Datensätzen extrem hilfreich, damit die Datensätze zu einem sauberen Gesamtdatensatz zusammengefügt werden können.\n\ndf_team1 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Aufgabe_2_Team1.csv\", delim = \";\") \n\ndf_team2 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/FelderhebungenSilhwaldKreise.csv\", delim = \",\")\n\ndf_team3 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebungen Waldstruktur Team 3 pink (Gruppe 4).csv\", delim = \";\")\n\ndf_team4 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebung_Gruppe 5.csv\", delim = \";\")\n\ndf_team5 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/TEAM 5 (violett) - Felderhebungen Waldstruktur.csv\", delim = \",\")\n\ndf_team6 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebung_Team6.csv\", \n                       delim = \";\", \n                       locale = locale(encoding = \"latin1\")) %&gt;%\n  mutate(Punkt = parse_number(Punkt)) \n\n\ndf_team7 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/gr7_ground_thruth_lidar.csv\", delim = \";\")\n\n\n# hier können die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der\n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_lidar &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_LIDAR_Waldstruktur_Reh_Kreise_241011.csv\", delim = \";\")\nstr(df_lidar)\n## spc_tbl_ [305 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Anz_reh_lokalisationen: num [1:305] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ x                     : num [1:305] 684900 684900 684900 684900 684875 ...\n##  $ y                     : num [1:305] 237100 237125 237150 237175 237075 ...\n##  $ DG_us_2014            : num [1:305] 0.0903 0.2717 0.468 0.7407 0.1811 ...\n##  $ DG_os_2014            : num [1:305] 0.908 0.959 0.871 0.986 0.86 ...\n##  $ DG_us_2022            : num [1:305] 0.269 0.823 0.936 0.359 0.245 ...\n##  $ DG_os_2022            : num [1:305] 0.945 0.99 0.953 0.997 0.898 ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   Anz_reh_lokalisationen = col_double(),\n##   ..   x = col_double(),\n##   ..   y = col_double(),\n##   ..   DG_us_2014 = col_double(),\n##   ..   DG_os_2014 = col_double(),\n##   ..   DG_us_2022 = col_double(),\n##   ..   DG_os_2022 = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n# Die eingelesenen Datensätze anschauen und versuchen zu einem Gesamtdatensatz\n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt &lt;- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5,df_team6,df_team7)\nstr(df_gesamt)\n## spc_tbl_ [175 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Kreis (r 12.5m)               : num [1:175] 0 1 2 3 4 5 6 7 8 9 ...\n##  $ X                             : num [1:175] 684900 684875 684875 684875 684850 ...\n##  $ Y                             : num [1:175] 237175 237125 237175 237250 237225 ...\n##  $ Deckungsgrad Rubus sp. [%]    : num [1:175] 20 20 15 55 17 10 15 10 20 65 ...\n##  $ DG Strauchschicht [%] (0.5-3m): num [1:175] 50 35 50 60 90 75 75 80 15 27 ...\n##  $ DG Baumschicht [%] (ab 3m)    : num [1:175] 30 40 65 70 60 70 60 45 63 40 ...\n##  $ Punkt                         : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Rubus sp.                     : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Strauchschicht                : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Baumschicht                   : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Kreis                         : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ DG_Rubus_sp                   : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ DG_Strauchschicht             : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  $ DG_Baumschicht                : num [1:175] NA NA NA NA NA NA NA NA NA NA ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   `Kreis (r 12.5m)` = col_double(),\n##   ..   X = col_double(),\n##   ..   Y = col_double(),\n##   ..   `Deckungsgrad Rubus sp. [%]` = col_double(),\n##   ..   `DG Strauchschicht [%] (0.5-3m)` = col_double(),\n##   ..   `DG Baumschicht [%] (ab 3m)` = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-1",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-1",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\n\n1.1 Einfügen zusätzliche Spalte pro Datensatz mit der Gruppenzugehörigkeit (Team1-7)\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensätzen gleich sind und der Gesamtdatensatz ohne Probleme zusammengefügt werden kann\n\n→ Befehle mutate und rename, mit pipes (alt: %&gt;%, neu: |&gt;) in einem Schritt möglich\n\n\n\n\nMusterlösung\n# .#################################################################################\n# 2. DATENVORVERARBEITUNG #####\n# .#################################################################################\n\ndf_team1 &lt;- df_team1 |&gt;\n  mutate(team = \"team1\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team2 &lt;- df_team2 |&gt;\n  mutate(team = \"team2\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team3 &lt;- df_team3 |&gt;\n  mutate(team = \"team3\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team4 &lt;- df_team4 |&gt;\n  mutate(team = \"team4\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team5 &lt;- df_team5 |&gt;\n  mutate(team = \"team5\") |&gt;\n  rename(\n    KreisID = \"Kreis (r 12.5m)\",\n    DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n    DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n    DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\"\n  )\n\ndf_team6 &lt;- df_team6 |&gt;\n  mutate(team = \"team6\") |&gt;\n  rename(\n    KreisID = \"Punkt\",\n    DG_Rubus = \"Rubus sp.\",\n    DG_Strauchschicht = \"Strauchschicht\",\n    DG_Baumschicht = \"Baumschicht\"\n  )\n\ndf_team7 &lt;- df_team7 |&gt;\n  mutate(team = \"team7\") |&gt;\n  rename(\n    KreisID = \"Kreis\",\n    DG_Rubus = \"DG_Rubus_sp\",\n    DG_Strauchschicht = \"DG_Strauchschicht\",\n    DG_Baumschicht = \"DG_Baumschicht\"\n  )\n\n\nDas Learning aus den Vorverarbeitungen die ich für euch übernommen habe und aus Aufgabe 1 ist, dass beim Erfassen und Dokumentieren von Daten aus verschiedenen Quellen darauf geachtet werden sollte, dies möglichst einheitlich zu tun (Dateiformate, Spalten, Bezeichnungen, Datentypen, usw.), damit kann man sich viel Arbeit ersparen. Hilfreich ist in diesem Zusammenhang immer ein einheitliches Feldprotokoll resp. eine Vorlage für die Erfassung der Daten zu erstellen.",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-2",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-2",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nZusammenführen der Teildatensätze zu einem Datensatz\n\n\nMusterlösung\ndf_gesamt &lt;- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6, df_team7)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-3",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-3",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der LIDAR Variablen in den Reh-Kreisen (Aufgabe3_LIDAR_Waldstruktur_Reh_Kreise_241011.csv).\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us_2022, DG_os_2022) aus dem LIDAR-Waldstruktur-Datensatz. –&gt; Welche Art von join? Welche Spalten zum Verbinden (join_by()) der Datensätze\n\n\nMusterlösung\ndf_with_LIDAR &lt;- left_join(df_gesamt, df_lidar, join_by(X == x, Y == y))",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-4",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-4",
    "title": "Daten(vor)verarbeitung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LiDAR-Daten (DG_xy_2022) erstellen (zusätzlich Einfärben der Gruppen und Regressionslinie darüberlegen). Korrelieren die Feldaufnahmen und die LiDAR basierte Waldstruktur?\nIm LiDAR Datensatz gibt es dieselben Variablen der Waldstruktur aus der LiDAR-Befliegung 2014. Ihr könnt untersuchen wie sich diese verändert haben und wie gut oder eben auch nicht sie mit euren Feldaufnahmen übereinstimmen.\n\n\nMusterlösung\n# .#####################################################################################\n# 4. VISUALISERUNG #####\n# .#####################################################################################\n\nggplot(df_with_LIDAR, aes(DG_us_2022, DG_Strauchschicht, color = team)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  xlim(0, 1) +\n  ylim(0, 100) +\n  geom_abline(slope = 100, intercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nMusterlösung\n\n\nwrite_delim(df_with_LIDAR, \"datasets/fallstudie_n/df_with_lidar_2024.csv\", delim = \";\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#zusatz-aufgabe-5",
    "href": "fallstudie_n/2_Datenverarbeitung.html#zusatz-aufgabe-5",
    "title": "Daten(vor)verarbeitung",
    "section": "(Zusatz-)Aufgabe 5",
    "text": "(Zusatz-)Aufgabe 5\nHerunterladen der Felderhebungsdaten von Moodle aus den vergangenen Jahren. Zusammenführen aller Datensätze. Explorative Datenanalyse zu den Veränderungen der Erhebungen ground truth LiDAR über die Zeit und zum Zusammenhang mit den LiDAR-basierten Waldstrukturvariablen aus den zwei Befliegungszeiträumen (2014 und 2022)\n!Achtung! es sind nicht alle Jahre gleich viele Teams an den Erhebungen beteiligt gewesen, daher gib es nicht für alle Teams in allen Jahren Daten.\n\n\nMusterlösung\n\nfiles &lt;- list.files(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_vergangene_Jahre\", pattern = \"*.csv\", full.names = TRUE)\n\n\ndf_ground_truth &lt;- files %&gt;%\n  map_df(~ read_delim(.x, delim = \";\") %&gt;% \n           mutate(Jahr = gsub(\"df_ground_truth_|\\\\.csv$\", \"\", basename(.x))))\n\n\ndf_without_LIDAR &lt;- df_with_LIDAR %&gt;% dplyr::select(KreisID:team) %&gt;% mutate(Jahr = \"2024\")\n\ndf_ground_truth &lt;- bind_rows(df_without_LIDAR, df_ground_truth)\n\ndf_ground_truth_with_LIDAR &lt;- left_join(df_ground_truth, df_lidar, join_by(X == x, Y == y))\n\ndf_ground_truth_with_LIDAR %&gt;%\n  ggplot(aes(KreisID, DG_Strauchschicht, color = as.numeric(Jahr), size = as.numeric(Jahr))) +\n  facet_wrap(~team, ncol = 1) +\n  geom_point() +\n  scale_color_viridis_c(option = \"plasma\") +  \n  scale_size_continuous(range = c(1, 3)) +  \n  labs(color = \"Year\", shape = \"Year\", size = \"Year\") +  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nMusterlösung\n\n\ndf_without_LIDAR &lt;- df_without_LIDAR %&gt;% dplyr::select(-Jahr)\n\nwrite_delim(df_without_LIDAR, \"datasets/fallstudie_n/df_ground_truth_2024.csv\", delim = \";\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Daten(vor)verarbeitung</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html",
    "title": "Berechnung Homeranges",
    "section": "",
    "text": "Libraries laden\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"terra\")\nlibrary(\"adehabitatHR\")\nlibrary(\"ggspatial\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#daten-einlesen",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#daten-einlesen",
    "title": "Berechnung Homeranges",
    "section": "Daten einlesen",
    "text": "Daten einlesen\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\n\nRehe &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Homeranges_Rehe_landforst_241011.csv\", delim = \";\")\n\nstr(Rehe)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-1",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-1",
    "title": "Berechnung Homeranges",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nIm Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\n\nRehe &lt;- Rehe |&gt;\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time),\n                                   format = \"%Y-%m-%d %H:%M:%S\"))",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-2",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-2",
    "title": "Berechnung Homeranges",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nMit den folgenden Zeilen können die GPS-Punkte visualisiert werden\n\n\nRehe_sf &lt;- st_as_sf(Rehe, coords = c(\"X\", \"Y\"), crs = 21781)\n\nRE13 &lt;- filter(Rehe_sf, TierID == \"RE13\")\n\nplot(RE13[\"TierID\"])\n\n\n\n\n\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\nHerumschrauben an der Ausdehnung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\n→ Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\n\n\nRE13_xy &lt;- st_coordinates(RE13)\n\nRE13_sp &lt;- as(RE13[\"TierID\"], \"Spatial\")\n\nsigma &lt;- 0.5 * (sd(RE13_xy[, 1]) + sd(RE13_xy[, 2]))\nn &lt;- nrow(RE13)\nhref &lt;- sigma * n ^ (-1 / 6) * 0.9 # scaled reference: href * 0.9\n\nkud &lt;- kernelUD(RE13_sp, h = href, grid = 25) \n\nhomerange &lt;- getverticeshr(kud, percent = 95) # Berechnung der Home Range (95% Isopleth)\n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr &lt;- st_as_sf(homerange)\n\nst_write(\n  hr,\n  dsn = \"results\",\n  layer = \"HR_RE13\",\n  driver = \"ESRI Shapefile\",\n  delete_layer = TRUE\n)\n## Deleting layer `HR_RE13' using driver `ESRI Shapefile'\n## Writing layer `HR_RE13' to data source `results' using driver `ESRI Shapefile'\n## Writing 1 features with 2 fields and geometry type Polygon.\n\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\nggplot(hr) + \n  geom_sf(size = 1, alpha = 0.3, color = \"red\", fill=\"red\") +\n  coord_sf(datum = sf::st_crs(21781))+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n  )\n\n\n\n\n\n\n\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nggplot(hr) + \n  geom_sf(size = 1, alpha = 0.3, color = \"red\", fill=\"red\") +\n  geom_sf(data = RE13, aes(fill = \"red\")) +\n  coord_sf(datum = sf::st_crs(21781))+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n  )\n\n\n\n\n\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden.\n\n\npk25_wpz &lt;- rast(\"datasets/fallstudie_n/pk25_wpz.tif\")\n\nggplot(hr, aes(color = \"red\", fill = \"red\")) +\n  annotation_spatial(pk25_wpz) +\n  geom_sf(size = 1, alpha = 0.3) +\n  geom_sf(data = RE13, aes(fill = \"red\")) +\n  coord_sf(datum = sf::st_crs(21781)) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\n\nAusdehnung des Grids basiert auf hr\nCellsize des Grids: 25m\n\n\n\nx25 &lt;- st_make_grid(hr, 25, what = \"centers\")\ngrid_plot &lt;- st_buffer(x25, 12.5)\n\nggplot(grid_plot, color = \"black\", fill = NA) +\n  geom_sf() +\n  geom_sf(data = RE13, color = \"blue\", ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\n  coord_sf(datum = 21781) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-3",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-3",
    "title": "Berechnung Homeranges",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nTesten der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar_2024 den wir letzte Woche erstellt haben\nDie Theorie in den Vormittagslektionen zu Korrelation folgt erst ab 29.10.\n\n\ndf_with_lidar &lt;- read_delim(\"datasets/fallstudie_n/df_with_lidar_2024.csv\", delim = \";\")\n\ncor.test(~ DG_Strauchschicht + DG_us_2022, data = df_with_lidar, method = \"pearson\")\n## \n##  Pearson's product-moment correlation\n## \n## data:  DG_Strauchschicht and DG_us_2022\n## t = 4.3863, df = 171, p-value = 2.011e-05\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1772221 0.4460498\n## sample estimates:\n##       cor \n## 0.3180136",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Berechnung Homeranges</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html",
    "href": "fallstudie_n/4_Multivariate_Modelle.html",
    "title": "Multivariate Modelle",
    "section": "",
    "text": "Libraries laden\nEinstieg Multivariate Modelle / Habitatselektionsmodell\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"PerformanceAnalytics\")\nlibrary(\"pastecs\")\nlibrary(\"car\")\nlibrary(\"psych\")\n\noptions(scipen = 999)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-1",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-1",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nEinlesen des Gesamtdatensatzes für die Multivariate Analyse von Moodle\n\nSichtung des Datensatzes, der Variablen und der Datentypen\nKontrolle wieviele Rehe in diesem Datensatz enthalten sind\n\n\n\nMusterlösung\nDF_mod &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe4_Datensatz_Habitatnutzung_Modelle_241028.csv\", delim = \";\")\n\nstr(DF_mod)\n\nclass(DF_mod$time_of_day)\n\ntable(DF_mod$id)\n\nDF_mod |&gt;\n  group_by(id) |&gt;\n  summarize(anzahl = n())\n\nlength(unique(DF_mod$id))",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-2",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-2",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nUnterteilung des Datensatzes in Teildatensätze entsprechend der Tageszeit\n\n\nMusterlösung\nDF_mod_night &lt;- DF_mod |&gt;\n  filter(time_of_day == \"night\")\n\nDF_mod_day &lt;- DF_mod |&gt;\n  filter(time_of_day == \"day\")\n\n# Kontrolle\ntable(DF_mod_night$time_of_day)\n## \n## night \n##  3964\n\ntable(DF_mod_day$time_of_day)\n## \n##  day \n## 3964",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-3",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-3",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nErstellen von Density Plots der Präsenz / Absenz in Abhängigkeit der unabhängigen Variablen. Diese Übung dient einer ersten groben Einschätzung der Wirkung der Umweltvariablen auf die abhängige Variable (Präsenz/Absenz in unserem Fall)\n\n# Ein Satz Density Plots für den Tagesdatensatz und einer für den Nachtdatensatz\n\npar(mfrow = c(3, 3), mar = c(4, 4, 3, 3)) # Vorbereitung Raster für Plots\n\n# innerhalb des for()-loops die Nummern der gewünschten Spalten einstellen\n\nfor (i in 5:12) {\n  dp &lt;- DF_mod_day |&gt; filter(pres_abs == 1) |&gt; pull(i)\n  dp &lt;- density(dp)\n  da &lt;- DF_mod_day |&gt; filter(pres_abs == 0) |&gt; pull(i)\n  da &lt;- density(da)\n  plot(0, 0, type = \"l\",\n    xlim = range(c(dp$x, da$x)),\n    ylim = range(dp$y, da$y),\n    xlab = names(DF_mod_day[i]),\n    ylab = \"Density\"\n  )\n  lines(dp$x, dp$y, col = \"blue\")             # Präsenz = used\n  lines(da$x, da$y, col = \"red\")              # Absenz = available\n}",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-4",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-4",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nTesten eurer erklärenden Variablen auf Normalverteilung (nur kontinuierliche)\n\n\nMusterlösung\n# klassischer Weg mit shapiro-wilk (vergl. Stats-Skript der Theorielektionen)\n# mehrere Spalten, verschiedenene statistische Kenngrössen werden angezeigt. Normalverteilung: Wert ganz unten. p&gt;0.05 = ja\n\nround(stat.desc(DF_mod_day[5:12], basic = F, norm = T),3)\n##                slope topo_pos dist_road_trails dist_road_only dist_sett\n## median        13.706    0.233           29.866         35.056   180.966\n## mean          15.356    0.733           42.619         47.657   211.911\n## SE.mean        0.163    0.093            0.668          0.690     2.359\n## CI.mean.0.95   0.319    0.183            1.309          1.353     4.624\n## var          105.214   34.574         1767.202       1888.047 22051.998\n## std.dev       10.257    5.880           42.038         43.452   148.499\n## coef.var       0.668    8.020            0.986          0.912     0.701\n## skewness       0.733    0.278            1.872          1.632     0.721\n## skew.2SE       9.425    3.578           24.071         20.976     9.269\n## kurtosis      -0.092    0.748            4.033          2.994    -0.174\n## kurt.2SE      -0.594    4.807           25.933         19.254    -1.119\n## normtest.W     0.942    0.990            0.807          0.843     0.945\n## normtest.p     0.000    0.000            0.000          0.000     0.000\n##              forest_prop us_2014 os_2014\n## median             0.655   0.059   0.785\n## mean               0.598   0.119   0.594\n## SE.mean            0.005   0.002   0.007\n## CI.mean.0.95       0.010   0.005   0.013\n## var                0.110   0.023   0.175\n## std.dev            0.332   0.150   0.419\n## coef.var           0.555   1.258   0.705\n## skewness          -0.405   1.647  -0.425\n## skew.2SE          -5.202  21.181  -5.467\n## kurtosis          -1.116   2.924  -1.572\n## kurt.2SE          -7.174  18.800 -10.106\n## normtest.W         0.910   0.793   0.781\n## normtest.p         0.000   0.000   0.000\n\n# empfohlener Weg\n\nggplot(DF_mod_day, aes(slope)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = mean(DF_mod_day$slope, na.rm = T), sd = sd(DF_mod_day$slope, na.rm = T)), color = \"black\", linewidth = 1)\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\n# Aufgabe 4: die Verteilung bei einem Teildatensatz zu testen reicht,\n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht,\n# nur die Nutzung durch das Reh nicht",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-5",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-5",
    "title": "Multivariate Modelle",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nExplorative Analysen der Variablen mit Scatterplots / Scatterplotmatrizen\n\nZu Scatterplots und Scatterplotmatrizen gibt es viele verschiedene Funktionen / Packages, schaut im Internet und sucht euch eines welches euch passt.\nTesten der Korrelation zwischen den Variablen (Parametrisch oder nicht-parametrische Methode? Ausserdem: gewisse Scatterplotmatrizen zeigen euch die Koeffizenten direkt an)\n\n\n\nMusterlösung\npairs.panels(DF_mod_day[5:12],\n  method = \"pearson\", # correlation method\n  hist.col = \"#00AFBB\",\n  density = TRUE, # show density plots\n  ellipses = TRUE # show correlation ellipses\n)\n\n\n\n\n\n\n\n\n\nMusterlösung\n\n# Aufgabe 5: die Korrelation bei einem Teildatensatz zu testen reicht,\n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht,\n# nur die Nutzung durch das Reh nicht.",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html",
    "href": "fallstudie_n/5_Variablenselektion.html",
    "title": "Variablenselektion Multivariate Modelle",
    "section": "",
    "text": "Libraries laden\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"PerformanceAnalytics\")\nlibrary(\"pastecs\")\nlibrary(\"lme4\")\nlibrary(\"bbmle\")\nlibrary(\"MuMIn\")\nlibrary(\"MASS\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Variablenselektion Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#variablenselektion",
    "href": "fallstudie_n/5_Variablenselektion.html#variablenselektion",
    "title": "Variablenselektion Multivariate Modelle",
    "section": "Variablenselektion",
    "text": "Variablenselektion\n→ Vorgehen analog Coppes u. a. (2017)",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Variablenselektion Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-1",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-1",
    "title": "Variablenselektion Multivariate Modelle",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nMit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden, vergl. Aufgabe 5 vorangehende Woche\n\nDF_mod &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe4_Datensatz_Habitatnutzung_Modelle_241028.csv\", delim = \";\")\n\nDF_mod_day &lt;- DF_mod |&gt;\n  filter(time_of_day == \"day\")\n\nround(cor(DF_mod_day[, 5:12], method = \"pearson\"), 2)\n##                  slope topo_pos dist_road_trails dist_road_only dist_sett\n## slope             1.00     0.09             0.31           0.34      0.14\n## topo_pos          0.09     1.00             0.06           0.09      0.08\n## dist_road_trails  0.31     0.06             1.00           0.93     -0.05\n## dist_road_only    0.34     0.09             0.93           1.00     -0.08\n## dist_sett         0.14     0.08            -0.05          -0.08      1.00\n## forest_prop       0.29     0.01            -0.03          -0.05      0.50\n## us_2014           0.28    -0.04             0.03           0.04      0.06\n## os_2014           0.45     0.06            -0.03          -0.01      0.34\n##                  forest_prop us_2014 os_2014\n## slope                   0.29    0.28    0.45\n## topo_pos                0.01   -0.04    0.06\n## dist_road_trails       -0.03    0.03   -0.03\n## dist_road_only         -0.05    0.04   -0.01\n## dist_sett               0.50    0.06    0.34\n## forest_prop             1.00    0.34    0.76\n## us_2014                 0.34    1.00    0.47\n## os_2014                 0.76    0.47    1.00\n\n# hier kann die Schwelle für die Korrelation gesetzt werden, 0.7 ist eher liberal /\n# 0.5 konservativ\n\ncor &lt;- round(cor(DF_mod_day[, 5:12], method = \"pearson\"), 2)\ncor[abs(cor) &lt; 0.7] &lt;- 0\ncor\n##                  slope topo_pos dist_road_trails dist_road_only dist_sett\n## slope                1        0             0.00           0.00         0\n## topo_pos             0        1             0.00           0.00         0\n## dist_road_trails     0        0             1.00           0.93         0\n## dist_road_only       0        0             0.93           1.00         0\n## dist_sett            0        0             0.00           0.00         1\n## forest_prop          0        0             0.00           0.00         0\n## us_2014              0        0             0.00           0.00         0\n## os_2014              0        0             0.00           0.00         0\n##                  forest_prop us_2014 os_2014\n## slope                   0.00       0    0.00\n## topo_pos                0.00       0    0.00\n## dist_road_trails        0.00       0    0.00\n## dist_road_only          0.00       0    0.00\n## dist_sett               0.00       0    0.00\n## forest_prop             1.00       0    0.76\n## us_2014                 0.00       1    0.00\n## os_2014                 0.76       0    1.00",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Variablenselektion Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-2",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-2",
    "title": "Variablenselektion Multivariate Modelle",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nSkalieren der Variablen, damit ihr Einfluss vergleichbar wird (Befehl scale(); Problem verschiedene Skalen der Variablen (bspw. Neigung in Grad, Distanz in Metern)); Umwandeln der Reh-ID in einen Faktor, damit dieser als Random Factor ins Model eingespiesen werden kann.\n\nDF_mod_day &lt;- DF_mod_day |&gt;\n  mutate(\n    slope_scaled = scale(slope),\n    topo_pos_scaled = scale(topo_pos),\n    us_scaled = scale(us_2014),\n    os_scaled = scale(os_2014),\n    forest_prop_scaled = scale(forest_prop),\n    dist_road_trails_scaled = scale(dist_road_trails),\n    dist_road_only_scaled = scale(dist_road_only),\n    dist_sett_scaled = scale(dist_sett),\n    id = as.factor(id)\n  )",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Variablenselektion Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-3",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-3",
    "title": "Variablenselektion Multivariate Modelle",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nSelektion der Variablen in einem univariaten Model\nEin erstes GLMM (Generalized Linear Mixed Effects Modell) aufbauen: Funktion und Modelformel\n\nwichtige Seite auf der man viele Hilfestellungen zu GLMM’s finden kann.\n\n\n# wir werden das package lme4 mit der Funktion glmer verwenden\n\n# die Hilfe von glmer aufrufen: ?glmer\n\n# glmer(formula, data = , family = binomial)\n\n# 1) formula:\n# Abhängige Variable ~ Erklärende Variable + Random Factor\n# In unseren Modellen kontrollieren wir für individuelle Unterschiede bei den Rehen\n# indem wir einen Random Factor definieren =&gt; (1 | id)\n\n# 2) data:\n# euer Datensatz\n\n# 3) family:\n# hier binomial\n\n# warum binomial? Verteilung Daten der abhängigen Variable Präsenz/Absenz\n\nggplot(DF_mod_day, aes(pres_abs)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n# --&gt; Binäre Verteilung =&gt; Binomiale Verteilung mit n = 1\n\n# und wie schaut die Verteilung der Daten der abhängigen Variable Nutzungsintensität\n# (nmb, werden wir in diesem Kurs aber nicht genauer anschauen) aus?",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Variablenselektion Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-4",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-4",
    "title": "Variablenselektion Multivariate Modelle",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nMit der GLMM Formel bauen wir in einem ersten Schritt eine univariate Variablenselektion auf.\nAls abhängige Variable verwenden wir die Präsenz/Absenz der Rehe in den Kreisen\n\n# Die erklärende Variable in m1 ist die erste Variable der korrelierenden Variablen\n# Die erklärende Variable in m2 ist die zweite Variable der korrelierenden Variablen\n\nmodell_1 &lt;- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id),\n  data = DF_mod_day,\n  family = binomial\n)\n\nmodell_2 &lt;- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id),\n  data = DF_mod_day,\n  family = binomial\n)\n\n# mit dieser Funktion können die Modellergebnisse inspiziert werden\nsummary(modell_1)\nsummary(modell_2)\n\n# Mit dieser Funktion kann der Informationgehalt der beiden Modelle gegeneinander\n# abgeschätzt werden\nbbmle::AICtab(modell_1, modell_2)\n\n# tieferer AIC -&gt; besser (AIC = Akaike information criterion)\n\n# ==&gt; dieses Vorgehen muss nun für alle korrelierten Variablen für jeden Teildatensatz\n# (Tag/Nacht) durchgeführt werden, um nur noch nicht (R &lt; 0.7) korrelierte Variablen\n# in das Modell einfliessen zu lassen\n\n\n\nMusterlösung\n\nmodell_1 &lt;- glmer(pres_abs ~ dist_road_trails_scaled + (1 | id), data = DF_mod_day, family = binomial)\nmodell_2 &lt;- glmer(pres_abs ~ dist_road_only_scaled + (1 | id), data = DF_mod_day, family = binomial)\n\nsummary(modell_1)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: pres_abs ~ dist_road_trails_scaled + (1 | id)\n##    Data: DF_mod_day\n## \n##      AIC      BIC   logLik deviance df.resid \n##   5104.9   5123.7  -2549.4   5098.9     3961 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.1046 -0.7564 -0.6277  1.0812  1.9219 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  id     (Intercept) 0.1996   0.4467  \n## Number of obs: 3964, groups:  id, 12\n## \n## Fixed effects:\n##                         Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)             -0.31120    0.13418  -2.319   0.0204 *  \n## dist_road_trails_scaled  0.37745    0.03848   9.808   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## dst_rd_trl_ -0.004\nsummary(modell_2)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: pres_abs ~ dist_road_only_scaled + (1 | id)\n##    Data: DF_mod_day\n## \n##      AIC      BIC   logLik deviance df.resid \n##   5098.4   5117.2  -2546.2   5092.4     3961 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -1.9625 -0.7615 -0.6080  1.0579  1.9425 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  id     (Intercept) 0.2103   0.4586  \n## Number of obs: 3964, groups:  id, 12\n## \n## Fixed effects:\n##                       Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)            -0.3024     0.1375  -2.199   0.0279 *  \n## dist_road_only_scaled   0.3950     0.0389  10.154   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## dst_rd_nly_ 0.002\n\nbbmle::AICtab(modell_1, modell_2)\n##          dAIC df\n## modell_2 0.0  3 \n## modell_1 6.5  3\n\n# tieferer AIC -&gt; besser (AIC = Akaike information criterion) -&gt; als deltaAIC ausgewiesen \n# Distanz zu Strassen (dist_road_only) = besser\n\n\nmodell_3 &lt;- glmer(pres_abs ~ forest_prop_scaled + (1 | id), data = DF_mod_day, family = binomial)\nmodell_4 &lt;- glmer(pres_abs ~ os_scaled + (1 | id), data = DF_mod_day, family = binomial)\n\nsummary(modell_3)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: pres_abs ~ forest_prop_scaled + (1 | id)\n##    Data: DF_mod_day\n## \n##      AIC      BIC   logLik deviance df.resid \n##   4726.1   4745.0  -2360.1   4720.1     3961 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.2952 -0.7853 -0.3909  0.9056  3.4113 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  id     (Intercept) 0.3069   0.554   \n## Number of obs: 3964, groups:  id, 12\n## \n## Fixed effects:\n##                    Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)         -0.4351     0.1649  -2.639  0.00831 ** \n## forest_prop_scaled   0.8805     0.0443  19.878  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## frst_prp_sc -0.060\nsummary(modell_4)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: pres_abs ~ os_scaled + (1 | id)\n##    Data: DF_mod_day\n## \n##      AIC      BIC   logLik deviance df.resid \n##   4863.0   4881.9  -2428.5   4857.0     3961 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.0044 -0.7984 -0.4421  0.9854  2.8754 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  id     (Intercept) 0.3228   0.5682  \n## Number of obs: 3964, groups:  id, 12\n## \n## Fixed effects:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -0.34748    0.16847  -2.063   0.0392 *  \n## os_scaled    0.67559    0.03862  17.494   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##           (Intr)\n## os_scaled -0.029\n\nbbmle::AICtab(modell_3, modell_4)\n##          dAIC  df\n## modell_3   0.0 3 \n## modell_4 136.9 3\n\n# tieferer AIC -&gt; besser (AIC = Akaike information criterion) -&gt; als deltaAIC ausgewiesen \n# Distanz zu Strassen (dist_road_only) = besser",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Variablenselektion Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-5",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-5",
    "title": "Variablenselektion Multivariate Modelle",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nSelektion der Variablen in einem multivariaten Model\nMit folgendem Code kann eine automatisierte Variablenselektion (dredge-Funktion) und ein Modelaveraging aufgebaut werden (siehe auch Stats-Skript von J.Dengler & Team)\n\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6\n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)\n\nf &lt;- pres_abs ~\n  V1 +\n  V2 +\n  V3 +\n  V4 +\n  V5 +\n  V6 \n\n# in diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel\n# daraus gemacht\n\nf_dredge &lt;- paste(c(f, \"+ (1 | id)\"), collapse = \" \") |&gt; as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm &lt;- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m &lt;- dredge(m)\nall_m\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte\n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nsw(all_m)\n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel &lt;- model.avg(all_m, rank = \"AICc\", subset = delta &lt; 2)\nsummary(avgmodel)\n\n# ==&gt; für den Nachtdatensatz muss der gleiche Prozess der Variablenselektion\n# durchgespielt werden.\n\n\n\nMusterlösung\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6\n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)\n\nf &lt;- pres_abs ~\n  slope_scaled +\n  topo_pos_scaled +\n  us_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_sett_scaled \n\n# in diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel\n# daraus gemacht\n\nf_dredge &lt;- paste(c(f, \"+ (1 | id)\"), collapse = \" \") |&gt; as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm &lt;- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m &lt;- dredge(m)\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte\n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nsw(all_m)\n##                      dist_road_only_scaled forest_prop_scaled us_scaled\n## Sum of weights:      1.00                  1.00               1.00     \n## N containing models:   32                    32                 32     \n##                      slope_scaled dist_sett_scaled topo_pos_scaled\n## Sum of weights:      0.99         0.46             0.30           \n## N containing models:   32           32               32\n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel &lt;- model.avg(all_m, rank = \"AICc\", subset = delta &lt; 2)\nsummary(avgmodel)\n## \n## Call:\n## model.avg(object = get.models(object = all_m, subset = delta &lt; \n##     2), rank = \"AICc\")\n## \n## Component model call: \n## glmer(formula = pres_abs ~ &lt;4 unique rhs&gt;, data = DF_mod_day, family = \n##      binomial, na.action = na.fail)\n## \n## Component models: \n##        df   logLik    AICc delta weight\n## 1346    6 -2268.17 4548.37  0.00   0.38\n## 12346   7 -2267.34 4548.71  0.34   0.32\n## 13456   7 -2268.01 4550.04  1.68   0.16\n## 123456  8 -2267.16 4550.35  1.98   0.14\n## \n## Term codes: \n## dist_road_only_scaled      dist_sett_scaled    forest_prop_scaled \n##                     1                     2                     3 \n##          slope_scaled       topo_pos_scaled             us_scaled \n##                     4                     5                     6 \n## \n## Model-averaged coefficients:  \n## (full average) \n##                        Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)           -0.432191   0.139554    0.139597   3.096  0.00196 ** \n## dist_road_only_scaled  0.416554   0.046861    0.046875   8.886  &lt; 2e-16 ***\n## forest_prop_scaled     0.818821   0.057266    0.057282  14.295  &lt; 2e-16 ***\n## slope_scaled          -0.158651   0.049230    0.049245   3.222  0.00127 ** \n## us_scaled              0.390805   0.041070    0.041083   9.513  &lt; 2e-16 ***\n## dist_sett_scaled      -0.036839   0.058045    0.058054   0.635  0.52571    \n## topo_pos_scaled        0.006678   0.022846    0.022852   0.292  0.77013    \n##  \n## (conditional average) \n##                       Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)           -0.43219    0.13955     0.13960   3.096  0.00196 ** \n## dist_road_only_scaled  0.41655    0.04686     0.04688   8.886  &lt; 2e-16 ***\n## forest_prop_scaled     0.81882    0.05727     0.05728  14.295  &lt; 2e-16 ***\n## slope_scaled          -0.15865    0.04923     0.04924   3.222  0.00127 ** \n## us_scaled              0.39081    0.04107     0.04108   9.513  &lt; 2e-16 ***\n## dist_sett_scaled      -0.08030    0.06208     0.06210   1.293  0.19597    \n## topo_pos_scaled        0.02197    0.03717     0.03718   0.591  0.55451    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nMusterlösung\n\n#| eval: false\n#| error: true\n#| echo: false\n\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V8\n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)\n\nf &lt;- pres_abs ~\n  slope_scaled +\n  topo_pos_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_road_trails_scaled +\n  dist_sett_scaled \n\n# inn diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel\n# daraus gemacht\n\nf_dredge &lt;- paste(c(f, \"+ (1 | id)\"), collapse = \" \") |&gt; as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm &lt;- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\ncar::vif(m)\n##            slope_scaled         topo_pos_scaled               us_scaled \n##                1.514328                1.083853                1.242521 \n##               os_scaled      forest_prop_scaled   dist_road_only_scaled \n##                2.626511                2.664066                5.844383 \n## dist_road_trails_scaled        dist_sett_scaled \n##                5.674976                1.427576\nmean(car::vif(m))\n## [1] 2.759777\n\n\n\n\n\n\nCoppes, Joy, Judith Ehrlacher, Dominik Thiel, Rudi Suchant, und Veronika Braunisch. 2017. „Outdoor recreation causes effective habitat reduction in capercaillie Tetrao urogallus: a major threat for geographically restricted populations“. Journal of avian biology 48 (12): 1583–94.",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Variablenselektion Multivariate Modelle</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html",
    "title": "Modellgüte und -diagnostics MM",
    "section": "",
    "text": "Libraries laden\nPackages die wir für die Modelle und die Diagnostics brauchen\nlibrary(\"lme4\")\nlibrary(\"bbmle\")\nlibrary(\"MuMIn\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"DHARMa\")\nlibrary(\"car\")\nlibrary(\"MASS\")\nlibrary(\"ROCR\")\nlibrary(\"sjPlot\")\nlibrary(\"ggeffects\")\nlibrary(\"sjstats\")\nlibrary(\"cowplot\")\nlibrary(\"gstat\")\nlibrary(\"purrr\")\nlibrary(\"broom.mixed\")",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modellgüte und -diagnostics MM</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#ausgangslage",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#ausgangslage",
    "title": "Modellgüte und -diagnostics MM",
    "section": "Ausgangslage",
    "text": "Ausgangslage\n\nDer Modellfit aus Aufgabe 5 von letzter Woche dient als Ausgangspunkt für die heutigen Übungen.\n\n\nDF_mod_day &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe4_Datensatz_Habitatnutzung_Modelle_241028.csv\", delim = \";\") |&gt;\n  filter(time_of_day == \"day\") |&gt;\n  mutate(\n    slope_scaled = scale(slope),\n    topo_pos_scaled = scale(topo_pos),\n    us_scaled = scale(us_2014),\n    os_scaled = scale(os_2014),\n    forest_prop_scaled = scale(forest_prop),\n    dist_road_trails_scaled = scale(dist_road_trails),\n    dist_road_only_scaled = scale(dist_road_only),\n    dist_sett_scaled = scale(dist_sett),\n    id = as.factor(id)\n  )\n\nf &lt;- pres_abs ~\n  slope_scaled +\n  topo_pos_scaled +\n  us_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_sett_scaled \n\nf &lt;- paste(c(f, \"+ (1 | id)\"), collapse = \" \") |&gt; as.formula()\n\nm_day &lt;- glmer(f, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nall_m &lt;- dredge(m_day)\nall_m\n## Global model call: glmer(formula = pres_abs ~ slope_scaled + topo_pos_scaled + us_scaled + \n##     forest_prop_scaled + dist_road_only_scaled + dist_sett_scaled + \n##     (1 | id), data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n## ---\n## Model selection table \n##      (Int) dst_rod_onl_scl dst_stt_scl frs_prp_scl  slp_scl top_pos_scl us_scl\n## 46 -0.4277          0.4145                  0.8012 -0.15640             0.3920\n## 48 -0.4369          0.4226   -0.079960      0.8387 -0.15920             0.3887\n## 62 -0.4285          0.4090                  0.8022 -0.15950   0.0213600 0.3931\n## 64 -0.4379          0.4170   -0.081080      0.8403 -0.16250   0.0226900 0.3898\n## 38 -0.4117          0.3804                  0.7412                      0.3627\n## 40 -0.4197          0.3869   -0.071040      0.7737                      0.3593\n## 54 -0.4119          0.3781                  0.7411            0.0084000 0.3629\n## 56 -0.4199          0.3843   -0.071390      0.7737            0.0092950 0.3596\n## 61 -0.4341                                  0.7850 -0.07165   0.0831200 0.3920\n## 53 -0.4255                                  0.7552            0.0742700 0.3787\n## 63 -0.4339                    0.001142      0.7845 -0.07165   0.0830900 0.3920\n## 55 -0.4254                    0.001146      0.7547            0.0742400 0.3787\n## 37 -0.4239                                  0.7540                      0.3761\n## 45 -0.4299                                  0.7760 -0.05319             0.3858\n## 39 -0.4231                    0.006821      0.7509                      0.3764\n## 47 -0.4291                    0.007447      0.7727 -0.05324             0.3861\n## 8  -0.4390          0.4070   -0.112600      0.9136                            \n## 16 -0.4463          0.4204   -0.117100      0.9414 -0.05910                   \n## 6  -0.4262          0.3966                  0.8639                            \n## 24 -0.4390          0.4067   -0.112700      0.9136            0.0011510       \n## 14 -0.4324          0.4083                  0.8876 -0.05398                   \n## 32 -0.4466          0.4191   -0.117500      0.9420 -0.05996   0.0061540       \n## 22 -0.4262          0.3968                  0.8639           -0.0007422       \n## 30 -0.4326          0.4075                  0.8878 -0.05449   0.0036870       \n## 21 -0.4374                                  0.8824            0.0638700       \n## 5  -0.4351                                  0.8805                            \n## 29 -0.4334                                  0.8678  0.03079   0.0601800       \n## 23 -0.4412                   -0.034380      0.8972            0.0648100       \n## 13 -0.4298                                  0.8603  0.04319                   \n## 7  -0.4383                   -0.028830      0.8930                            \n## 31 -0.4373                   -0.033890      0.8826  0.03050   0.0611400       \n## 15 -0.4330                   -0.028670      0.8727  0.04314                   \n## 44 -0.2756          0.3214    0.389700              0.10970             0.5118\n## 60 -0.2756          0.3213    0.389700              0.10960   0.0005417 0.5119\n## 36 -0.2822          0.3431    0.411700                                  0.5425\n## 52 -0.2824          0.3407    0.411300                        0.0102700 0.5428\n## 43 -0.2764                    0.424900              0.17800             0.4999\n## 59 -0.2773                    0.424200              0.17050   0.0421400 0.5038\n## 42 -0.2855          0.3410                          0.17340             0.5253\n## 58 -0.2855          0.3402                          0.17280   0.0042270 0.5256\n## 51 -0.2874                    0.465000                        0.0623500 0.5514\n## 34 -0.2978          0.3782                                              0.5763\n## 35 -0.2866                    0.468800                                  0.5488\n## 50 -0.2980          0.3737                                    0.0209100 0.5767\n## 41 -0.2860                                          0.25420             0.5137\n## 57 -0.2868                                          0.24550   0.0463100 0.5178\n## 49 -0.3036                                                    0.0779400 0.5905\n## 33 -0.3032                                                              0.5879\n## 12 -0.2624          0.2999    0.421000              0.29320                   \n## 28 -0.2615          0.3051    0.421200              0.29670  -0.0298900       \n## 4  -0.2835          0.3525    0.490900                                        \n## 20 -0.2834          0.3533    0.491100                       -0.0043180       \n## 11 -0.2604                    0.453400              0.34800                   \n## 27 -0.2606                    0.453400              0.34720   0.0059370       \n## 10 -0.2714          0.3218                          0.36790                   \n## 26 -0.2707          0.3261                          0.37140  -0.0270500       \n## 3  -0.2827                    0.552500                                        \n## 19 -0.2839                    0.549900                        0.0434200       \n## 9  -0.2701                                          0.43490                   \n## 25 -0.2704                                          0.43330   0.0098340       \n## 2  -0.3024          0.3950                                                    \n## 18 -0.3026          0.3935                                    0.0082640       \n## 17 -0.3050                                                    0.0618200       \n## 1  -0.3039                                                                    \n##    df    logLik   AICc  delta weight\n## 46  6 -2268.173 4548.4   0.00  0.373\n## 48  7 -2267.341 4548.7   0.34  0.314\n## 62  7 -2268.008 4550.0   1.68  0.161\n## 64  8 -2267.155 4550.3   1.98  0.139\n## 38  5 -2273.321 4556.7   8.29  0.006\n## 40  6 -2272.657 4557.3   8.97  0.004\n## 54  6 -2273.295 4558.6  10.24  0.002\n## 56  7 -2272.625 4559.3  10.91  0.002\n## 61  6 -2308.279 4628.6  80.21  0.000\n## 53  5 -2309.406 4628.8  80.46  0.000\n## 63  7 -2308.279 4630.6  82.22  0.000\n## 55  6 -2309.406 4630.8  82.47  0.000\n## 37  4 -2311.629 4631.3  82.90  0.000\n## 45  5 -2310.989 4632.0  83.62  0.000\n## 39  5 -2311.623 4633.3  84.89  0.000\n## 47  6 -2310.981 4634.0  85.62  0.000\n## 8   5 -2315.206 4640.4  92.06  0.000\n## 16  6 -2314.415 4640.9  92.48  0.000\n## 6   4 -2316.927 4641.9  93.50  0.000\n## 24  6 -2315.206 4642.4  94.06  0.000\n## 14  5 -2316.266 4642.5  94.18  0.000\n## 32  7 -2314.401 4642.8  94.46  0.000\n## 22  5 -2316.927 4643.9  95.50  0.000\n## 30  6 -2316.261 4644.5  96.17  0.000\n## 21  4 -2358.374 4724.8 176.39  0.000\n## 5   3 -2360.069 4726.1 177.78  0.000\n## 29  5 -2358.151 4726.3 177.95  0.000\n## 23  5 -2358.215 4726.4 178.08  0.000\n## 13  4 -2359.618 4727.2 178.88  0.000\n## 7   4 -2359.957 4727.9 179.56  0.000\n## 31  6 -2357.997 4728.0 179.65  0.000\n## 15  5 -2359.507 4729.0 180.66  0.000\n## 44  6 -2380.847 4773.7 225.35  0.000\n## 60  7 -2380.847 4775.7 227.36  0.000\n## 36  5 -2383.854 4777.7 229.35  0.000\n## 52  6 -2383.813 4779.6 231.28  0.000\n## 43  5 -2409.667 4829.3 280.98  0.000\n## 59  6 -2408.958 4829.9 281.57  0.000\n## 42  5 -2410.217 4830.4 282.08  0.000\n## 58  6 -2410.211 4832.4 284.07  0.000\n## 51  5 -2416.609 4843.2 294.87  0.000\n## 34  4 -2418.060 4844.1 295.76  0.000\n## 35  4 -2418.198 4844.4 296.04  0.000\n## 50  5 -2417.890 4845.8 297.43  0.000\n## 41  4 -2444.453 4896.9 348.55  0.000\n## 57  5 -2443.591 4897.2 348.83  0.000\n## 49  4 -2460.365 4928.7 380.37  0.000\n## 33  3 -2462.877 4931.8 383.39  0.000\n## 12  5 -2468.927 4947.9 399.50  0.000\n## 28  6 -2468.571 4949.2 400.79  0.000\n## 4   4 -2493.662 4995.3 446.97  0.000\n## 20  5 -2493.654 4997.3 448.96  0.000\n## 11  4 -2496.060 5000.1 451.76  0.000\n## 27  5 -2496.045 5002.1 453.74  0.000\n## 10  4 -2504.812 5017.6 469.27  0.000\n## 26  5 -2504.515 5019.0 470.68  0.000\n## 3   3 -2533.366 5072.7 524.37  0.000\n## 19  4 -2532.551 5073.1 524.74  0.000\n## 9   3 -2537.714 5081.4 533.07  0.000\n## 25  4 -2537.673 5083.4 534.99  0.000\n## 2   3 -2546.184 5098.4 550.01  0.000\n## 18  4 -2546.156 5100.3 551.95  0.000\n## 17  3 -2598.835 5203.7 655.31  0.000\n## 1   2 -2600.524 5205.1 656.68  0.000\n## Models ranked by AICc(x) \n## Random terms (all models): \n##   1 | id\n\navgmodel &lt;- model.avg(all_m, rank = \"AICc\", subset = delta &lt; 2)\nsummary(avgmodel)\n## \n## Call:\n## model.avg(object = get.models(object = all_m, subset = delta &lt; \n##     2), rank = \"AICc\")\n## \n## Component model call: \n## glmer(formula = pres_abs ~ &lt;4 unique rhs&gt;, data = DF_mod_day, family = \n##      binomial, na.action = na.fail)\n## \n## Component models: \n##        df   logLik    AICc delta weight\n## 1346    6 -2268.17 4548.37  0.00   0.38\n## 12346   7 -2267.34 4548.71  0.34   0.32\n## 13456   7 -2268.01 4550.04  1.68   0.16\n## 123456  8 -2267.16 4550.35  1.98   0.14\n## \n## Term codes: \n## dist_road_only_scaled      dist_sett_scaled    forest_prop_scaled \n##                     1                     2                     3 \n##          slope_scaled       topo_pos_scaled             us_scaled \n##                     4                     5                     6 \n## \n## Model-averaged coefficients:  \n## (full average) \n##                        Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)           -0.432191   0.139554    0.139597   3.096  0.00196 ** \n## dist_road_only_scaled  0.416554   0.046861    0.046875   8.886  &lt; 2e-16 ***\n## forest_prop_scaled     0.818821   0.057266    0.057282  14.295  &lt; 2e-16 ***\n## slope_scaled          -0.158651   0.049230    0.049245   3.222  0.00127 ** \n## us_scaled              0.390805   0.041070    0.041083   9.513  &lt; 2e-16 ***\n## dist_sett_scaled      -0.036839   0.058045    0.058054   0.635  0.52571    \n## topo_pos_scaled        0.006678   0.022846    0.022852   0.292  0.77013    \n##  \n## (conditional average) \n##                       Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)           -0.43219    0.13955     0.13960   3.096  0.00196 ** \n## dist_road_only_scaled  0.41655    0.04686     0.04688   8.886  &lt; 2e-16 ***\n## forest_prop_scaled     0.81882    0.05727     0.05728  14.295  &lt; 2e-16 ***\n## slope_scaled          -0.15865    0.04923     0.04924   3.222  0.00127 ** \n## us_scaled              0.39081    0.04107     0.04108   9.513  &lt; 2e-16 ***\n## dist_sett_scaled      -0.08030    0.06208     0.06210   1.293  0.19597    \n## topo_pos_scaled        0.02197    0.03717     0.03718   0.591  0.55451    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDie Modellresultate aus dem avgmodel sind grundsätzlich die finalen Resultate die bereits interpretiert werden könnten. Allerdings funktionieren die Diagnosetests und die Darstellung der Resultate mit diesem gemittelten Modell nicht sehr gut, weshalb wir einen re-fit mit glmer machen müssen (an den Resultaten ändert sich dadurch nichts)\n\n\n# hier zum Vergleich, dass die Resulate sich nur marginal verändern\n\nsummary(avgmodel)\n## \n## Call:\n## model.avg(object = get.models(object = all_m, subset = delta &lt; \n##     2), rank = \"AICc\")\n## \n## Component model call: \n## glmer(formula = pres_abs ~ &lt;4 unique rhs&gt;, data = DF_mod_day, family = \n##      binomial, na.action = na.fail)\n## \n## Component models: \n##        df   logLik    AICc delta weight\n## 1346    6 -2268.17 4548.37  0.00   0.38\n## 12346   7 -2267.34 4548.71  0.34   0.32\n## 13456   7 -2268.01 4550.04  1.68   0.16\n## 123456  8 -2267.16 4550.35  1.98   0.14\n## \n## Term codes: \n## dist_road_only_scaled      dist_sett_scaled    forest_prop_scaled \n##                     1                     2                     3 \n##          slope_scaled       topo_pos_scaled             us_scaled \n##                     4                     5                     6 \n## \n## Model-averaged coefficients:  \n## (full average) \n##                        Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)           -0.432191   0.139554    0.139597   3.096  0.00196 ** \n## dist_road_only_scaled  0.416554   0.046861    0.046875   8.886  &lt; 2e-16 ***\n## forest_prop_scaled     0.818821   0.057266    0.057282  14.295  &lt; 2e-16 ***\n## slope_scaled          -0.158651   0.049230    0.049245   3.222  0.00127 ** \n## us_scaled              0.390805   0.041070    0.041083   9.513  &lt; 2e-16 ***\n## dist_sett_scaled      -0.036839   0.058045    0.058054   0.635  0.52571    \n## topo_pos_scaled        0.006678   0.022846    0.022852   0.292  0.77013    \n##  \n## (conditional average) \n##                       Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)           -0.43219    0.13955     0.13960   3.096  0.00196 ** \n## dist_road_only_scaled  0.41655    0.04686     0.04688   8.886  &lt; 2e-16 ***\n## forest_prop_scaled     0.81882    0.05727     0.05728  14.295  &lt; 2e-16 ***\n## slope_scaled          -0.15865    0.04923     0.04924   3.222  0.00127 ** \n## us_scaled              0.39081    0.04107     0.04108   9.513  &lt; 2e-16 ***\n## dist_sett_scaled      -0.08030    0.06208     0.06210   1.293  0.19597    \n## topo_pos_scaled        0.02197    0.03717     0.03718   0.591  0.55451    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(m_day)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: \n## pres_abs ~ slope_scaled + topo_pos_scaled + us_scaled + forest_prop_scaled +  \n##     dist_road_only_scaled + dist_sett_scaled + (1 | id)\n##    Data: DF_mod_day\n## \n##      AIC      BIC   logLik deviance df.resid \n##   4550.3   4600.6  -2267.2   4534.3     3956 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -6.5371 -0.7152 -0.3711  0.8621  3.8118 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  id     (Intercept) 0.2106   0.4589  \n## Number of obs: 3964, groups:  id, 12\n## \n## Fixed effects:\n##                       Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)           -0.43788    0.13888  -3.153  0.00162 ** \n## slope_scaled          -0.16249    0.04946  -3.285  0.00102 ** \n## topo_pos_scaled        0.02269    0.03718   0.610  0.54172    \n## us_scaled              0.38984    0.04111   9.482  &lt; 2e-16 ***\n## forest_prop_scaled     0.84030    0.05835  14.401  &lt; 2e-16 ***\n## dist_road_only_scaled  0.41697    0.04740   8.797  &lt; 2e-16 ***\n## dist_sett_scaled      -0.08108    0.06215  -1.305  0.19203    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr) slp_sc tp_ps_ us_scl frst__ dst___\n## slope_scald  0.040                                   \n## top_ps_scld -0.011 -0.110                            \n## us_scaled    0.006 -0.233  0.048                     \n## frst_prp_sc -0.093 -0.358  0.046 -0.144              \n## dst_rd_nly_ -0.008 -0.213 -0.194  0.024  0.158       \n## dst_stt_scl  0.053  0.050 -0.030  0.056 -0.508 -0.128",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modellgüte und -diagnostics MM</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-1",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-1",
    "title": "Modellgüte und -diagnostics MM",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nBerechung der AUC (area under the receiver operating characteristic curve) = Mass der Modellgüte\nFür die Berechnung des AUC findet ihr weiterführende Informationen unter: Link\n\nprob &lt;- predict(m_day, type = c(\"response\"))\npred &lt;- prediction(prob, DF_mod_day$pres_abs)\n\n# AUC\n\nauc &lt;- performance(pred, measure = \"auc\")@y.values[[1]]\nauc",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modellgüte und -diagnostics MM</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-2",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-2",
    "title": "Modellgüte und -diagnostics MM",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nInterpretieren der Modell-Residuen mittels Tests auf verschiedene Aspekte\n\nModel testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package.\nunbedingt die Vignette des DHARMa-Package konsultieren: Link\n\n\n# Residuals werden über eine Simulation auf eine Standard-Skala transformiert und\n# können anschliessend getestet werden. Dabei kann die Anzahl Simulationen eingestellt\n# werden (dauert je nach dem sehr lange)\n\nsimulationOutput &lt;- simulateResiduals(fittedModel = m_day, n = 10000)\n\n# plotting and testing scaled residuals\n\nplot(simulationOutput)\ntestResiduals(simulationOutput)\n\n# The most common concern for GLMMs is overdispersion, underdispersion and\n# zero-inflation.\n\n# separate test for dispersion\n\ntestDispersion(simulationOutput)\n\n# test for Zeroinflation\n\ntestZeroInflation(simulationOutput)\n\n# test for spatial Autocorrelation\n\n# calculating x, y positions per group\ngroupLocations &lt;- aggregate(DF_mod_day[, 3:4], list(DF_mod_day$x, DF_mod_day$y), mean)\ngroupLocations$group &lt;- paste(groupLocations$Group.1, groupLocations$Group.2)\ngroupLocations &lt;- groupLocations |&gt; dplyr::select(x,y,group)\n\n\n\n# calculating residuals per group\nres2 = recalculateResiduals(simulationOutput, group = groupLocations$group)\n\n# running the spatial test on grouped residuals\ntestSpatialAutocorrelation(res2, groupLocations$x, groupLocations$y, plot = F)\n\n# Testen auf Multicollinearität (dh zu starke Korrelationen im finalen Modell, zB falls\n# auf Grund der ökologischen Plausibilität stark korrelierte Variablen im Modell)\n# use VIF values: if values less then 5 is ok (sometimes &gt; 10), if mean of VIF values\n# not substantially greater than 1 (say 5), no need to worry.\n\ncar::vif(m_day)\nmean(car::vif(m_day))",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modellgüte und -diagnostics MM</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-3",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-3",
    "title": "Modellgüte und -diagnostics MM",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nGraphische Darstellung der Modellresultate\n\n# graphische Darstellung der gesamten Modellresultate\n\nplot_model(m_day, transform = NULL, show.values = TRUE, value.offset = .3)\n\n# Plotten der vorhergesagten Wahrscheinlichkeit, dass ein Kreis besetzt ist, in\n# Abhängigkeit der erklärenden Variable basierend auf den Modellresultaten.\n\nplot_model(m_day, type = \"pred\", terms = \"us_scaled [all]\")\n\n# Problem: skalierte Variablen lassen sich nicht so ohne weiteres plotten, hier ein Hack um das Problem zu umgehen (Danke chatGPT!). \n# Die Einstellungen müssen für jede Variable separat geändert werden\n\n# Plot erstellen\nplot_obj &lt;- plot_model(m_day, type = \"pred\", terms = \"us_scaled [all]\")\n\n# Rücktransformation für die Achse: hier skalierten Werte anpassen\n\noriginal_mean &lt;- mean(DF_mod_day$us_2014)  \noriginal_sd &lt;- sd(DF_mod_day$us_2014)  \n\nmin_us &lt;- min(DF_mod_day$us_2014)\nmax_us &lt;- max(DF_mod_day$us_2014)\n\n\nplot_obj +\n  scale_x_continuous(\n    limits = c((min_us - original_mean) / original_sd, (max_us - original_mean) / original_sd),  # skaliert\n    breaks = seq((min_us - original_mean) / original_sd, (max_us - original_mean) / original_sd, length.out = 5),  # automatische Schritte\n    labels = function(x) round(x * original_sd + original_mean, 2),  # Rücktransformierte Labels\n    name = \"Deckungsgrad Strauchschicht\"\n  )\n\n\n# Funktion um viele Plots auf einem zusammenbringen: cowplot-package (hat auch sonst\n# gute Funktionen für schöne Layouts für Grafiken)\n\ncowplot::plot_grid()",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modellgüte und -diagnostics MM</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-4",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-4",
    "title": "Modellgüte und -diagnostics MM",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nErmittlung des individuellen Beitrags der einzelnen Variablen im Gesamtmodell (leave-one-out Ansatz)\n\nBestimmen delta AIC nach Coppes u. a. (2017) → Vergleich des Gesamtmodells gegenüber einem Modell ohne die entsprechende Variable.\nAuftrag auf nächste Woche: Kurze Vorstellung der Modellresultate & Diagnostics im Plenum und Diskussion der Ergebnisse (keine PP-Präsentation nötig)\n\n\n\nMusterlösung\n\nvars &lt;- c(\"slope_scaled\", \"forest_prop_scaled\", \"dist_road_only_scaled\", \"dist_sett_scaled\", \"topo_pos_scaled\", \"us_scaled\")\n\n# leave-one-out models\nmodelle &lt;- lapply(vars, function(var) {\n  formula &lt;- as.formula(paste(\"pres_abs ~\", paste(setdiff(vars, var), collapse = \" + \"), \"+ (1 | id)\"))\n  glmer(formula, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n})\n\n\nnames(modelle) &lt;- paste0(\"m_\", vars)\n\nmodelle &lt;- c(list(m_day = m_day), modelle)\n\n\nbbmle::AICtab(modelle)\n##                         dAIC  df\n## m_topo_pos_scaled         0.0 7 \n## m_dist_sett_scaled        1.3 7 \n## m_day                     1.6 8 \n## m_slope_scaled           10.6 7 \n## m_dist_road_only_scaled  81.9 7 \n## m_us_scaled              94.1 7 \n## m_forest_prop_scaled    227.0 7",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modellgüte und -diagnostics MM</span>"
    ]
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#section",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#section",
    "title": "Modellgüte und -diagnostics MM",
    "section": "",
    "text": "Coppes, Joy, Judith Ehrlacher, Dominik Thiel, Rudi Suchant, und Veronika Braunisch. 2017. „Outdoor recreation causes effective habitat reduction in capercaillie Tetrao urogallus: a major threat for geographically restricted populations“. Journal of avian biology 48 (12): 1583–94.",
    "crumbs": [
      "Fallstudie N",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modellgüte und -diagnostics MM</span>"
    ]
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "Literaturverzeichnis",
    "section": "",
    "text": "Bochud, Murielle, Angéline Chatelan, Juan-Manuel Blanco, and Sigrid\nMaria Beer-Borst. 2017. “Anthropometric Characteristics and\nIndicators of Eating and Physical Activity Behaviors in the Swiss Adult\nPopulation: Results from menuCH 2014-2015.”\n\n\nBorcard, Daniel, François Gillet, Pierre Legendre, et al. 2011.\nNumerical Ecology with r. Vol. 2. Springer.\n\n\nCoppes, Joy, Judith Ehrlacher, Dominik Thiel, Rudi Suchant, and Veronika\nBraunisch. 2017. “Outdoor Recreation Causes Effective Habitat\nReduction in Capercaillie Tetrao Urogallus: A Major Threat for\nGeographically Restricted Populations.” Journal of Avian\nBiology 48 (12): 1583–94.\n\n\nGilgen, \"Kurt, and Alma Sartoris\". \"2010\". “\"Empfehlung Zur\nPlanung von Windenergieanlagen: Die Anwendung von\nRaumplanungsinstrumenten Und Kriterien Zur Standortwahl\".”\n\"Eidgenössisches Departement für Umwelt, Verkehr, Energie und\nKommunikation UVEK\".\n\n\nKovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen?\nWirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.\n\n\nLo, Steson, and Sally Andrews. 2015. “To Transform or Not to\nTransform: Using Generalized Linear Mixed Models to Analyse Reaction\nTime Data.” Frontiers in Psychology 6. https://doi.org/10.3389/fpsyg.2015.01171.\n\n\nScherler, Patrick. 2020. “Drivers of Departure and Prospecting in\nDispersing Juvenile Red Kites (Milvus Milvus).” PhD thesis,\nUniversity of Zurich.\n\n\nSchielzeth, Holger, Niels J Dingemanse, Shinichi Nakagawa, David F\nWestneat, Hassen Allegue, Céline Teplitsky, Denis Réale, Ned A\nDochtermann, László Zsolt Garamszegi, and Yimen G Araya-Ajoy. 2020.\n“Robustness of Linear Mixed-Effects Models to Violations of\nDistributional Assumptions.” Methods in Ecology and\nEvolution 11 (9): 1141–52.\n\n\nTegou, Leda-Ioanna, Heracles Polatidis, and Dias A. Haralambopoulos.\n2010. “Environmental Management Framework for Wind Farm Siting:\nMethodology and Case Study.” Journal of Environmental\nManagement 91 (11): 2134–47. https://doi.org/10.1016/j.jenvman.2010.05.010.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093.",
    "crumbs": [
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Literaturverzeichnis</span>"
    ]
  }
]